//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-30033411
// Cuda compilation tools, release 11.4, V11.4.48
// Based on NVVM 7.0.1
//

.version 7.4
.target sm_75
.address_size 64

	// .globl	_Z26tensor_core_example_8x8x16PiPKiS1_S1_
.extern .func  (.param .b32 func_retval0) vprintf
(
	.param .b64 vprintf_param_0,
	.param .b64 vprintf_param_1
)
;
.global .align 1 .b8 _ZN39_INTERNAL_17_hello_mma_cpp1_ii_63e784576thrust6system6detail10sequential3seqE[1];
.global .align 1 .b8 _ZN39_INTERNAL_17_hello_mma_cpp1_ii_63e784576thrust6system3cpp3parE[1];
.global .align 1 .b8 _ZN39_INTERNAL_17_hello_mma_cpp1_ii_63e784576thrust8cuda_cub3parE[1];
.global .align 1 .b8 _ZN39_INTERNAL_17_hello_mma_cpp1_ii_63e784576thrust12placeholders2_1E[1];
.global .align 1 .b8 _ZN39_INTERNAL_17_hello_mma_cpp1_ii_63e784576thrust12placeholders2_2E[1];
.global .align 1 .b8 _ZN39_INTERNAL_17_hello_mma_cpp1_ii_63e784576thrust12placeholders2_3E[1];
.global .align 1 .b8 _ZN39_INTERNAL_17_hello_mma_cpp1_ii_63e784576thrust12placeholders2_4E[1];
.global .align 1 .b8 _ZN39_INTERNAL_17_hello_mma_cpp1_ii_63e784576thrust12placeholders2_5E[1];
.global .align 1 .b8 _ZN39_INTERNAL_17_hello_mma_cpp1_ii_63e784576thrust12placeholders2_6E[1];
.global .align 1 .b8 _ZN39_INTERNAL_17_hello_mma_cpp1_ii_63e784576thrust12placeholders2_7E[1];
.global .align 1 .b8 _ZN39_INTERNAL_17_hello_mma_cpp1_ii_63e784576thrust12placeholders2_8E[1];
.global .align 1 .b8 _ZN39_INTERNAL_17_hello_mma_cpp1_ii_63e784576thrust12placeholders2_9E[1];
.global .align 1 .b8 _ZN39_INTERNAL_17_hello_mma_cpp1_ii_63e784576thrust12placeholders3_10E[1];
.global .align 1 .b8 _ZN39_INTERNAL_17_hello_mma_cpp1_ii_63e784576thrust3seqE[1];
.global .align 1 .b8 _ZN39_INTERNAL_17_hello_mma_cpp1_ii_63e784576thrust6deviceE[1];
.global .align 1 .b8 $str[45] = {98, 108, 111, 99, 107, 73, 100, 120, 46, 120, 58, 32, 37, 100, 44, 32, 116, 104, 114, 101, 97, 100, 73, 100, 120, 46, 120, 58, 32, 37, 100, 44, 32, 99, 100, 95, 105, 100, 120, 58, 32, 37, 100, 10, 0};

.visible .entry _Z26tensor_core_example_8x8x16PiPKiS1_S1_(
	.param .u64 _Z26tensor_core_example_8x8x16PiPKiS1_S1__param_0,
	.param .u64 _Z26tensor_core_example_8x8x16PiPKiS1_S1__param_1,
	.param .u64 _Z26tensor_core_example_8x8x16PiPKiS1_S1__param_2,
	.param .u64 _Z26tensor_core_example_8x8x16PiPKiS1_S1__param_3
)
{
	.local .align 8 .b8 	__local_depot0[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<19>;


	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd1, [_Z26tensor_core_example_8x8x16PiPKiS1_S1__param_0];
	ld.param.u64 	%rd2, [_Z26tensor_core_example_8x8x16PiPKiS1_S1__param_1];
	ld.param.u64 	%rd3, [_Z26tensor_core_example_8x8x16PiPKiS1_S1__param_2];
	ld.param.u64 	%rd4, [_Z26tensor_core_example_8x8x16PiPKiS1_S1__param_3];
	cvta.to.global.u64 	%rd5, %rd1;
	cvta.to.global.u64 	%rd6, %rd4;
	cvta.to.global.u64 	%rd7, %rd3;
	cvta.to.global.u64 	%rd8, %rd2;
	mov.u32 	%r7, %tid.x;
	shl.b32 	%r8, %r7, 1;
	mov.u32 	%r9, %ctaid.x;
	add.u64 	%rd9, %SP, 0;
	add.u64 	%rd10, %SPL, 0;
	st.local.v2.u32 	[%rd10], {%r9, %r7};
	st.local.u32 	[%rd10+8], %r8;
	mov.u64 	%rd11, $str;
	cvta.global.u64 	%rd12, %rd11;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd12;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd9;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r10, [retval0+0];
	} // callseq 0
	mul.wide.s32 	%rd13, %r8, 4;
	add.s64 	%rd14, %rd5, %rd13;
	mul.wide.s32 	%rd15, %r7, 4;
	add.s64 	%rd16, %rd8, %rd15;
	ld.global.u32 	%r3, [%rd16];
	add.s64 	%rd17, %rd7, %rd15;
	ld.global.u32 	%r4, [%rd17];
	add.s64 	%rd18, %rd6, %rd13;
	ld.global.u32 	%r5, [%rd18];
	ld.global.u32 	%r6, [%rd18+4];
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.s32.s8.s8.s32 {%r1, %r2}, {%r3}, {%r4}, {%r5, %r6};

	// end inline asm
	st.global.u32 	[%rd14], %r1;
	st.global.u32 	[%rd14+4], %r2;
	ret;

}
	// .globl	_ZN3cub11EmptyKernelIvEEvv
.visible .entry _ZN3cub11EmptyKernelIvEEvv()
{



	ret;

}

