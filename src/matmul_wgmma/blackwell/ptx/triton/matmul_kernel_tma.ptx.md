```MLIR
//
// Generated by LLVM NVPTX Back-End
//

.version 8.7
.target sm_100a
.address_size 64

	// .globl	matmul_kernel_tma       // -- Begin function matmul_kernel_tma
.extern .shared .align 16 .b8 global_smem[];
                                        // @matmul_kernel_tma
.visible .entry matmul_kernel_tma(
	.param .align 64 .b8 matmul_kernel_tma_param_0[128], // a_tensor_map
	.param .u32 matmul_kernel_tma_param_1,
	.param .u32 matmul_kernel_tma_param_2,
	.param .u64 matmul_kernel_tma_param_3,
	.param .u64 matmul_kernel_tma_param_4,
	.param .align 64 .b8 matmul_kernel_tma_param_5[128], // b_tensor_map
	.param .u32 matmul_kernel_tma_param_6,
	.param .u32 matmul_kernel_tma_param_7,
	.param .u64 matmul_kernel_tma_param_8,
	.param .u64 matmul_kernel_tma_param_9,
	.param .align 64 .b8 matmul_kernel_tma_param_10[128], // c_tensor_map
	.param .u32 matmul_kernel_tma_param_11,
	.param .u32 matmul_kernel_tma_param_12,
	.param .u64 matmul_kernel_tma_param_13,
	.param .u64 matmul_kernel_tma_param_14,
	.param .u32 matmul_kernel_tma_param_15, // M
	.param .u32 matmul_kernel_tma_param_16, // N
	.param .u32 matmul_kernel_tma_param_17, // K
	.param .u64 .ptr .global .align 1 matmul_kernel_tma_param_18, // unused
	.param .u64 .ptr .global .align 1 matmul_kernel_tma_param_19  // unused
)
.reqntid 384
// Requires 12 warps (12 * 32 = 384 threads)

.maxnreg 168
{
	.reg .pred 	%p<72>;
	.reg .b32 	%r<272>;
	.reg .b64 	%rd<46>;
	.loc	1 51 0                          // matmul_tma_kernel.py:51:0
$L__func_begin0:
	.loc	1 51 0                          // matmul_tma_kernel.py:51:0

// %bb.0:
	ld.param.b32 	%r25, [matmul_kernel_tma_param_16];
// %r25 = N

	ld.param.b32 	%r24, [matmul_kernel_tma_param_15];
// %r24 = M

	mov.b64 	%rd4, matmul_kernel_tma_param_0;
// %rd4 = address of a_tensor_map

	mov.b64 	%rd5, matmul_kernel_tma_param_10;
// %rd5 = address of c_tensor_map

$L__tmp0:
	.loc	1 51 0                          // matmul_tma_kernel.py:51
	cvta.param.u64 	%rd1, %rd5;
// Convert address of c_tensor_map to an u64 value
// Now we can regard:
// %rd1 = c_tensor_map

	mov.b64 	%rd6, matmul_kernel_tma_param_5;
// %rd6 = address of b_tensor_map

	cvta.param.u64 	%rd8, %rd6;
// Convert address of b_tensor_map to an u64 value
// %rd8 = b_tensor_map

	cvta.param.u64 	%rd7, %rd4;
// Convert address of a_tensor_map to an u64 value
// %rd7 = a_tensor_map

	mov.u32 	%r1, %tid.x;
// %r1 = tid

	shr.u32 	%r2, %r1, 5;
// %r2 = tid // 2^5
// %r2 = warp_id

	shfl.sync.idx.b32 	%r3, %r2, 0, 31, -1;
// Sync the executing warp. All threads in every warp will sync at this point.
// %r3 = warp_id

	setp.lt.u32 	%p1, %r3, 8;
// %p1 = true if warp_id < 8
// %p1 = false if warp_id >= 8

	@%p1 bra 	$L__BB0_15;
// Warp 0-7:
//  Jump to `$L__BB0_15`

	bra.uni 	$L__BB0_1;
// Warp 8-11:
//  Jump to `$L__BB0_1`


// ========================================================================================
// Warp 0-7 jump here
// ========================================================================================
$L__BB0_15:
	setmaxnreg.inc.sync.aligned.u32 	240;
	setp.lt.u32 	%p38, %r1, 32;
// %p38 = true if tid < 32

	mov.b32 	%r147, global_smem;
// %r147 = global_smem

	// begin inline asm
	@%p38 tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [%r147], 64;
	// end inline asm
// This is the core Blackwell-specific instruction which allocates a TMEM tile of 128 rows by 64 columns (128 rows are implicitly fixed by hardware).
//
// `@%p38` (Predication): The instruction executes only if `%p38` is True (i.e., only threads 0â€“31 execute this).
// `tcgen05`: "Tensor Core Generation 5". This identifies the instruction as part of the Blackwell ISA (SM100+), replacing Hopper's `wgmma` instructions.
// `.alloc`: Allocates space in TMEM (Tensor Memory). TMEM is a dedicated memory space on the SM used to feed Tensor Cores efficiently without consuming registers or standard Shared Memory bandwidth.
// `.cta_group::1`: Specifies the scope of the allocation. `cta_group::1` indicates the allocation is for a single CTA (Cooperative Thread Array / Thread Block). Blackwell supports "multicast" or grouped behaviors (`cta_group::2`) where TMEM might be shared or accessed across multiple thread blocks in a cluster; here it is local to one.
// `.sync.aligned`: A standard warp-level synchronization qualifier. It requires all threads in the active mask (the warp) to execute this instruction simultaneously and be converged.
// `.shared::cta`: Specifies the state space of the destination operand. The result of the allocation (the TMEM handle) will be written into Shared Memory.
// `.b32`: The data type of the destination operand (a 32-bit bitfield/address).
// `[%r147]` (Destination): The instruction writes the base address (handle) of the allocated TMEM into the shared memory address stored in register `%r147`.
// `64` (Source): The size of the allocation. TMEM is allocated in units of columns. This requests 64 columns of Tensor Memory.
//
// Why is this necessary?
// In Blackwell, you must explicitly allocate and deallocate TMEM. Unlike registers (managed by the compiler) or Shared Memory (declared statically or dynamically at launch), TMEM is managed via these specific `tcgen05` instructions. The allocation logic must typically be performed by a single elected warp (usually warp 0) to avoid redundant allocations, which is why the code uses the `tid.x < 32` check.

	bar.sync 	0, 256;
// Sync warp 0-7

	ld.shared.b32 	%r214, [global_smem];
// %r214 = TMEM handle

	bar.sync 	0, 256;
// Sync warp 0-7

	// begin inline asm
	@%p38 tcgen05.relinquish_alloc_permit.cta_group::1.sync.aligned;
	// end inline asm
// Release TMEM allocation permit

	.loc	1 59 24                         // matmul_tma_kernel.py:59:24
	mov.u32 	%r215, %ctaid.x;
// %r215 = blockIdx.x = pid

$L__tmp1:
	.loc	2 41 22                         // standard.py:41:22 @[ matmul_tma_kernel.py:60:27 ]
	add.s32 	%r216, %r24, 63;
// %r216 = M + 64 - 1
// Here, 64 means BLOCK_M

	.loc	2 41 28                         // standard.py:41:28 @[ matmul_tma_kernel.py:60:27 ]
	shr.s32 	%r217, %r216, 31;
	shr.u32 	%r218, %r217, 26;
// Since M > 0:
// %r218 = 0

	add.s32 	%r219, %r216, %r218;
	shr.s32 	%r220, %r219, 6;
// 2^6 = 64
// %r220 = (M + 64 - 1) // 64 = num_pid_m (the number of tiles along dimension M)
// %r220 = num_pid_m

$L__tmp2:
	.loc	2 41 22                         // standard.py:41:22 @[ matmul_tma_kernel.py:61:27 ]
	add.s32 	%r221, %r25, 63;
// %r221 = N + 64 - 1
// Here, 64 means BLOCK_N

	.loc	2 41 28                         // standard.py:41:28 @[ matmul_tma_kernel.py:61:27 ]
	shr.s32 	%r222, %r221, 31;
	shr.u32 	%r223, %r222, 26;
// Since N > 0:
// %r223 = 0

	add.s32 	%r224, %r221, %r223;
	shr.s32 	%r225, %r224, 6;
// %r225 = (N + 64 - 1) // 64 = num_pid_n (the number of tiles along dimension N)
// %r225 = num_pid_n

$L__tmp3:
	.loc	1 62 38                         // matmul_tma_kernel.py:62:38
	shl.b32 	%r226, %r225, 3;
// %r226 = num_pid_n * GROUP_M = num_pid_in_group
// %r226 = num_pid_in_group

	.loc	1 63 22                         // matmul_tma_kernel.py:63:22
	div.s32 	%r227, %r215, %r226;
// %r227 = pid // num_pid_in_group = group_id
// %r227 = group_id

	.loc	1 64 29                         // matmul_tma_kernel.py:64:29
	shl.b32 	%r228, %r227, 3;
// %r228 = group_id * GROUP_M = first_pid_m
// %r228 = first_pid_m

	.loc	1 65 35                         // matmul_tma_kernel.py:65:35
	sub.s32 	%r229, %r220, %r228;
// %r229 = num_pid_m - first_pid_m

	.loc	1 65 48                         // matmul_tma_kernel.py:65:48
	min.s32 	%r230, %r229, 8;
// %r230 = min(num_pid_m - first_pid_m, 8) = group_m
// %r230 = group_m

	.loc	1 66 33                         // matmul_tma_kernel.py:66:33
	rem.s32 	%r231, %r215, %r230;
// %r231 = pid % group_m

	.loc	1 66 27                         // matmul_tma_kernel.py:66:27
	add.s32 	%r232, %r228, %r231;
// %r232 = first_pid_m + (pid % group_m) = pid_m
// %r232 = pid_m

	.loc	1 67 19                         // matmul_tma_kernel.py:67:19
	mul.lo.s32 	%r233, %r227, %r226;
// %r233 = group_id * num_pid_in_group

	sub.s32 	%r234, %r215, %r233;
// %r234 = pid - group_id * num_pid_in_group

	.loc	1 67 40                         // matmul_tma_kernel.py:67:40
	div.s32 	%r235, %r234, %r230;
// group_id = pid // num_pid_in_group
// r = pid % num_pid_in_group
// => pid = group_id * num_pid_in_group + pid % num_pid_in_group
// => pid - group_id * num_pid_in_group = pid % num_pid_in_group
// %r235 = (pid - group_id * num_pid_in_group) // group_m = pid_n
// %r235 = (pid % num_pid_in_group) // group_m = pid_n
// %r235 = pid_n

	.loc	1 70 22                         // matmul_tma_kernel.py:70:22
	shl.b32 	%r212, %r232, 6;
// %r212 = pid_m * 64 = offs_am
// %r212 = offs_am

	.loc	1 71 22                         // matmul_tma_kernel.py:71:22
	shl.b32 	%r236, %r235, 6;
// %r236 = pid_n * 64 = offs_bn
// %r236 = offs_bn

	.loc	1 78 37                         // matmul_tma_kernel.py:78:37
	shfl.sync.idx.b32 	%r237, %r2, 0, 31, -1;
// %r237 = warp_id

	shl.b32 	%r238, %r237, 21;
// %r238 = warp_id * 2^21
// %r238 = [0, 1, ..., 11] * 2^21
// ===========================
// warp 0:
//   %r238 = 0
// warp 1:
//   %r238 = 2^21
// warp 2:
//   %r238 = 2^22
// warp 3:
//   %r238 = 2^22 + 2^21
// ===========================
// warp 4:
//   %r238 = 2^23 + 0
// warp 5:
//   %r238 = 2^23 + 2^21
// warp 6:
//   %r238 = 2^23 + 2^22
// warp 7:
//   %r238 = 2^23 + 2^22 + 2^21
// ===========================

	and.b32 	%r239, %r238, 6291456;
// 6291456 = 00000000011000000000000000000000
// 6291456 = 2^22 + 2^21
// %r239 = %r238 and (2^22 + 2^21)
// ============================================
// warp 0-3:
//   %r239 = [0, 2^21, 2^22, 2^22 + 2^21]
// warp 4-7:
//   %r239 = [0, 2^21, 2^22, 2^22 + 2^21]

	add.s32 	%r240, %r239, %r214;
// %r240 = %r239 + TMEM_handle
// TMEM
//   col index: bits 15-0
//   row index: bits 31-16
// %r240 = TMEM_handle + [ROW_0, ROW_32, ROW_64, ROW_96]

	shl.b32 	%r241, %r237, 3;
// %r241 = warp_id * 2^3

	and.b32 	%r242, %r241, 32;
// %r242 = warp_id * 2^3 and 2^5
// warp 0-3:
//   %r242 = 0
// warp 4-7:
//   %r242 = 2^5

	add.s32 	%r148, %r240, %r242;
// warp 0-3:
//   %r148 = TMEM_handle + [ROW_0, ROW_32, ROW_64, ROW_96]
// warp 4-7:
//   %r148 = TMEM_handle + [ROW_0, ROW_32, ROW_64, ROW_96] + 2^5 (COL_32)

	mov.pred 	%p40, -1;
	mov.b32 	%r149, 0;

	// begin inline asm
	@%p40 tcgen05.st.sync.aligned.16x32bx2.x16.b32 [%r148 + 0], 16, {%r149, %r149, %r149, %r149, %r149, %r149, %r149, %r149, %r149, %r149, %r149, %r149, %r149, %r149, %r149, %r149};
	// end inline asm
// Reset TMEM

	// begin inline asm
	tcgen05.wait::st.sync.aligned;
	// end inline asm

	bar.sync 	0, 256;

	.loc	1 74 31                         // matmul_tma_kernel.py:74:31
	setp.eq.b32 	%p41, %r1, 0;
// %p41 = true if tid == 0

	add.s32 	%r165, %r147, 82048;
// %r165 = global_smem + 82048

	// begin inline asm
	@%p41 mbarrier.init.shared::cta.b64 [%r165], 1;
	// end inline asm

	bar.sync 	0, 256;

	add.s32 	%r166, %r147, 82056;
// %r166 = %r165 + 8

	// begin inline asm
	@%p41 mbarrier.init.shared::cta.b64 [%r166], 1;
	// end inline asm

	bar.sync 	0, 256;

	add.s32 	%r167, %r147, 82064;
// %r167 = %r166 + 8

	// begin inline asm
	@%p41 mbarrier.init.shared::cta.b64 [%r167], 1;
	// end inline asm

	bar.sync 	0, 256;

	add.s32 	%r168, %r147, 82072;
// %r168 = %r167 + 8

	// begin inline asm
	@%p41 mbarrier.init.shared::cta.b64 [%r168], 1;
	// end inline asm

	bar.sync 	0, 256;

	add.s32 	%r169, %r147, 82080;
// %r169 = %r168 + 8

	// begin inline asm
	@%p41 mbarrier.init.shared::cta.b64 [%r169], 1;
	// end inline asm
	
	add.s32 	%r170, %r147, 82096;
	// begin inline asm
	@%p41 mbarrier.init.shared::cta.b64 [%r170], 1;
	// end inline asm
	bar.sync 	0, 256;
	add.s32 	%r171, %r147, 82104;
	// begin inline asm
	@%p41 mbarrier.init.shared::cta.b64 [%r171], 1;
	// end inline asm
	bar.sync 	0, 256;
	add.s32 	%r172, %r147, 82112;
	// begin inline asm
	@%p41 mbarrier.init.shared::cta.b64 [%r172], 1;
	// end inline asm
	bar.sync 	0, 256;
	add.s32 	%r173, %r147, 82120;
	// begin inline asm
	@%p41 mbarrier.init.shared::cta.b64 [%r173], 1;
	// end inline asm
	bar.sync 	0, 256;
	add.s32 	%r174, %r147, 82128;
	// begin inline asm
	@%p41 mbarrier.init.shared::cta.b64 [%r174], 1;
	// end inline asm

	.loc	1 0 0                           // matmul_tma_kernel.py:0
	bar.sync 	0, 256;

	// begin inline asm
	@%p41 mbarrier.arrive.shared::cta.b64 _, [%r165];
	// end inline asm
	bar.sync 	0, 256;
	// begin inline asm
	@%p41 mbarrier.arrive.shared::cta.b64 _, [%r166];
	// end inline asm
	bar.sync 	0, 256;
	// begin inline asm
	@%p41 mbarrier.arrive.shared::cta.b64 _, [%r167];
	// end inline asm
	bar.sync 	0, 256;
	// begin inline asm
	@%p41 mbarrier.arrive.shared::cta.b64 _, [%r168];
	// end inline asm
	bar.sync 	0, 256;
	// begin inline asm
	@%p41 mbarrier.arrive.shared::cta.b64 _, [%r169];
	// end inline asm
// 
// Prime Consumer mbarriers %r165-169

	.loc	1 74 31                         // matmul_tma_kernel.py:74:31
	bar.sync 	0, 256;
	add.s32 	%r180, %r147, 82144;
// %r180 = global_smem + 82144

	// begin inline asm
	@%p41 mbarrier.init.shared::cta.b64 [%r180], 1;
	// end inline asm

	st.shared.b32 	[global_smem+82152], 33554689;
// 33554689 = 00000010|00000000|00000001|00000001

	st.shared.b32 	[global_smem+81920], %r214;
// Store TMEM_handle to [global_smem+81920]

	barrier.sync 	1;
// Warp 0-7 arrive at the 1st CTA sync point

	setmaxnreg.inc.sync.aligned.u32 	240;

	barrier.sync 	1;
// Warp 0-7 arrive at the 2nd CTA sync point

	barrier.sync 	1;
// Warp 0-7 arrive at the 3rd CTA sync point

	setmaxnreg.inc.sync.aligned.u32 	240;
	.loc	1 78 37                         // matmul_tma_kernel.py:78:37

	bar.sync 	0, 256;
	// begin inline asm
	
{
	.reg .pred complete;
	waitLoop:
	mbarrier.try_wait.parity.shared.b64 complete, [%r180], %r149;
	@!complete bra.uni waitLoop;
}
// Warp 0-7 wait for the final mbarrier %r180 to be satisfied.
// Once %r180 is satisfied, MMA has consumed all k_tiles along K dimension

	// end inline asm
	.loc	1 74 31                         // matmul_tma_kernel.py:74:31
	bar.sync 	0, 256;
	// begin inline asm
	@%p41 mbarrier.inval.shared::cta.b64 [%r180];
	// end inline asm
	// begin inline asm
	@%p41 mbarrier.inval.shared::cta.b64 [%r170];
	// end inline asm
	bar.sync 	0, 256;
	// begin inline asm
	@%p41 mbarrier.inval.shared::cta.b64 [%r171];
	// end inline asm
	bar.sync 	0, 256;
	// begin inline asm
	@%p41 mbarrier.inval.shared::cta.b64 [%r172];
	// end inline asm
	bar.sync 	0, 256;
	// begin inline asm
	@%p41 mbarrier.inval.shared::cta.b64 [%r173];
	// end inline asm
	bar.sync 	0, 256;
	// begin inline asm
	@%p41 mbarrier.inval.shared::cta.b64 [%r174];
	// end inline asm
	// begin inline asm
	@%p41 mbarrier.inval.shared::cta.b64 [%r165];
	// end inline asm
	bar.sync 	0, 256;
	// begin inline asm
	@%p41 mbarrier.inval.shared::cta.b64 [%r166];
	// end inline asm
	bar.sync 	0, 256;
	// begin inline asm
	@%p41 mbarrier.inval.shared::cta.b64 [%r167];
	// end inline asm
	bar.sync 	0, 256;
	// begin inline asm
	@%p41 mbarrier.inval.shared::cta.b64 [%r168];
	// end inline asm
	bar.sync 	0, 256;
	// begin inline asm
	@%p41 mbarrier.inval.shared::cta.b64 [%r169];
	// end inline asm
	.loc	1 78 37                         // matmul_tma_kernel.py:78:37
	// begin inline asm
	tcgen05.ld.sync.aligned.16x32bx2.x16.b32 {%r194, %r195, %r196, %r197, %r198, %r199, %r200, %r201, %r202, %r203, %r204, %r205, %r206, %r207, %r208, %r209}, [%r148 + 0], 16;
	// end inline asm
	// begin inline asm
	tcgen05.wait::ld.sync.aligned;
	// end inline asm
// Warp 0-7 load TMEM to registers

	.loc	1 82 37                         // matmul_tma_kernel.py:82:37
	shl.b32 	%r243, %r1, 7;
	and.b32 	%r244, %r243, 1920;
	shl.b32 	%r245, %r1, 6;
	and.b32 	%r246, %r245, 14336;
	shl.b32 	%r247, %r1, 4;
	and.b32 	%r248, %r247, 112;
	shl.b32 	%r249, %r1, 2;
	and.b32 	%r250, %r249, 64;
	or.b32 	%r251, %r246, %r248;
	xor.b32 	%r252, %r251, %r250;
	or.b32 	%r253, %r252, %r244;
	add.s32 	%r254, %r147, %r253;
	st.shared.v4.b32 	[%r254], {%r194, %r195, %r196, %r197};
	xor.b32 	%r255, %r253, 16;
	add.s32 	%r256, %r147, %r255;
	st.shared.v4.b32 	[%r256], {%r198, %r199, %r200, %r201};
	xor.b32 	%r257, %r253, 32;
	add.s32 	%r258, %r147, %r257;
	st.shared.v4.b32 	[%r258], {%r202, %r203, %r204, %r205};
	xor.b32 	%r259, %r253, 48;
	add.s32 	%r260, %r147, %r259;
	st.shared.v4.b32 	[%r260], {%r206, %r207, %r208, %r209};
// Warp 0-7 store registers into SMEM in 128B swizzling mode

	// begin inline asm
	fence.proxy.async.shared::cta;
	// end inline asm

	bar.sync 	0, 256;
	elect.sync 	%r261|%p70, -1;
	setp.lt.u32 	%p71, %r1, 64;
	and.pred 	%p68, %p71, %p70;
	and.b32 	%r262, %r237, 1;
	shl.b32 	%r263, %r262, 13;
	add.s32 	%r213, %r147, %r263;
	shl.b32 	%r264, %r262, 5;
	or.b32 	%r211, %r264, %r236;

	// begin inline asm
	@%p68 cp.async.bulk.tensor.2d.global.shared::cta.bulk_group [%rd1, {%r211, %r212}], [%r213];
	// end inline asm
	cp.async.bulk.commit_group;
	cp.async.bulk.wait_group.read 	0;
// TMA matrix D in SMEM -> GMEM


	bar.sync 	0, 256;
	.loc	1 82 4                          // matmul_tma_kernel.py:82:4
	bar.sync 	0, 256;

	// begin inline asm
	@%p38 tcgen05.dealloc.cta_group::1.sync.aligned.b32 %r214, 64;
	// end inline asm
// Deallocate TMEM

	st.shared.b32 	[global_smem+82152], 50529027;
// Update branch indices

	barrier.sync 	1;
// Warp 0-7 arrive at the 4th CTA sync point

// Warp 0-11 jump here as final point
$L__BB0_16:                             // %common.ret
	.loc	1 0 0                           // matmul_tma_kernel.py:0
	ret;


// Warp 8-11 jump here for the first time
$L__BB0_1:                              // %.preheader.preheader
	ld.param.b32 	%r26, [matmul_kernel_tma_param_17];
	// %r26 = K

	bra.uni 	$L__BB0_2;


// ========================================================================================
// Warp 11 jumps here
// ========================================================================================
$L__BB0_14:                             //   in Loop: Header=BB0_2 Depth=1
	.loc	1 74 31                         // matmul_tma_kernel.py:74:31
	setmaxnreg.inc.sync.aligned.u32 	24;
	barrier.sync 	1;
// Warp 11 arrives the 2nd CTA sync point

	barrier.sync 	1;
// Warp 11 arrives the 3rd CTA sync point

	setmaxnreg.inc.sync.aligned.u32 	24;

// Warp 8-11 jump here from $L__BB0_1 for the first time
$L__BB0_2:                              // %.preheader
                                        // =>This Loop Header: Depth=1
                                        //     Child Loop BB0_12 Depth 2
                                        //     Child Loop BB0_8 Depth 2
	.loc	1 51 0                          // matmul_tma_kernel.py:51
	setmaxnreg.dec.sync.aligned.u32 	24;
	barrier.sync 	1;
// Warp 8-9 arrive at 1st CTA sync point
// Warp 10 arrives at 1st CTA sync point
// Warp 11 arrives at 1st CTA sync point
// 
// Warp 8-9 arrive at 4th CTA sync point
// Warp 10 arrives at 4th CTA sync point
// Warp 11 arrives at 4th CTA sync point

	mov.b32 	%r28, global_smem;
	add.s32 	%r29, %r28, %r3;
// %r29 = global_smem + warp_id (8-11)

	ld.shared.b8 	%r27, [%r29+82144];
// %r27 = global_smem + [8, 9, 10, 11] + 82144
// %r27 = global_smem + [82152, 82153, 82154, 82155]
// [global_smem + 82152] = 00000010|00000000|00000001|00000001
// ---------------------------------------------------------------
// warp 8:
//   %r27 = 1
// warp 9:
//   %r27 = 1
// warp 10:
//   %r27 = 0
// warp 11:
//   %r27 = 2
// 
// Next cycle
// [global_smem + 82152] = 50529027
// 50529027 = 00000011|00000011|00000011|00000011
// Warp 8-11:
//   %r27 = 3

	setp.gt.u32 	%p2, %r27, 3;
// %p2 = false because %r27 < 3

	@%p2 bra 	$L__BB0_4;
// Warp 8-11 skip this instruction

// %bb.3:                               // %.preheader
                                        //   in Loop: Header=BB0_2 Depth=1
	$L_brx_0: .branchtargets
		$L__BB0_5,
		$L__BB0_10,
		$L__BB0_14,
		$L__BB0_16;
	brx.idx 	%r27, $L_brx_0;
// Warp 8-9:
//   Jump to `$L__BB0_10`
// Warp 10:
//   Jump to `$L__BB0_5`
// Warp 11:
//   Jump to `$L__BB0_14`
// 
// Next cycle
// Warp 8-11
//   Jump to `$L__BB0_16`

// ========================================================================================
// Warp 10 jumps here
// ========================================================================================
$L__BB0_5:                              //   in Loop: Header=BB0_2 Depth=1
	.loc	1 74 31                         // matmul_tma_kernel.py:74:31
	setmaxnreg.inc.sync.aligned.u32 	24;
	ld.shared.b32 	%r82, [global_smem+81920];
// %r82 = TMEM_handle

	barrier.sync 	1;
// Warp 10 arrives at the 2nd CTA sync point

$L__tmp4:
	.loc	2 41 22                         // standard.py:41:22 @[ matmul_tma_kernel.py:69:25 ]
	add.s32 	%r75, %r26, 63;
// %r75 = K + 64 - 1

$L__tmp5:
	.loc	1 74 31                         // matmul_tma_kernel.py:74:31
	setp.lt.s32 	%p12, %r75, 64;
// %p12 = true if K <= 0
// %p12 = false because K > 0

	@%p12 bra 	$L__BB0_9;
// This will be skipped

// %bb.6:                               // %.lr.ph3
                                        //   in Loop: Header=BB0_2 Depth=1
	.loc	1 0 0                           // matmul_tma_kernel.py:0
	shr.s32 	%r76, %r75, 31;
	shr.u32 	%r77, %r76, 26;
	add.s32 	%r78, %r75, %r77;
	shr.s32 	%r79, %r78, 6;
// %r79 = k_tiles

	add.s32 	%r265, %r79, -1;
// Before MMA Loop
// %r265 = k_tiles - 1
// %r265 is k_tile_counter

	bar.warp.sync 	-1;
// Sync warp 10

	add.s32 	%r80, %r28, 82096;
// %r80 = global_smem + 82096 = %r170

	mov.b32 	%r266, 0;
// Init parity phase %r266 = 0

	// begin inline asm
	
{
	.reg .pred complete;
	waitLoop:
	mbarrier.try_wait.parity.shared.b64 complete, [%r80], %r266;
	@!complete bra.uni waitLoop;
}
// Warp 10 waits for Producer mbarrier %r170 to be satisfied by Copy Engine (warp 8-9)

	// end inline asm
	.loc	1 78 37                         // matmul_tma_kernel.py:78:37
	setp.eq.b32 	%p23, %r265, 0;
// %p23 = true if k_tile_counter == 0
// %p23 = false if k_tile_counter > 0

	elect.sync 	%r91|%p14, -1;
// Warp 10 elects the leader thread

	bfe.u32 	%r92, %r28, 4, 14;
// Extract start address
// %r92 = 14_bits_of_start_addr

	cvt.u64.u32 	%rd19, %r92;
// Convert 14_bits_of_start_addr to u64

	or.b64 	%rd9, %rd19, 4611686293338849280;
// %rd9 = a_matrix_descriptor

	add.s32 	%r93, %r28, 40960;
// %r93 = global_smem + 40960

	bfe.u32 	%r94, %r93, 4, 14;
	cvt.u64.u32 	%rd20, %r94;
	or.b64 	%rd10, %rd20, 4611686293338849280;
// %rd10 = b_matrix_descriptor

	mov.b32 	%r83, 68157456;
// 68157456 = 000|00100|0|001000|00000000000010000
// bits 22-17 = 001000 => N = 2^3 * 8 = 64
// bits 28-24 = 00100 => M = 2^2 * 16 = 64

	mov.pred 	%p13, 0;
// %p13 = 0

	// begin inline asm
	@%p14 tcgen05.mma.cta_group::1.kind::f16 [ %r82 + 0 ], %rd9, %rd10, %r83, %p13;
	// end inline asm
// The 1st MMA m64n64k16
// %r82 = TMEM_handle
// %rd9 = a_matrix_descriptor
// %rd10 = b_matrix_descriptor
// %r83 = instruction_descriptor
// %p13: Accumulation Flag
//   If True (1): Add result to the existing value in TMEM.
//   If False (0): Overwrite the value in TMEM, effectively treating as zero.

	add.s32 	%r95, %r28, 32;
// Here, 32 = 16 * 2 bytes
// %r95 = global_smem + 16 * 2 bytes
// %r95 is the start address of the next 64x16 tile_m

	bfe.u32 	%r96, %r95, 4, 14;
	cvt.u64.u32 	%rd21, %r96;
	or.b64 	%rd11, %rd21, 4611686293338849280;
// %rd11 is the next a_matrix_descriptor

	add.s32 	%r97, %r28, 40992;
	bfe.u32 	%r98, %r97, 4, 14;
	cvt.u64.u32 	%rd22, %r98;
	or.b64 	%rd12, %rd22, 4611686293338849280;
// %rd12 is the next b_matrix_descriptor

	mov.pred 	%p15, -1;
	// begin inline asm
	@%p14 tcgen05.mma.cta_group::1.kind::f16 [ %r82 + 0 ], %rd11, %rd12, %r83, %p15;
	// end inline asm
// The 2nd MMA m64n64k16
// %p15 = true: Add result to the existing value in TMEM.

	add.s32 	%r99, %r28, 64;
	bfe.u32 	%r100, %r99, 4, 14;
	cvt.u64.u32 	%rd23, %r100;
	or.b64 	%rd13, %rd23, 4611686293338849280;
	add.s32 	%r101, %r28, 41024;
	bfe.u32 	%r102, %r101, 4, 14;
	cvt.u64.u32 	%rd24, %r102;
	or.b64 	%rd14, %rd24, 4611686293338849280;
	// begin inline asm
	@%p14 tcgen05.mma.cta_group::1.kind::f16 [ %r82 + 0 ], %rd13, %rd14, %r83, %p15;
	// end inline asm
// The 3rd MMA m64n64k16

	add.s32 	%r103, %r28, 96;
	bfe.u32 	%r104, %r103, 4, 14;
	cvt.u64.u32 	%rd25, %r104;
	or.b64 	%rd15, %rd25, 4611686293338849280;
	add.s32 	%r105, %r28, 41056;
	bfe.u32 	%r106, %r105, 4, 14;
	cvt.u64.u32 	%rd26, %r106;
	or.b64 	%rd16, %rd26, 4611686293338849280;
	// begin inline asm
	@%p14 tcgen05.mma.cta_group::1.kind::f16 [ %r82 + 0 ], %rd15, %rd16, %r83, %p15;
	// end inline asm
// The 4th MMA m64n64k16

	add.s32 	%r107, %r28, 82048;
	cvt.u64.u32 	%rd17, %r107;
// %rd17 = %r165

	// begin inline asm
	@%p14 tcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [%rd17];
	// end inline asm
// 1. Commit MMA ops
// 2. Satisfy Consumer mbarrier %r165

	and.pred 	%p22, %p23, %p14;
// If k_tile_counter > 0: %p23 = false => %p22 = false

	add.s32 	%r108, %r28, 82144;
// %r180 = global_smem + 82144
// %r108 = %r180 (the final mbarrier)

	cvt.u64.u32 	%rd18, %r108;
// %rd18 = the final mbarrier

	// begin inline asm
	@%p22 tcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [%rd18];
	// end inline asm
// Usually, this will be skipped
// Satisfy the final mbarrier %r180 if k_tile_counter == 0

	.loc	1 74 31                         // matmul_tma_kernel.py:74:31
	add.s32 	%r109, %r26, -1;
// %r109 = K - 1

	setp.lt.u32 	%p24, %r109, 64;
// %p24 = true if K <= 64 (this case never happen)
// %p24 = false if K > 64

	@%p24 bra 	$L__BB0_9;
// This will be skipped

// %bb.7:                               // %.peel.next.preheader
                                        //   in Loop: Header=BB0_2 Depth=1
	.loc	1 0 31                          // matmul_tma_kernel.py:0:31
	mov.b32 	%r267, 1;
// %r267 = 1

//---------------------------------------------------------------------------------------
// Warp 10 enters MMA Loop
//---------------------------------------------------------------------------------------
$L__BB0_8:                              // %.peel.next
                                        //   Parent Loop BB0_2 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	.loc	1 74 31                         // matmul_tma_kernel.py:74:31
	shl.b32 	%r122, %r267, 3;
// Iteration 1:
//   %r267 = 1
//   %r122 = 1 * 8
// 

	add.s32 	%r124, %r28, %r122;
// Iteration 1:
//   %r124 = global_smem + 1 * 8

	add.s32 	%r125, %r124, 82048;
// Iteration 1:
//   %r125 = global_smem + 82048 + 1 * 8 = %r166

	add.s32 	%r112, %r124, 82096;
// Iteration 1:
//   %r112 = global_smem + 82096 + 1 * 8 = %r171

	.loc	1 76 24                         // matmul_tma_kernel.py:76:24
	shl.b32 	%r126, %r267, 13;
// Iteration 1:
//   %r126 = 1 * 8192 (8192 = 64*64*2 = size of 64x64xf16 tile)

	add.s32 	%r127, %r28, %r126;
// Iteration 1:
//   %r127 = global_smem + 1 * 8192 (the start address of next 64x64xf16 tile_m)

	.loc	1 77 24                         // matmul_tma_kernel.py:77:24
	add.s32 	%r128, %r127, 40960;
// Iteration 1:
//   %r128 = global_smem + 40960 + 1 * 8192 (the start address of next 64x64xf16 tile_n)

	.loc	1 0 0                           // matmul_tma_kernel.py:0
	bar.warp.sync 	-1;
// Sync warp 10

	// begin inline asm
{
	.reg .pred complete;
	waitLoop:
	mbarrier.try_wait.parity.shared.b64 complete, [%r112], %r266;
	@!complete bra.uni waitLoop;
}
	// end inline asm
// Warp 10 waits for Producer mbarrier %r171-174 to be satisfied.

	.loc	1 78 37                         // matmul_tma_kernel.py:78:37
	setp.eq.b32 	%p35, %r265, 1;
// %p35 = true if k_tile_counter == 1 (this is the final MMA)

	elect.sync 	%r129|%p26, -1;
// Warp 10 elects the leader thread

	bfe.u32 	%r130, %r127, 4, 14;
	cvt.u64.u32 	%rd37, %r130;
	or.b64 	%rd27, %rd37, 4611686293338849280;
	bfe.u32 	%r131, %r128, 4, 14;
	cvt.u64.u32 	%rd38, %r131;
	or.b64 	%rd28, %rd38, 4611686293338849280;
	mov.b32 	%r115, 68157456;
	mov.pred 	%p25, -1;
	// begin inline asm
	@%p26 tcgen05.mma.cta_group::1.kind::f16 [ %r82 + 0 ], %rd27, %rd28, %r115, %p25;
	// end inline asm
	add.s32 	%r132, %r127, 32;
	bfe.u32 	%r133, %r132, 4, 14;
	cvt.u64.u32 	%rd39, %r133;
	or.b64 	%rd29, %rd39, 4611686293338849280;
	add.s32 	%r134, %r127, 40992;
	bfe.u32 	%r135, %r134, 4, 14;
	cvt.u64.u32 	%rd40, %r135;
	or.b64 	%rd30, %rd40, 4611686293338849280;
	// begin inline asm
	@%p26 tcgen05.mma.cta_group::1.kind::f16 [ %r82 + 0 ], %rd29, %rd30, %r115, %p25;
	// end inline asm
	add.s32 	%r136, %r127, 64;
	bfe.u32 	%r137, %r136, 4, 14;
	cvt.u64.u32 	%rd41, %r137;
	or.b64 	%rd31, %rd41, 4611686293338849280;
	add.s32 	%r138, %r127, 41024;
	bfe.u32 	%r139, %r138, 4, 14;
	cvt.u64.u32 	%rd42, %r139;
	or.b64 	%rd32, %rd42, 4611686293338849280;
	// begin inline asm
	@%p26 tcgen05.mma.cta_group::1.kind::f16 [ %r82 + 0 ], %rd31, %rd32, %r115, %p25;
	// end inline asm
	add.s32 	%r140, %r127, 96;
	bfe.u32 	%r141, %r140, 4, 14;
	cvt.u64.u32 	%rd43, %r141;
	or.b64 	%rd33, %rd43, 4611686293338849280;
	add.s32 	%r142, %r127, 41056;
	bfe.u32 	%r143, %r142, 4, 14;
	cvt.u64.u32 	%rd44, %r143;
	or.b64 	%rd34, %rd44, 4611686293338849280;
	// begin inline asm
	@%p26 tcgen05.mma.cta_group::1.kind::f16 [ %r82 + 0 ], %rd33, %rd34, %r115, %p25;
	// end inline asm
	cvt.u64.u32 	%rd35, %r125;
	// begin inline asm
	@%p26 tcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [%rd35];
	// end inline asm

// 
// 1. MMA tile_m and tile_n (64x64xf16)
// 2. Satisfy Consumer mbarriers %r166-169
// 


	and.pred 	%p34, %p35, %p26;
// %p35 = true if k_tile_counter == 1 (the final (tile_m, tile_n) was consumed)
// %p26 = true for the leader thread
// %p34 = true if k_tile_counter == 1 and (is leader)

	// begin inline asm
	@%p34 tcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [%rd18];
	// end inline asm
// Usually, this will be skipped
// Satisfy the final mbarrier %r180 if k_tile_counter == 1 (the final (tile_m, tile_n) was consumed)

	.loc	1 0 0                           // matmul_tma_kernel.py:0
	add.s32 	%r145, %r267, 1;
// %r267 = stage_counter (num_stages = 5)
// %r145 = %r267 + 1
// %r145 = stage_counter + 1

	setp.eq.b32 	%p36, %r145, 5;
// %p36 = true if (stage_counter + 1) == 5

	selp.b32 	%r267, 0, %r145, %p36;
// Update stage_counter (%r267)
// If (stage_counter + 1) < 5:
//   stage_counter = stage_counter + 1
// If stage_counter == 5:
//   stage_counter = 0

	selp.b32 	%r146, 1, 0, %p36;
	xor.b32 	%r266, %r266, %r146;
// Update parity phase (%r266)

	.loc	1 74 31                         // matmul_tma_kernel.py:74:31
	add.s32 	%r265, %r265, -1;
// Update k_tile_counter
// k_tile_counter = k_tile_counter - 1

	setp.ne.b32 	%p37, %r265, 0;
// %p37 = true if k_tile_counter > 0
// %p37 = false if k_tile_counter == 0

	@%p37 bra 	$L__BB0_8;
// Warp 10 jumps back to MMA Loop Head


// Warp 10 falls here if k_tiles have been consumed
$L__BB0_9:                              // %._crit_edge4
                                        //   in Loop: Header=BB0_2 Depth=1
	barrier.sync 	1;
// Warp 10 arrives the 2nd CTA sync point

	setmaxnreg.inc.sync.aligned.u32 	24;

	bra.uni 	$L__BB0_2;
// Warp 10 jumps to `$L__BB0_2`


// ========================================================================================
// Warp 8-9 jump here
// ========================================================================================
$L__BB0_10:                             //   in Loop: Header=BB0_2 Depth=1
	.loc	1 74 31                         // matmul_tma_kernel.py:74:31
	setmaxnreg.inc.sync.aligned.u32 	24;
	barrier.sync 	1;
// Warp 8-9 arrive at 2nd CTA sync point

$L__tmp6:
	.loc	2 41 22                         // standard.py:41:22 @[ matmul_tma_kernel.py:69:25 ]
	add.s32 	%r51, %r26, 63;
// %r51 = K + 64 - 1

$L__tmp7:
	.loc	1 74 31                         // matmul_tma_kernel.py:74:31
	setp.lt.s32 	%p3, %r51, 64;
// %p3 = true if K <= 0
// %p3 = false if K > 0 (This is normal case)

	@%p3 bra 	$L__BB0_13;
// In normal case, this will be skipped

// %bb.11:                              // %.lr.ph
                                        //   in Loop: Header=BB0_2 Depth=1
	.loc	1 0 31                          // matmul_tma_kernel.py:0:31
	mov.u32 	%r30, %ctaid.x;
// %r30 = blockIdx.x = pid

	add.s32 	%r31, %r24, 63;
	shr.s32 	%r32, %r31, 31;
	shr.u32 	%r33, %r32, 26;
	add.s32 	%r34, %r31, %r33;
	shr.s32 	%r35, %r34, 6;
// %r35 = num_pid_m

	add.s32 	%r36, %r25, 63;
	shr.s32 	%r37, %r36, 31;
	shr.u32 	%r38, %r37, 26;
	add.s32 	%r39, %r36, %r38;
	shr.s32 	%r40, %r39, 6;
// %r40 = num_pid_n

	shl.b32 	%r41, %r40, 3;
// %r41 = num_pid_n * 8 = num_pid_in_group

	div.s32 	%r42, %r30, %r41;
// %r42 = pid // num_pid_in_group = group_id

	shl.b32 	%r43, %r42, 3;
// %r43 = group_id * 8 = first_pid_m

	sub.s32 	%r44, %r35, %r43;
// %r44 = num_pid_m - first_pid_m

	min.s32 	%r45, %r44, 8;
// group_m = min(num_pid_m - first_pid_m, 8)
// %r45 = group_m

	mul.lo.s32 	%r46, %r42, %r41;
// %r46 = group_id * num_pid_in_group

	sub.s32 	%r47, %r30, %r46;
// %r47 = pid - group_id * num_pid_in_group

	div.s32 	%r48, %r47, %r45;
// %r48 = (pid - group_id * num_pid_in_group) // group_m = pid_n
// %r48 = pid_n

	shl.b32 	%r65, %r48, 6;
// offs_bn = pid_n * 64
// %r65 = offs_bn

	rem.s32 	%r49, %r30, %r45;
// %r49 = pid % group_m

	add.s32 	%r50, %r49, %r43;
// pid_m = first_pid_m + (pid % group_m)
// %r50 = pid_m

	shl.b32 	%r61, %r50, 6;
// offs_am = pid_m * 64
// %r61 = offs_am

	shr.s32 	%r52, %r51, 31;
	shr.u32 	%r53, %r52, 26;
	add.s32 	%r54, %r51, %r53;
	shr.s32 	%r269, %r54, 6;
// k_tiles = (K + 64 - 1) // 64
// %r269 = k_tiles

	add.s32 	%r15, %r1, -256;
// %r15 = tid - 256

	mov.b32 	%r268, 0;
	mov.b32 	%r270, %r268;
	mov.b32 	%r271, %r268;
// Init
//   %r268 = 0
//   %r270 = 0
//   %r271 = 0

//---------------------------------------------------------------------------------------
// Warp 8-9 enter Copy Loop
//---------------------------------------------------------------------------------------
$L__BB0_12:                             //   Parent Loop BB0_2 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	setp.lt.u32 	%p7, %r15, 32;
// warp 8:
//   %p7 = true because %r15 < 32
// warp 9:
//   %p7 = false because %r15 >= 32

	setp.eq.b32 	%p4, %r15, 0;
// %p4 = true for the first tid of warp 8

	.loc	1 74 31                         // matmul_tma_kernel.py:74:31
	shl.b32 	%r67, %r271, 3;
// %r271 is stage_counter
// Iteration 0:
//   %r67 = 0 * 8
// Iteration 1:
//   %r67 = 1 * 8
// Iteration 2:
//   %r67 = 2 * 8
// Iteration 3:
//   %r67 = 3 * 8
// Iteration 4:
//   %r67 = 4 * 8
// -----------------------------
// Next cycle
// -----------------------------
// Iteration 0:
//   %r67 = 0 * 8

	add.s32 	%r69, %r28, %r67;
// Iteration 0:
//   %r69 = global_smem + 0
// Iteration 1:
//   %r69 = global_smem + 1 * 8
// Iteration 2:
//   %r69 = global_smem + 2 * 8
// Iteration 3:
//   %r69 = global_smem + 3 * 8
// Iteration 4:
//   %r69 = global_smem + 4 * 8
// -----------------------------
// Next cycle
// -----------------------------
// Iteration 0:
//   %r69 = global_smem + 0

	add.s32 	%r56, %r69, 82048;
// Iteration 0:
//   %r56 = global_smem + 82048 = %r165
// Iteration 1:
//   %r56 = global_smem + 1 * 8 + 82048 = %r166
// Iteration 2:
//   %r56 = global_smem + 2 * 8 + 82048 = %r167
// Iteration 3:
//   %r56 = global_smem + 3 * 8 + 82048 = %r168
// Iteration 4:
//   %r56 = global_smem + 4 * 8 + 82048 = %r169
// -----------------------------
// Next cycle
// -----------------------------
// Iteration 0:
//   %r56 = global_smem + 82048 = %r165

	.loc	1 0 0                           // matmul_tma_kernel.py:0
	// begin inline asm
{
	.reg .pred complete;
	waitLoop:
	mbarrier.try_wait.parity.shared.b64 complete, [%r56], %r270;
	@!complete bra.uni waitLoop;
}
	// end inline asm
// Warp 8-9 wait for Consumer mbarriers %r165-169 to be satisfied
// %r165-169 are primed for the first time
// 
// Next cycle
// Warp 8-9 wait for Consumer mbarriers %r165-169 to be satisfied

	.loc	1 74 31                         // matmul_tma_kernel.py:74:31
	add.s32 	%r62, %r69, 82096;
// Iteration 0:
//   %r62 = global_smem + 0 + 82096 = %r170
// Iteration 1:
//   %r62 = global_smem + 8 + 82096 = %r171

	.loc	1 0 0                           // matmul_tma_kernel.py:0
	bar.sync 	3, 64;
// Sync 64 threads of warp 8-9

	// begin inline asm
	@%p4 mbarrier.arrive.expect_tx.shared.b64 _, [%r62], 16384;
	// end inline asm
// Expect 2*64*64*2 bytes (2 tiles of 64x64xf16 for A and B)

	.loc	1 76 24                         // matmul_tma_kernel.py:76:24
	shl.b32 	%r70, %r271, 13;
// 2^13 = 8192 = 64*64*2
// Iteration 0:
//   %r70 = 0 * 2^13 = 0 * 8192
// Iteration 1:
//   %r70 = 1 * 2^13 = 1 * 8192

	add.s32 	%r59, %r28, %r70;
// Iteration 0:
//   %r59 = global_smem + 8192 * 0 (the 1st tile_m)
// Iteration 1:
//   %r59 = global_smem + 8192 * 1 (the 2nd tile_m)

	.loc	1 0 0                           // matmul_tma_kernel.py:0
	bar.sync 	3, 64;
// Sync 64 threads of warp 8-9

	elect.sync 	%r71|%p8, -1;
// %p8 = true if tid is leader

	and.pred 	%p5, %p7, %p8;
// %p5 = true if (tid is in warp 8) and (tid is leader)

	// begin inline asm
	@%p5 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes [%r59], [%rd7, {%r268, %r61}], [%r62];
	// end inline asm
// Only the leader thread of warp 8 executes this
// Iteration 0:
//   TMA a_tensor_map[0, offs_am] -> global_smem
// Iteration 1:
//   TMA a_tensor_map[64, offs_am] -> global_smem

	.loc	1 77 24                         // matmul_tma_kernel.py:77:24
	add.s32 	%r63, %r59, 40960;
// Iteration 0:
//   %r63 = global_smem + 40960 + 8192 * 0
// Iteration 0:
//   %r63 = global_smem + 40960 + 8192 * 1

	.loc	1 0 0                           // matmul_tma_kernel.py:0
	bar.sync 	3, 64;
	elect.sync 	%r72|%p9, -1;
	and.pred 	%p6, %p7, %p9;
// %p6 = true if (tid is in warp 8) and (tid is leader)

	// begin inline asm
	@%p6 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes [%r63], [%rd8, {%r268, %r65}], [%r62];
	// end inline asm
// Only the leader thread of warp 8 executes this
// Iteration 0:
//   TMA b_tensor_map[0, offs_bn] -> global_smem + 40960
//   Satisfy Producer mbarrier %r170
// Iteration 1:
//   TMA b_tensor_map[64, offs_bn] -> global_smem + 40960 + 8192 * 1
//   Satisfy Producer mbarrier %r171
// Iteration 2:
//   TMA b_tensor_map[128, offs_bn] -> global_smem + 40960 + 8192 * 2
//   Satisfy Producer mbarrier %r172
// Iteration 3:
//   TMA b_tensor_map[192, offs_bn] -> global_smem + 40960 + 8192 * 3
//   Satisfy Producer mbarrier %r173
// Iteration 4:
//   TMA b_tensor_map[256, offs_bn] -> global_smem + 40960 + 8192 * 4
//   Satisfy Producer mbarrier %r174

	add.s32 	%r73, %r271, 1;
// Iteration 0:
//   %r73 = 0 + 1 = 1
// Iteration 1:
//   %r73 = 1 + 1 = 2
// Iteration 2:
//   %r73 = 2 + 1 = 3
// Iteration 3:
//   %r73 = 3 + 1 = 4
// Iteration 4:
//   %r73 = 4 + 1 = 5

	setp.eq.b32 	%p10, %r73, 5;
// Iteration 0-3:
//   %p10 = false because %r73 < 5
// Iteration 4:
//   %p10 = true because %r73 == 5

	selp.b32 	%r271, 0, %r73, %p10;
// -------------------------------
// Update stage_counter
// -------------------------------
// %r271 acts like stage_counter (where, num_stages = 5)
// Iteration 0:
//   %r271 = %r73 = 1
// Iteration 1:
//   %r271 = %r73 = 2
// Iteration 2:
//   %r271 = %r73 = 3
// Iteration 3:
//   %r271 = %r73 = 4
// Iteration 4:
//   %r271 = 0

	selp.b32 	%r74, 1, 0, %p10;
// Iteration 0-3:
//   %r74 = 0
// Iteration 4:
//   %r74 = 1

	xor.b32 	%r270, %r270, %r74;
// Update parity phase
// Iteration 0-3:
//   %r270 = 0
// Iteration 4:
//   %r270 = 1

	.loc	1 74 31                         // matmul_tma_kernel.py:74:31
	add.s32 	%r269, %r269, -1;
// --------------------------------
// Update k_tile_counter
// --------------------------------
// Initially, %r269 = k_tiles
// %r269 = k_tile_counter
// k_tile_counter = k_tile_counter - 1

	add.s32 	%r268, %r268, 64;
// --------------------------------
// Update column index for TMA
// --------------------------------
// Initially, %r268 = 0
// %r268 = %r268 + 64 = 64


	setp.ne.b32 	%p11, %r269, 0;
// %p11 = true if k_tile_counter > 0
// %p11 = false if k_tile_counter == 0

	@%p11 bra 	$L__BB0_12;
// Warp 8-9 jump back to Copy Engine Loop Head


// Warp 8-9 fall here if k_tiles have been copied
$L__BB0_13:                             // %._crit_edge
                                        //   in Loop: Header=BB0_2 Depth=1
	barrier.sync 	1;
// Warp 8-9 arrive the 2nd CTA sync point

	setmaxnreg.inc.sync.aligned.u32 	24;
	bra.uni 	$L__BB0_2;
// Warp 8-9 jump to `$L__BB0_2`

$L__BB0_4:                              //   in Loop: Header=BB0_2 Depth=1
	.loc	1 51 0                          // matmul_tma_kernel.py:51
	barrier.sync 	1;
	barrier.sync 	1;
	bra.uni 	$L__BB0_2;
$L__tmp8:
$L__func_end0:
                                        // -- End function
}
	.file	1 "/data03/home/son.nguyen/workspace/triton.cpp/matmul_tma_kernel.py"
	.file	2 "/data03/home/son.nguyen/.pyenv/versions/3.11.2/lib/python3.11/site-packages/triton/language/standard.py"
	.section	.debug_abbrev
	{
.b8 1                                   // Abbreviation Code
.b8 17                                  // DW_TAG_compile_unit
.b8 1                                   // DW_CHILDREN_yes
.b8 37                                  // DW_AT_producer
.b8 8                                   // DW_FORM_string
.b8 19                                  // DW_AT_language
.b8 5                                   // DW_FORM_data2
.b8 3                                   // DW_AT_name
.b8 8                                   // DW_FORM_string
.b8 16                                  // DW_AT_stmt_list
.b8 6                                   // DW_FORM_data4
.b8 27                                  // DW_AT_comp_dir
.b8 8                                   // DW_FORM_string
.b8 0                                   // EOM(1)
.b8 0                                   // EOM(2)
.b8 2                                   // Abbreviation Code
.b8 46                                  // DW_TAG_subprogram
.b8 0                                   // DW_CHILDREN_no
.b8 3                                   // DW_AT_name
.b8 8                                   // DW_FORM_string
.b8 32                                  // DW_AT_inline
.b8 11                                  // DW_FORM_data1
.b8 0                                   // EOM(1)
.b8 0                                   // EOM(2)
.b8 3                                   // Abbreviation Code
.b8 46                                  // DW_TAG_subprogram
.b8 1                                   // DW_CHILDREN_yes
.b8 17                                  // DW_AT_low_pc
.b8 1                                   // DW_FORM_addr
.b8 18                                  // DW_AT_high_pc
.b8 1                                   // DW_FORM_addr
.b8 49                                  // DW_AT_abstract_origin
.b8 19                                  // DW_FORM_ref4
.b8 0                                   // EOM(1)
.b8 0                                   // EOM(2)
.b8 4                                   // Abbreviation Code
.b8 29                                  // DW_TAG_inlined_subroutine
.b8 0                                   // DW_CHILDREN_no
.b8 49                                  // DW_AT_abstract_origin
.b8 19                                  // DW_FORM_ref4
.b8 17                                  // DW_AT_low_pc
.b8 1                                   // DW_FORM_addr
.b8 18                                  // DW_AT_high_pc
.b8 1                                   // DW_FORM_addr
.b8 88                                  // DW_AT_call_file
.b8 11                                  // DW_FORM_data1
.b8 89                                  // DW_AT_call_line
.b8 11                                  // DW_FORM_data1
.b8 87                                  // DW_AT_call_column
.b8 11                                  // DW_FORM_data1
.b8 0                                   // EOM(1)
.b8 0                                   // EOM(2)
.b8 0                                   // EOM(3)
	}
	.section	.debug_info
	{
.b32 202                                // Length of Unit
.b8 2                                   // DWARF version number
.b8 0
.b32 .debug_abbrev                      // Offset Into Abbrev. Section
.b8 8                                   // Address Size (in bytes)
.b8 1                                   // Abbrev [1] 0xb:0xc3 DW_TAG_compile_unit
.b8 116                                 // DW_AT_producer
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2                                   // DW_AT_language
.b8 0
.b8 109                                 // DW_AT_name
.b8 97
.b8 116
.b8 109
.b8 117
.b8 108
.b8 95
.b8 116
.b8 109
.b8 97
.b8 95
.b8 107
.b8 101
.b8 114
.b8 110
.b8 101
.b8 108
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line                        // DW_AT_stmt_list
.b8 47                                  // DW_AT_comp_dir
.b8 100
.b8 97
.b8 116
.b8 97
.b8 48
.b8 51
.b8 47
.b8 104
.b8 111
.b8 109
.b8 101
.b8 47
.b8 115
.b8 111
.b8 110
.b8 46
.b8 110
.b8 103
.b8 117
.b8 121
.b8 101
.b8 110
.b8 47
.b8 119
.b8 111
.b8 114
.b8 107
.b8 115
.b8 112
.b8 97
.b8 99
.b8 101
.b8 47
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 46
.b8 99
.b8 112
.b8 112
.b8 0
.b8 2                                   // Abbrev [2] 0x5b:0x14 DW_TAG_subprogram
.b8 109                                 // DW_AT_name
.b8 97
.b8 116
.b8 109
.b8 117
.b8 108
.b8 95
.b8 107
.b8 101
.b8 114
.b8 110
.b8 101
.b8 108
.b8 95
.b8 116
.b8 109
.b8 97
.b8 0
.b8 1                                   // DW_AT_inline
.b8 3                                   // Abbrev [3] 0x6f:0x5e DW_TAG_subprogram
.b64 $L__func_begin0                    // DW_AT_low_pc
.b64 $L__func_end0                      // DW_AT_high_pc
.b32 91                                 // DW_AT_abstract_origin
.b8 4                                   // Abbrev [4] 0x84:0x18 DW_TAG_inlined_subroutine
.b32 91                                 // DW_AT_abstract_origin
.b64 $L__tmp1                           // DW_AT_low_pc
.b64 $L__tmp2                           // DW_AT_high_pc
.b8 1                                   // DW_AT_call_file
.b8 60                                  // DW_AT_call_line
.b8 27                                  // DW_AT_call_column
.b8 4                                   // Abbrev [4] 0x9c:0x18 DW_TAG_inlined_subroutine
.b32 91                                 // DW_AT_abstract_origin
.b64 $L__tmp2                           // DW_AT_low_pc
.b64 $L__tmp3                           // DW_AT_high_pc
.b8 1                                   // DW_AT_call_file
.b8 61                                  // DW_AT_call_line
.b8 27                                  // DW_AT_call_column
.b8 4                                   // Abbrev [4] 0xb4:0x18 DW_TAG_inlined_subroutine
.b32 91                                 // DW_AT_abstract_origin
.b64 $L__tmp4                           // DW_AT_low_pc
.b64 $L__tmp7                           // DW_AT_high_pc
.b8 1                                   // DW_AT_call_file
.b8 69                                  // DW_AT_call_line
.b8 25                                  // DW_AT_call_column
.b8 0                                   // End Of Children Mark
.b8 0                                   // End Of Children Mark
	}
	.section	.debug_macinfo	{	}
```