//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35404655
// Cuda compilation tools, release 12.8, V12.8.61
// Based on NVVM 20.0.0
//

.version 8.7
.target sm_100a
.address_size 64

	// .globl	blackwell_mmav5_bf16_fp32
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std3__45__cpo5beginE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std3__45__cpo3endE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std3__45__cpo6cbeginE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std3__45__cpo4cendE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std3__45__cpo6rbeginE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std3__45__cpo4rendE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std3__45__cpo7crbeginE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std3__45__cpo5crendE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std3__450_GLOBAL__N__5b744765_17_mmav5_with_tma_cu_5c1f3a496ignoreE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std3__419piecewise_constructE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std3__48in_placeE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std3__47nulloptE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std3__420unreachable_sentinelE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std6ranges3__45__cpo4swapE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std6ranges3__45__cpo9iter_moveE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std6ranges3__45__cpo7advanceE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std6ranges3__45__cpo5beginE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std6ranges3__45__cpo3endE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std6ranges3__45__cpo6cbeginE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std6ranges3__45__cpo4cendE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std6ranges3__45__cpo9iter_swapE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std6ranges3__45__cpo4nextE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std6ranges3__45__cpo4prevE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std6ranges3__45__cpo4dataE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std6ranges3__45__cpo5cdataE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std6ranges3__45__cpo4sizeE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std6ranges3__45__cpo5ssizeE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a494cuda3std6ranges3__45__cpo8distanceE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a496thrust21THRUST_200700_1000_NS8cuda_cub3parE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a496thrust21THRUST_200700_1000_NS8cuda_cub10par_nosyncE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a496thrust21THRUST_200700_1000_NS6system6detail10sequential3seqE[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a496thrust21THRUST_200700_1000_NS12placeholders2_1E[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a496thrust21THRUST_200700_1000_NS12placeholders2_2E[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a496thrust21THRUST_200700_1000_NS12placeholders2_3E[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a496thrust21THRUST_200700_1000_NS12placeholders2_4E[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a496thrust21THRUST_200700_1000_NS12placeholders2_5E[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a496thrust21THRUST_200700_1000_NS12placeholders2_6E[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a496thrust21THRUST_200700_1000_NS12placeholders2_7E[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a496thrust21THRUST_200700_1000_NS12placeholders2_8E[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a496thrust21THRUST_200700_1000_NS12placeholders2_9E[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a496thrust21THRUST_200700_1000_NS12placeholders3_10E[1];
.global .align 1 .b8 _ZN48_INTERNAL_5b744765_17_mmav5_with_tma_cu_5c1f3a496thrust21THRUST_200700_1000_NS3seqE[1];
.extern .shared .align 16 .b8 global_smem[];

.visible .entry blackwell_mmav5_bf16_fp32(
	.param .align 64 .b8 blackwell_mmav5_bf16_fp32_param_0[128],
	.param .align 64 .b8 blackwell_mmav5_bf16_fp32_param_1[128],
	.param .align 64 .b8 blackwell_mmav5_bf16_fp32_param_2[128],
	.param .u32 blackwell_mmav5_bf16_fp32_param_3,
	.param .u32 blackwell_mmav5_bf16_fp32_param_4,
	.param .u32 blackwell_mmav5_bf16_fp32_param_5
)
{
	.reg .pred 	%p<29>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<353>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<45>;

	mov.b64 	%rd3, blackwell_mmav5_bf16_fp32_param_0;
	mov.b64 	%rd4, blackwell_mmav5_bf16_fp32_param_1;
	mov.b64 	%rd5, blackwell_mmav5_bf16_fp32_param_2;
	ld.param.u32 	%r26, [blackwell_mmav5_bf16_fp32_param_3];
	ld.param.u32 	%r28, [blackwell_mmav5_bf16_fp32_param_4];
	ld.param.u32 	%r25, [blackwell_mmav5_bf16_fp32_param_5];
	mov.b32 	%r30, 0;
	st.shared.u32 	[global_smem+98392], %r30;
	mov.u32 	%r31, %ctaid.y;
	mov.u32 	%r33, %ctaid.x;
	shl.b32 	%r1, %r31, 6;
	shl.b32 	%r2, %r33, 6;
	setp.lt.s32 	%p1, %r25, 1;
	add.s32 	%r35, %r1, 64;
	setp.gt.s32 	%p2, %r35, %r26;
	add.s32 	%r36, %r2, 64;
	setp.gt.s32 	%p3, %r36, %r28;
	or.pred  	%p4, %p2, %p3;
	or.pred  	%p5, %p4, %p1;
	@%p5 bra 	$L__BB0_13;
	add.s32 	%r37, %r25, 63;
	shr.u32 	%r346, %r37, 6;
	mov.u32 	%r4, %tid.x;
	shr.u32 	%r5, %r4, 5;
	shfl.sync.idx.b32	%r6|%p6, %r5, 0, 31, -1;
	bar.sync 	0;
	setp.gt.s32 	%p7, %r6, 7;
	@%p7 bra 	$L__BB0_3;
	setp.eq.s32 	%p22, %r4, 0;
	selp.u32 	%r215, 1, 0, %p22;
	setp.eq.s32 	%p23, %r6, 0;
	selp.u32 	%r230, 1, 0, %p23;
	// begin inline asm
	
	bar.sync	0, 256;

	// end inline asm
	mov.u32 	%r232, global_smem;
	add.s32 	%r168, %r232, 98392;
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r230, 1;
		@%p tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [%r168], 64;
	}

	// end inline asm
	// begin inline asm
	
	bar.sync	0, 256;

	// end inline asm
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r230, 1;
		@%p tcgen05.relinquish_alloc_permit.cta_group::1.sync.aligned;
	}

	// end inline asm
	mov.b32 	%r171, -1;
	mov.b32 	%r219, 0;
	mov.u32 	%r170, %r219;
	// begin inline asm
	
	{
		.reg .pred p;
		elect.sync _|p, %r171;
		@p mov.s32 %r170, 1;
	}

	// end inline asm
	ld.shared.u32 	%r233, [global_smem+98392];
	shl.b32 	%r234, %r6, 21;
	and.b32  	%r235, %r234, 6291456;
	shl.b32 	%r236, %r6, 3;
	and.b32  	%r237, %r236, 32;
	or.b32  	%r238, %r237, %r235;
	add.s32 	%r231, %r238, %r233;
	// begin inline asm
	
	bar.sync	0, 256;

	// end inline asm
	// begin inline asm
	
	tcgen05.st.sync.aligned.16x32bx2.x16.b32 [%r231 + 0], 16, {%r219, %r219, %r219, %r219, %r219, %r219, %r219, %r219, %r219, %r219, %r219, %r219, %r219, %r219, %r219, %r219};

	// end inline asm
	// begin inline asm
	
	tcgen05.wait::st.sync.aligned;

	// end inline asm
	add.s32 	%r206, %r232, 98304;
	mov.b32 	%r217, 1;
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r215, 1;
		@%p mbarrier.init.shared::cta.b64 [%r206], %r217;
	}

	// end inline asm
	add.s32 	%r208, %r232, 98312;
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r215, 1;
		@%p mbarrier.init.shared::cta.b64 [%r208], %r217;
	}

	// end inline asm
	add.s32 	%r210, %r232, 98320;
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r215, 1;
		@%p mbarrier.init.shared::cta.b64 [%r210], %r217;
	}

	// end inline asm
	add.s32 	%r212, %r232, 98328;
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r215, 1;
		@%p mbarrier.init.shared::cta.b64 [%r212], %r217;
	}

	// end inline asm
	add.s32 	%r214, %r232, 98336;
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r215, 1;
		@%p mbarrier.init.shared::cta.b64 [%r214], %r217;
	}

	// end inline asm
	add.s32 	%r191, %r232, 98344;
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r215, 1;
		@%p mbarrier.init.shared::cta.b64 [%r191], %r217;
	}

	// end inline asm
	add.s32 	%r194, %r232, 98352;
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r215, 1;
		@%p mbarrier.init.shared::cta.b64 [%r194], %r217;
	}

	// end inline asm
	add.s32 	%r197, %r232, 98360;
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r215, 1;
		@%p mbarrier.init.shared::cta.b64 [%r197], %r217;
	}

	// end inline asm
	add.s32 	%r200, %r232, 98368;
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r215, 1;
		@%p mbarrier.init.shared::cta.b64 [%r200], %r217;
	}

	// end inline asm
	add.s32 	%r203, %r232, 98376;
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r215, 1;
		@%p mbarrier.init.shared::cta.b64 [%r203], %r217;
	}

	// end inline asm
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r215, 1;
		@%p mbarrier.arrive.shared::cta.b64 _, [%r206];
	}

	// end inline asm
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r215, 1;
		@%p mbarrier.arrive.shared::cta.b64 _, [%r208];
	}

	// end inline asm
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r215, 1;
		@%p mbarrier.arrive.shared::cta.b64 _, [%r210];
	}

	// end inline asm
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r215, 1;
		@%p mbarrier.arrive.shared::cta.b64 _, [%r212];
	}

	// end inline asm
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r215, 1;
		@%p mbarrier.arrive.shared::cta.b64 _, [%r214];
	}

	// end inline asm
	add.s32 	%r218, %r232, 98384;
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r215, 1;
		@%p mbarrier.init.shared::cta.b64 [%r218], %r217;
	}

	// end inline asm
	// begin inline asm
	
	barrier.sync	1;

	// end inline asm
	// begin inline asm
	
	barrier.sync	1;

	// end inline asm
	mov.b32 	%r220, 10000000;
	// begin inline asm
	
	{
		.reg .pred complete;
		LAB_WAIT:
			mbarrier.try_wait.parity.shared.b64 complete, [%r218], %r219, %r220;
			@!complete bra.uni LAB_WAIT;
	}

	// end inline asm
	// begin inline asm
	
	bar.sync	0, 256;

	// end inline asm
	// begin inline asm
	
	tcgen05.ld.sync.aligned.16x32bx2.x16.b32 {%f1, %f2, %f3, %f4, %f5, %f6, %f7, %f8, %f9, %f10, %f11, %f12, %f13, %f14, %f15, %f16}, [%r231 + 0], 16;

	// end inline asm
	// begin inline asm
	
	tcgen05.wait::ld.sync.aligned;

	// end inline asm
	// begin inline asm
	
	bar.sync	0, 256;

	// end inline asm
	and.b32  	%r239, %r4, 16;
	shl.b32 	%r240, %r5, 9;
	and.b32  	%r241, %r240, 1536;
	shl.b32 	%r242, %r4, 5;
	and.b32  	%r243, %r242, 480;
	or.b32  	%r244, %r243, %r239;
	or.b32  	%r245, %r244, %r241;
	setp.gt.s32 	%p24, %r6, 3;
	shl.b32 	%r246, %r4, 2;
	and.b32  	%r247, %r246, 28;
	xor.b32  	%r248, %r247, %r245;
	selp.b32 	%r249, 2048, 0, %p24;
	or.b32  	%r250, %r248, %r249;
	shl.b32 	%r251, %r250, 2;
	add.s32 	%r225, %r232, 81920;
	add.s32 	%r252, %r225, %r251;
	st.shared.f32 	[%r252], %f1;
	shl.b32 	%r253, %r245, 2;
	or.b32  	%r254, %r253, 4;
	cvt.u16.u32 	%rs1, %r242;
	shr.u16 	%rs2, %rs1, 1;
	cvt.u32.u16 	%r255, %rs2;
	and.b32  	%r256, %r255, 112;
	xor.b32  	%r257, %r256, %r254;
	shr.u32 	%r258, %r257, 2;
	add.s32 	%r259, %r258, %r249;
	shl.b32 	%r260, %r259, 2;
	add.s32 	%r261, %r225, %r260;
	st.shared.f32 	[%r261], %f2;
	or.b32  	%r262, %r253, 8;
	xor.b32  	%r263, %r256, %r262;
	shr.u32 	%r264, %r263, 2;
	add.s32 	%r265, %r264, %r249;
	shl.b32 	%r266, %r265, 2;
	add.s32 	%r267, %r225, %r266;
	st.shared.f32 	[%r267], %f3;
	or.b32  	%r268, %r253, 12;
	xor.b32  	%r269, %r256, %r268;
	shr.u32 	%r270, %r269, 2;
	add.s32 	%r271, %r270, %r249;
	shl.b32 	%r272, %r271, 2;
	add.s32 	%r273, %r225, %r272;
	st.shared.f32 	[%r273], %f4;
	or.b32  	%r274, %r253, 16;
	xor.b32  	%r275, %r256, %r274;
	shr.u32 	%r276, %r275, 2;
	add.s32 	%r277, %r276, %r249;
	shl.b32 	%r278, %r277, 2;
	add.s32 	%r279, %r225, %r278;
	st.shared.f32 	[%r279], %f5;
	or.b32  	%r280, %r253, 20;
	xor.b32  	%r281, %r256, %r280;
	shr.u32 	%r282, %r281, 2;
	add.s32 	%r283, %r282, %r249;
	shl.b32 	%r284, %r283, 2;
	add.s32 	%r285, %r225, %r284;
	st.shared.f32 	[%r285], %f6;
	or.b32  	%r286, %r253, 24;
	xor.b32  	%r287, %r256, %r286;
	shr.u32 	%r288, %r287, 2;
	add.s32 	%r289, %r288, %r249;
	shl.b32 	%r290, %r289, 2;
	add.s32 	%r291, %r225, %r290;
	st.shared.f32 	[%r291], %f7;
	or.b32  	%r292, %r253, 28;
	xor.b32  	%r293, %r256, %r292;
	shr.u32 	%r294, %r293, 2;
	add.s32 	%r295, %r294, %r249;
	shl.b32 	%r296, %r295, 2;
	add.s32 	%r297, %r225, %r296;
	st.shared.f32 	[%r297], %f8;
	or.b32  	%r298, %r253, 32;
	xor.b32  	%r299, %r256, %r298;
	shr.u32 	%r300, %r299, 2;
	add.s32 	%r301, %r300, %r249;
	shl.b32 	%r302, %r301, 2;
	add.s32 	%r303, %r225, %r302;
	st.shared.f32 	[%r303], %f9;
	or.b32  	%r304, %r253, 36;
	xor.b32  	%r305, %r256, %r304;
	shr.u32 	%r306, %r305, 2;
	add.s32 	%r307, %r306, %r249;
	shl.b32 	%r308, %r307, 2;
	add.s32 	%r309, %r225, %r308;
	st.shared.f32 	[%r309], %f10;
	or.b32  	%r310, %r253, 40;
	xor.b32  	%r311, %r256, %r310;
	shr.u32 	%r312, %r311, 2;
	add.s32 	%r313, %r312, %r249;
	shl.b32 	%r314, %r313, 2;
	add.s32 	%r315, %r225, %r314;
	st.shared.f32 	[%r315], %f11;
	or.b32  	%r316, %r253, 44;
	xor.b32  	%r317, %r256, %r316;
	shr.u32 	%r318, %r317, 2;
	add.s32 	%r319, %r318, %r249;
	shl.b32 	%r320, %r319, 2;
	add.s32 	%r321, %r225, %r320;
	st.shared.f32 	[%r321], %f12;
	or.b32  	%r322, %r253, 48;
	xor.b32  	%r323, %r256, %r322;
	shr.u32 	%r324, %r323, 2;
	add.s32 	%r325, %r324, %r249;
	shl.b32 	%r326, %r325, 2;
	add.s32 	%r327, %r225, %r326;
	st.shared.f32 	[%r327], %f13;
	or.b32  	%r328, %r253, 52;
	xor.b32  	%r329, %r256, %r328;
	shr.u32 	%r330, %r329, 2;
	add.s32 	%r331, %r330, %r249;
	shl.b32 	%r332, %r331, 2;
	add.s32 	%r333, %r225, %r332;
	st.shared.f32 	[%r333], %f14;
	or.b32  	%r334, %r253, 56;
	xor.b32  	%r335, %r256, %r334;
	shr.u32 	%r336, %r335, 2;
	add.s32 	%r337, %r336, %r249;
	shl.b32 	%r338, %r337, 2;
	add.s32 	%r339, %r225, %r338;
	st.shared.f32 	[%r339], %f15;
	or.b32  	%r340, %r253, 60;
	xor.b32  	%r341, %r256, %r340;
	shr.u32 	%r342, %r341, 2;
	add.s32 	%r343, %r342, %r249;
	shl.b32 	%r344, %r343, 2;
	add.s32 	%r345, %r225, %r344;
	st.shared.f32 	[%r345], %f16;
	// begin inline asm
	
	bar.sync	0, 256;

	// end inline asm
	// begin inline asm
	
	fence.proxy.async.shared::cta;

	// end inline asm
	setp.ne.s32 	%p25, %r170, 0;
	and.pred  	%p26, %p23, %p25;
	selp.u32 	%r222, 1, 0, %p26;
	setp.eq.s32 	%p27, %r6, 1;
	and.pred  	%p28, %p27, %p25;
	selp.u32 	%r226, 1, 0, %p28;
	cvta.param.u64 	%rd43, %rd5;
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r222, 1;
		@%p cp.async.bulk.tensor.2d.global.shared::cta.bulk_group [%rd43, {%r2, %r1}], [%r225];
	}

	// end inline asm
	add.s32 	%r227, %r2, 32;
	add.s32 	%r229, %r232, 90112;
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r226, 1;
		@%p cp.async.bulk.tensor.2d.global.shared::cta.bulk_group [%rd43, {%r227, %r1}], [%r229];
	}

	// end inline asm
	// begin inline asm
	
	bar.sync	0, 256;

	// end inline asm
	// begin inline asm
	
	cp.async.bulk.commit_group;

	// end inline asm
	// begin inline asm
	
	cp.async.bulk.wait_group.read 0;

	// end inline asm
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r230, 1;
		@%p tcgen05.dealloc.cta_group::1.sync.aligned.b32 %r231, 64;
	}

	// end inline asm
	bra.uni 	$L__BB0_12;
$L__BB0_3:
	setp.eq.s32 	%p8, %r6, 9;
	@%p8 bra 	$L__BB0_8;
	setp.ne.s32 	%p9, %r6, 8;
	@%p9 bra 	$L__BB0_12;
	// begin inline asm
	
	barrier.sync	1;

	// end inline asm
	setp.eq.s32 	%p19, %r4, 256;
	selp.u32 	%r7, 1, 0, %p19;
	cvta.param.u64 	%rd1, %rd3;
	cvta.param.u64 	%rd2, %rd4;
	mov.b32 	%r347, 0;
	mov.u32 	%r162, global_smem;
	mov.u32 	%r348, %r347;
	mov.u32 	%r349, %r347;
$L__BB0_6:
	.pragma "nounroll";
	// begin inline asm
	
	bar.warp.sync	-1;

	// end inline asm
	shl.b32 	%r161, %r347, 3;
	add.s32 	%r163, %r162, %r161;
	add.s32 	%r145, %r163, 98304;
	mov.b32 	%r147, 10000000;
	// begin inline asm
	
	{
		.reg .pred complete;
		LAB_WAIT:
			mbarrier.try_wait.parity.shared.b64 complete, [%r145], %r348, %r147;
			@!complete bra.uni LAB_WAIT;
	}

	// end inline asm
	add.s32 	%r149, %r163, 98344;
	mov.b32 	%r150, 16384;
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r7, 1;
		@%p mbarrier.arrive.expect_tx.shared.b64 _, [%r149], %r150;
	}

	// end inline asm
	shl.b32 	%r164, %r347, 13;
	add.s32 	%r152, %r162, %r164;
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r7, 1;
		@%p cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes [%r152], [%rd1, {%r349, %r1}], [%r149];
	}

	// end inline asm
	add.s32 	%r157, %r152, 40960;
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r7, 1;
		@%p cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes [%r157], [%rd2, {%r349, %r2}], [%r149];
	}

	// end inline asm
	setp.eq.s32 	%p20, %r347, 4;
	selp.s32 	%r165, -1, 0, %p20;
	xor.b32  	%r348, %r348, %r165;
	add.s32 	%r166, %r347, 1;
	selp.b32 	%r347, 0, %r166, %p20;
	add.s32 	%r14, %r346, -1;
	add.s32 	%r349, %r349, 64;
	setp.gt.u32 	%p21, %r346, 1;
	mov.u32 	%r346, %r14;
	@%p21 bra 	$L__BB0_6;
	// begin inline asm
	
	barrier.sync	1;

	// end inline asm
	bra.uni 	$L__BB0_12;
$L__BB0_8:
	// begin inline asm
	
	barrier.sync	1;

	// end inline asm
	ld.shared.u32 	%r16, [global_smem+98392];
	mov.u32 	%r64, global_smem;
	// begin inline asm
	
	bar.warp.sync	-1;

	// end inline asm
	add.s32 	%r38, %r64, 98344;
	mov.b32 	%r45, 0;
	mov.b32 	%r40, 10000000;
	// begin inline asm
	
	{
		.reg .pred complete;
		LAB_WAIT:
			mbarrier.try_wait.parity.shared.b64 complete, [%r38], %r45, %r40;
			@!complete bra.uni LAB_WAIT;
	}

	// end inline asm
	mov.b32 	%r42, -1;
	mov.u32 	%r41, %r45;
	// begin inline asm
	
	{
		.reg .pred p;
		elect.sync _|p, %r42;
		@p mov.s32 %r41, 1;
	}

	// end inline asm
	shr.u32 	%r65, %r64, 4;
	and.b32  	%r66, %r65, 16383;
	cvt.u64.u32 	%rd14, %r66;
	or.b64  	%rd6, %rd14, 4611686293305294848;
	add.s32 	%r67, %r64, 40960;
	shr.u32 	%r68, %r67, 4;
	and.b32  	%r69, %r68, 16383;
	cvt.u64.u32 	%rd15, %r69;
	or.b64  	%rd7, %rd15, 4611686293305294848;
	mov.b32 	%r59, 68158608;
	// begin inline asm
	
	{
		.reg .pred %p1;
		.reg .pred %p2;
		setp.eq.u32	%p1, %r41, 1;
		setp.eq.u32	%p2, %r45, 1;
		@%p1 tcgen05.mma.cta_group::1.kind::f16 [ %r16 + 0 ], %rd6, %rd7, %r59, %p2;
	}

	// end inline asm
	add.s32 	%r70, %r64, 32;
	shr.u32 	%r71, %r70, 4;
	and.b32  	%r72, %r71, 16383;
	cvt.u64.u32 	%rd16, %r72;
	or.b64  	%rd8, %rd16, 4611686293305294848;
	add.s32 	%r73, %r64, 40992;
	shr.u32 	%r74, %r73, 4;
	and.b32  	%r75, %r74, 16383;
	cvt.u64.u32 	%rd17, %r75;
	or.b64  	%rd9, %rd17, 4611686293305294848;
	mov.b32 	%r57, 1;
	// begin inline asm
	
	{
		.reg .pred %p1;
		.reg .pred %p2;
		setp.eq.u32	%p1, %r41, 1;
		setp.eq.u32	%p2, %r57, 1;
		@%p1 tcgen05.mma.cta_group::1.kind::f16 [ %r16 + 0 ], %rd8, %rd9, %r59, %p2;
	}

	// end inline asm
	add.s32 	%r76, %r64, 64;
	shr.u32 	%r77, %r76, 4;
	and.b32  	%r78, %r77, 16383;
	cvt.u64.u32 	%rd18, %r78;
	or.b64  	%rd10, %rd18, 4611686293305294848;
	add.s32 	%r79, %r64, 41024;
	shr.u32 	%r80, %r79, 4;
	and.b32  	%r81, %r80, 16383;
	cvt.u64.u32 	%rd19, %r81;
	or.b64  	%rd11, %rd19, 4611686293305294848;
	// begin inline asm
	
	{
		.reg .pred %p1;
		.reg .pred %p2;
		setp.eq.u32	%p1, %r41, 1;
		setp.eq.u32	%p2, %r57, 1;
		@%p1 tcgen05.mma.cta_group::1.kind::f16 [ %r16 + 0 ], %rd10, %rd11, %r59, %p2;
	}

	// end inline asm
	add.s32 	%r82, %r64, 96;
	shr.u32 	%r83, %r82, 4;
	and.b32  	%r84, %r83, 16383;
	cvt.u64.u32 	%rd20, %r84;
	or.b64  	%rd12, %rd20, 4611686293305294848;
	add.s32 	%r85, %r64, 41056;
	shr.u32 	%r86, %r85, 4;
	and.b32  	%r87, %r86, 16383;
	cvt.u64.u32 	%rd21, %r87;
	or.b64  	%rd13, %rd21, 4611686293305294848;
	// begin inline asm
	
	{
		.reg .pred %p1;
		.reg .pred %p2;
		setp.eq.u32	%p1, %r41, 1;
		setp.eq.u32	%p2, %r57, 1;
		@%p1 tcgen05.mma.cta_group::1.kind::f16 [ %r16 + 0 ], %rd12, %rd13, %r59, %p2;
	}

	// end inline asm
	add.s32 	%r61, %r64, 98304;
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r41, 1;
		@%p tcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [%r61];
	}

	// end inline asm
	setp.eq.s32 	%p10, %r346, 1;
	setp.ne.s32 	%p11, %r41, 0;
	and.pred  	%p12, %p10, %p11;
	selp.u32 	%r62, 1, 0, %p12;
	add.s32 	%r63, %r64, 98384;
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r62, 1;
		@%p tcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [%r63];
	}

	// end inline asm
	setp.lt.u32 	%p13, %r25, 65;
	@%p13 bra 	$L__BB0_11;
	add.s32 	%r350, %r346, -2;
	mov.b32 	%r352, 1;
	mov.b32 	%r351, 0;
$L__BB0_10:
	.pragma "nounroll";
	setp.ne.s32 	%p14, %r41, 0;
	// begin inline asm
	
	bar.warp.sync	-1;

	// end inline asm
	shl.b32 	%r113, %r352, 3;
	add.s32 	%r115, %r64, %r113;
	add.s32 	%r90, %r115, 98344;
	mov.b32 	%r92, 10000000;
	// begin inline asm
	
	{
		.reg .pred complete;
		LAB_WAIT:
			mbarrier.try_wait.parity.shared.b64 complete, [%r90], %r351, %r92;
			@!complete bra.uni LAB_WAIT;
	}

	// end inline asm
	shl.b32 	%r116, %r352, 13;
	add.s32 	%r117, %r64, %r116;
	shr.u32 	%r118, %r117, 4;
	and.b32  	%r119, %r118, 16383;
	cvt.u64.u32 	%rd30, %r119;
	or.b64  	%rd22, %rd30, 4611686293305294848;
	add.s32 	%r120, %r117, 40960;
	shr.u32 	%r121, %r120, 4;
	and.b32  	%r122, %r121, 16383;
	cvt.u64.u32 	%rd31, %r122;
	or.b64  	%rd23, %rd31, 4611686293305294848;
	mov.b32 	%r106, 1;
	mov.b32 	%r108, 68158608;
	// begin inline asm
	
	{
		.reg .pred %p1;
		.reg .pred %p2;
		setp.eq.u32	%p1, %r41, 1;
		setp.eq.u32	%p2, %r106, 1;
		@%p1 tcgen05.mma.cta_group::1.kind::f16 [ %r16 + 0 ], %rd22, %rd23, %r108, %p2;
	}

	// end inline asm
	add.s32 	%r123, %r117, 32;
	shr.u32 	%r124, %r123, 4;
	and.b32  	%r125, %r124, 16383;
	cvt.u64.u32 	%rd32, %r125;
	or.b64  	%rd24, %rd32, 4611686293305294848;
	add.s32 	%r126, %r117, 40992;
	shr.u32 	%r127, %r126, 4;
	and.b32  	%r128, %r127, 16383;
	cvt.u64.u32 	%rd33, %r128;
	or.b64  	%rd25, %rd33, 4611686293305294848;
	// begin inline asm
	
	{
		.reg .pred %p1;
		.reg .pred %p2;
		setp.eq.u32	%p1, %r41, 1;
		setp.eq.u32	%p2, %r106, 1;
		@%p1 tcgen05.mma.cta_group::1.kind::f16 [ %r16 + 0 ], %rd24, %rd25, %r108, %p2;
	}

	// end inline asm
	add.s32 	%r129, %r117, 64;
	shr.u32 	%r130, %r129, 4;
	and.b32  	%r131, %r130, 16383;
	cvt.u64.u32 	%rd34, %r131;
	or.b64  	%rd26, %rd34, 4611686293305294848;
	add.s32 	%r132, %r117, 41024;
	shr.u32 	%r133, %r132, 4;
	and.b32  	%r134, %r133, 16383;
	cvt.u64.u32 	%rd35, %r134;
	or.b64  	%rd27, %rd35, 4611686293305294848;
	// begin inline asm
	
	{
		.reg .pred %p1;
		.reg .pred %p2;
		setp.eq.u32	%p1, %r41, 1;
		setp.eq.u32	%p2, %r106, 1;
		@%p1 tcgen05.mma.cta_group::1.kind::f16 [ %r16 + 0 ], %rd26, %rd27, %r108, %p2;
	}

	// end inline asm
	add.s32 	%r135, %r117, 96;
	shr.u32 	%r136, %r135, 4;
	and.b32  	%r137, %r136, 16383;
	cvt.u64.u32 	%rd36, %r137;
	or.b64  	%rd28, %rd36, 4611686293305294848;
	add.s32 	%r138, %r117, 41056;
	shr.u32 	%r139, %r138, 4;
	and.b32  	%r140, %r139, 16383;
	cvt.u64.u32 	%rd37, %r140;
	or.b64  	%rd29, %rd37, 4611686293305294848;
	// begin inline asm
	
	{
		.reg .pred %p1;
		.reg .pred %p2;
		setp.eq.u32	%p1, %r41, 1;
		setp.eq.u32	%p2, %r106, 1;
		@%p1 tcgen05.mma.cta_group::1.kind::f16 [ %r16 + 0 ], %rd28, %rd29, %r108, %p2;
	}

	// end inline asm
	add.s32 	%r110, %r115, 98304;
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r41, 1;
		@%p tcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [%r110];
	}

	// end inline asm
	setp.eq.s32 	%p15, %r350, 0;
	and.pred  	%p16, %p15, %p14;
	selp.u32 	%r111, 1, 0, %p16;
	// begin inline asm
	
	{
		.reg .pred %p;
		setp.eq.b32	%p, %r111, 1;
		@%p tcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [%r63];
	}

	// end inline asm
	setp.eq.s32 	%p17, %r352, 4;
	selp.s32 	%r141, -1, 0, %p17;
	xor.b32  	%r351, %r351, %r141;
	add.s32 	%r142, %r352, 1;
	selp.b32 	%r352, 0, %r142, %p17;
	add.s32 	%r24, %r350, -1;
	add.s32 	%r143, %r350, 1;
	setp.gt.u32 	%p18, %r143, 1;
	mov.u32 	%r350, %r24;
	@%p18 bra 	$L__BB0_10;
$L__BB0_11:
	// begin inline asm
	
	barrier.sync	1;

	// end inline asm
$L__BB0_12:
	// begin inline asm
	
	barrier.sync	1;

	// end inline asm
$L__BB0_13:
	ret;

}
	// .globl	_ZN3cub18CUB_200700_1000_NS11EmptyKernelIvEEvv
.visible .entry _ZN3cub18CUB_200700_1000_NS11EmptyKernelIvEEvv()
{


	ret;

}
	// .globl	_ZN3cub18CUB_200700_1000_NS6detail8for_each13static_kernelINS2_12policy_hub_t12policy_350_tEmN6thrust21THRUST_200700_1000_NS8cuda_cub20__uninitialized_fill7functorINS7_10device_ptrI13__nv_bfloat16EESC_EEEEvT0_T1_
.visible .entry _ZN3cub18CUB_200700_1000_NS6detail8for_each13static_kernelINS2_12policy_hub_t12policy_350_tEmN6thrust21THRUST_200700_1000_NS8cuda_cub20__uninitialized_fill7functorINS7_10device_ptrI13__nv_bfloat16EESC_EEEEvT0_T1_(
	.param .u64 _ZN3cub18CUB_200700_1000_NS6detail8for_each13static_kernelINS2_12policy_hub_t12policy_350_tEmN6thrust21THRUST_200700_1000_NS8cuda_cub20__uninitialized_fill7functorINS7_10device_ptrI13__nv_bfloat16EESC_EEEEvT0_T1__param_0,
	.param .align 8 .b8 _ZN3cub18CUB_200700_1000_NS6detail8for_each13static_kernelINS2_12policy_hub_t12policy_350_tEmN6thrust21THRUST_200700_1000_NS8cuda_cub20__uninitialized_fill7functorINS7_10device_ptrI13__nv_bfloat16EESC_EEEEvT0_T1__param_1[16]
)
.maxntid 256, 1, 1
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<16>;

	ld.param.u16 	%rs2, [_ZN3cub18CUB_200700_1000_NS6detail8for_each13static_kernelINS2_12policy_hub_t12policy_350_tEmN6thrust21THRUST_200700_1000_NS8cuda_cub20__uninitialized_fill7functorINS7_10device_ptrI13__nv_bfloat16EESC_EEEEvT0_T1__param_1+8];
	ld.param.u64 	%rd4, [_ZN3cub18CUB_200700_1000_NS6detail8for_each13static_kernelINS2_12policy_hub_t12policy_350_tEmN6thrust21THRUST_200700_1000_NS8cuda_cub20__uninitialized_fill7functorINS7_10device_ptrI13__nv_bfloat16EESC_EEEEvT0_T1__param_1];
	ld.param.u64 	%rd5, [_ZN3cub18CUB_200700_1000_NS6detail8for_each13static_kernelINS2_12policy_hub_t12policy_350_tEmN6thrust21THRUST_200700_1000_NS8cuda_cub20__uninitialized_fill7functorINS7_10device_ptrI13__nv_bfloat16EESC_EEEEvT0_T1__param_0];
	mov.u32 	%r7, %ctaid.x;
	mul.wide.u32 	%rd1, %r7, 512;
	sub.s64 	%rd2, %rd5, %rd1;
	setp.lt.u64 	%p1, %rd2, 512;
	@%p1 bra 	$L__BB2_2;
	mov.u32 	%r9, %tid.x;
	cvta.to.global.u64 	%rd11, %rd4;
	cvt.u64.u32 	%rd12, %r9;
	add.s64 	%rd13, %rd1, %rd12;
	add.s64 	%rd14, %rd13, %rd13;
	add.s64 	%rd15, %rd11, %rd14;
	st.global.u16 	[%rd15], %rs2;
	st.global.u16 	[%rd15+512], %rs2;
	bra.uni 	$L__BB2_6;
$L__BB2_2:
	cvta.to.global.u64 	%rd6, %rd4;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd7, %r1;
	setp.le.u64 	%p2, %rd2, %rd7;
	add.s64 	%rd8, %rd1, %rd7;
	add.s64 	%rd9, %rd8, %rd8;
	add.s64 	%rd3, %rd6, %rd9;
	@%p2 bra 	$L__BB2_4;
	st.global.u16 	[%rd3], %rs2;
$L__BB2_4:
	add.s32 	%r8, %r1, 256;
	cvt.u64.u32 	%rd10, %r8;
	setp.le.u64 	%p3, %rd2, %rd10;
	@%p3 bra 	$L__BB2_6;
	st.global.u16 	[%rd3+512], %rs2;
$L__BB2_6:
	ret;

}
	// .globl	_ZN3cub18CUB_200700_1000_NS6detail8for_each13static_kernelINS2_12policy_hub_t12policy_350_tEmN6thrust21THRUST_200700_1000_NS8cuda_cub20__uninitialized_fill7functorINS7_10device_ptrIfEEfEEEEvT0_T1_
.visible .entry _ZN3cub18CUB_200700_1000_NS6detail8for_each13static_kernelINS2_12policy_hub_t12policy_350_tEmN6thrust21THRUST_200700_1000_NS8cuda_cub20__uninitialized_fill7functorINS7_10device_ptrIfEEfEEEEvT0_T1_(
	.param .u64 _ZN3cub18CUB_200700_1000_NS6detail8for_each13static_kernelINS2_12policy_hub_t12policy_350_tEmN6thrust21THRUST_200700_1000_NS8cuda_cub20__uninitialized_fill7functorINS7_10device_ptrIfEEfEEEEvT0_T1__param_0,
	.param .align 8 .b8 _ZN3cub18CUB_200700_1000_NS6detail8for_each13static_kernelINS2_12policy_hub_t12policy_350_tEmN6thrust21THRUST_200700_1000_NS8cuda_cub20__uninitialized_fill7functorINS7_10device_ptrIfEEfEEEEvT0_T1__param_1[16]
)
.maxntid 256, 1, 1
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<5>;
	.reg .b32 	%r<10>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<16>;

	ld.param.f32 	%f2, [_ZN3cub18CUB_200700_1000_NS6detail8for_each13static_kernelINS2_12policy_hub_t12policy_350_tEmN6thrust21THRUST_200700_1000_NS8cuda_cub20__uninitialized_fill7functorINS7_10device_ptrIfEEfEEEEvT0_T1__param_1+8];
	ld.param.u64 	%rd4, [_ZN3cub18CUB_200700_1000_NS6detail8for_each13static_kernelINS2_12policy_hub_t12policy_350_tEmN6thrust21THRUST_200700_1000_NS8cuda_cub20__uninitialized_fill7functorINS7_10device_ptrIfEEfEEEEvT0_T1__param_1];
	ld.param.u64 	%rd5, [_ZN3cub18CUB_200700_1000_NS6detail8for_each13static_kernelINS2_12policy_hub_t12policy_350_tEmN6thrust21THRUST_200700_1000_NS8cuda_cub20__uninitialized_fill7functorINS7_10device_ptrIfEEfEEEEvT0_T1__param_0];
	mov.u32 	%r7, %ctaid.x;
	mul.wide.u32 	%rd1, %r7, 512;
	sub.s64 	%rd2, %rd5, %rd1;
	setp.lt.u64 	%p1, %rd2, 512;
	@%p1 bra 	$L__BB3_2;
	mov.u32 	%r9, %tid.x;
	cvta.to.global.u64 	%rd11, %rd4;
	cvt.u64.u32 	%rd12, %r9;
	add.s64 	%rd13, %rd1, %rd12;
	shl.b64 	%rd14, %rd13, 2;
	add.s64 	%rd15, %rd11, %rd14;
	st.global.f32 	[%rd15], %f2;
	st.global.f32 	[%rd15+1024], %f2;
	bra.uni 	$L__BB3_6;
$L__BB3_2:
	cvta.to.global.u64 	%rd6, %rd4;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd7, %r1;
	setp.le.u64 	%p2, %rd2, %rd7;
	add.s64 	%rd8, %rd1, %rd7;
	shl.b64 	%rd9, %rd8, 2;
	add.s64 	%rd3, %rd6, %rd9;
	@%p2 bra 	$L__BB3_4;
	st.global.f32 	[%rd3], %f2;
$L__BB3_4:
	add.s32 	%r8, %r1, 256;
	cvt.u64.u32 	%rd10, %r8;
	setp.le.u64 	%p3, %rd2, %rd10;
	@%p3 bra 	$L__BB3_6;
	st.global.f32 	[%rd3+1024], %f2;
$L__BB3_6:
	ret;

}
