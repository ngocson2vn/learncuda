<!DOCTYPE html> <html lang=en-US style><!--
 Page saved with SingleFile 
 url: https://research.colfax-intl.com/tutorial-hopper-tma/ 
 saved date: Wed Dec 04 2024 20:52:17 GMT+0800 (Singapore Standard Time)
--><meta charset=utf-8>
<meta name=viewport content="width=device-width, initial-scale=1">
<meta name=robots content=max-image-preview:large>
<style>img:is([sizes="auto"i],[sizes^="auto,"i]){contain-intrinsic-size:3000px 1500px}</style>
<meta name=dlm-version content=5.0.14><title>CUTLASS Tutorial: Mastering the NVIDIA® Tensor Memory Accelerator (TMA) – Colfax Research</title>
<link rel=alternate type=application/rss+xml title="Colfax Research » Feed" href=https://research.colfax-intl.com/feed/>
<link rel=alternate type=application/rss+xml title="Colfax Research » Comments Feed" href=https://research.colfax-intl.com/comments/feed/>
<link rel=alternate type=application/rss+xml title="Colfax Research » CUTLASS Tutorial: Mastering the NVIDIA® Tensor Memory Accelerator (TMA) Comments Feed" href=https://research.colfax-intl.com/tutorial-hopper-tma/feed/>
<style>@media only screen and (max-width:640px){}@media only screen and (max-width:640px){}@media only screen and (max-width:320px){}</style>
<style>.wp-block-image img{box-sizing:border-box;height:auto;max-width:100%;vertical-align:bottom}@media (prefers-reduced-motion:no-preference){}.wp-block-image :where(figcaption){margin-bottom:1em;margin-top:.5em}@supports ((-webkit-mask-image:none) or (mask-image:none)) or (-webkit-mask-image:none){.wp-block-image.is-style-circle-mask img{border-radius:0;-webkit-mask-image:url(data:image/svg+xml;utf8,<svg\ viewBox=\"0\ 0\ 100\ 100\"\ xmlns=\"http://www.w3.org/2000/svg\"><circle\ cx=\"50\"\ cy=\"50\"\ r=\"50\"\/><\/svg>);mask-image:url(data:image/svg+xml;utf8,<svg\ viewBox=\"0\ 0\ 100\ 100\"\ xmlns=\"http://www.w3.org/2000/svg\"><circle\ cx=\"50\"\ cy=\"50\"\ r=\"50\"\/><\/svg>);mask-mode:alpha;-webkit-mask-position:center;mask-position:center;-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:contain;mask-size:contain}}:root :where(.wp-block-image.is-style-rounded img,.wp-block-image .is-style-rounded img){border-radius:9999px}@media (prefers-reduced-motion:no-preference){}@keyframes show-content-image{0%{visibility:hidden}99%{visibility:hidden}to{visibility:visible}}@keyframes turn-on-visibility{0%{opacity:0}to{opacity:1}}@keyframes turn-off-visibility{0%{opacity:1;visibility:visible}99%{opacity:0;visibility:visible}to{opacity:0;visibility:hidden}}@keyframes lightbox-zoom-in{0%{transform:translate(calc((-100vw + var(--wp--lightbox-scrollbar-width))/2 + var(--wp--lightbox-initial-left-position)),calc(-50vh + var(--wp--lightbox-initial-top-position))) scale(var(--wp--lightbox-scale))}to{transform:translate(-50%,-50%) scale(1)}}@keyframes lightbox-zoom-out{0%{transform:translate(-50%,-50%) scale(1);visibility:visible}99%{visibility:visible}to{transform:translate(calc((-100vw + var(--wp--lightbox-scrollbar-width))/2 + var(--wp--lightbox-initial-left-position)),calc(-50vh + var(--wp--lightbox-initial-top-position))) scale(var(--wp--lightbox-scale));visibility:hidden}}</style>
<style id=wp-block-navigation-link-inline-css>.wp-block-navigation .wp-block-navigation-item__label{overflow-wrap:break-word}</style>
<style>.wp-block-navigation{position:relative;--navigation-layout-justification-setting:flex-start;--navigation-layout-direction:row;--navigation-layout-wrap:wrap;--navigation-layout-justify:flex-start;--navigation-layout-align:center}.wp-block-navigation ul{margin-bottom:0;margin-left:0;margin-top:0;padding-left:0}.wp-block-navigation ul,.wp-block-navigation ul li{list-style:none;padding:0}.wp-block-navigation .wp-block-navigation-item{align-items:center;background-color:inherit;display:flex;position:relative}.wp-block-navigation .wp-block-navigation-item .wp-block-navigation__submenu-container:empty{display:none}.wp-block-navigation .wp-block-navigation-item__content{display:block}.wp-block-navigation .wp-block-navigation-item__content.wp-block-navigation-item__content{color:inherit}.wp-block-navigation :where(a),.wp-block-navigation :where(a:active),.wp-block-navigation :where(a:focus){text-decoration:none}.wp-block-navigation .wp-block-navigation__submenu-icon{align-self:center;background-color:inherit;border:none;color:currentColor;display:inline-block;font-size:inherit;height:.6em;line-height:0;margin-left:.25em;padding:0;width:.6em}.wp-block-navigation .wp-block-navigation__submenu-icon svg{display:inline-block;height:inherit;margin-top:.075em;width:inherit}.wp-block-navigation .has-child .wp-block-navigation__submenu-container{align-items:normal;background-color:inherit;color:inherit;display:flex;flex-direction:column;height:0;left:-1px;opacity:0;overflow:hidden;position:absolute;top:100%;transition:opacity .1s linear;visibility:hidden;width:0;z-index:2}.wp-block-navigation .has-child .wp-block-navigation__submenu-container>.wp-block-navigation-item>.wp-block-navigation-item__content{display:flex;flex-grow:1}.wp-block-navigation .has-child .wp-block-navigation__submenu-container .wp-block-navigation-item__content{margin:0}@media (min-width:782px){}.wp-block-navigation .has-child .wp-block-navigation-submenu__toggle[aria-expanded=true]~.wp-block-navigation__submenu-container,.wp-block-navigation .has-child:not(.open-on-click):hover>.wp-block-navigation__submenu-container,.wp-block-navigation .has-child:not(.open-on-click):not(.open-on-hover-click):focus-within>.wp-block-navigation__submenu-container{height:auto;min-width:200px;opacity:1;overflow:visible;visibility:visible;width:auto}@media (min-width:782px){}.wp-block-navigation-submenu .wp-block-navigation__submenu-icon svg{stroke:currentColor}.wp-block-navigation-submenu__toggle{cursor:pointer}.wp-block-navigation__responsive-close,.wp-block-navigation__responsive-container,.wp-block-navigation__responsive-container-content,.wp-block-navigation__responsive-dialog{gap:inherit}:where(.wp-block-navigation.has-background .wp-block-navigation-item a:not(.wp-element-button)),:where(.wp-block-navigation.has-background .wp-block-navigation-submenu a:not(.wp-element-button)){padding:.5em 1em}:where(.wp-block-navigation .wp-block-navigation__submenu-container .wp-block-navigation-item a:not(.wp-element-button)),:where(.wp-block-navigation .wp-block-navigation__submenu-container .wp-block-navigation-submenu a:not(.wp-element-button)),:where(.wp-block-navigation .wp-block-navigation__submenu-container .wp-block-navigation-submenu button.wp-block-navigation-item__content),:where(.wp-block-navigation .wp-block-navigation__submenu-container .wp-block-pages-list__item button.wp-block-navigation-item__content){padding:.5em 1em}@media (min-width:782px){}.wp-block-navigation:not(.has-background) .wp-block-navigation__submenu-container{background-color:#fff;border:1px solid #00000026}.wp-block-navigation:not(.has-text-color) .wp-block-navigation__submenu-container{color:#000}.wp-block-navigation__container{align-items:var(--navigation-layout-align,initial);display:flex;flex-direction:var(--navigation-layout-direction,initial);flex-wrap:var(--navigation-layout-wrap,wrap);justify-content:var(--navigation-layout-justify,initial);margin:0}.wp-block-navigation__container:only-child,.wp-block-page-list:only-child{flex-grow:1}@keyframes overlay-menu__fade-in-animation{0%{opacity:0;transform:translateY(.5em)}to{opacity:1;transform:translateY(0)}}.wp-block-navigation__responsive-container{bottom:0;display:none;left:0;position:fixed;right:0;top:0}.wp-block-navigation__responsive-container :where(.wp-block-navigation-item a){color:inherit}.wp-block-navigation__responsive-container .wp-block-navigation__responsive-container-content{align-items:var(--navigation-layout-align,initial);display:flex;flex-direction:var(--navigation-layout-direction,initial);flex-wrap:var(--navigation-layout-wrap,wrap);justify-content:var(--navigation-layout-justify,initial)}.wp-block-navigation__responsive-container:not(.is-menu-open.is-menu-open){background-color:inherit!important;color:inherit!important}@media (prefers-reduced-motion:reduce){}@media (min-width:600px){.wp-block-navigation__responsive-container:not(.hidden-by-default):not(.is-menu-open){background-color:inherit;display:block;position:relative;width:100%;z-index:auto}}.wp-block-navigation__responsive-container-close svg,.wp-block-navigation__responsive-container-open svg{fill:currentColor;display:block;height:24px;pointer-events:none;width:24px}@media (min-width:600px){}.wp-block-navigation__responsive-close{width:100%}.wp-block-navigation__responsive-close:focus{outline:none}.wp-block-navigation__responsive-dialog{position:relative}@media (min-width:782px){}</style>
<style id=wp-block-group-inline-css>.wp-block-group{box-sizing:border-box}:where(.wp-block-group.wp-block-group-is-layout-constrained){position:relative}</style>
<style id=wp-block-paragraph-inline-css>:root :where(p.has-background){padding:1.25em 2.375em}:where(p.has-text-color:not(.has-link-color)) a{color:inherit}</style>
<style id=wp-block-search-inline-css>.wp-block-search__button{margin-left:10px;word-break:normal}.wp-block-search__button.has-icon{line-height:0}.wp-block-search__button svg{width:1.25em;fill:currentColor;vertical-align:text-bottom}:where(.wp-block-search__button){border:1px solid #ccc;padding:6px 10px}.wp-block-search__inside-wrapper{display:flex;flex:auto;flex-wrap:nowrap;max-width:100%}.wp-block-search__input{appearance:none;border:1px solid #949494;flex-grow:1;margin-left:0;margin-right:0;min-width:3rem;padding:8px;text-decoration:unset!important}:where(.wp-block-search__input){font-family:inherit;font-size:inherit;font-style:inherit;font-weight:inherit;letter-spacing:inherit;line-height:inherit;text-transform:inherit}:where(.wp-block-search__button-inside .wp-block-search__inside-wrapper){border:1px solid #949494;box-sizing:border-box;padding:4px}:where(.wp-block-search__button-inside .wp-block-search__inside-wrapper) .wp-block-search__input{border:none;border-radius:0;padding:0 4px}:where(.wp-block-search__button-inside .wp-block-search__inside-wrapper) .wp-block-search__input:focus{outline:none}:where(.wp-block-search__button-inside .wp-block-search__inside-wrapper) :where(.wp-block-search__button){padding:4px 8px}</style>
<style id=wp-block-spacer-inline-css>.wp-block-spacer{clear:both}</style>
<style id=wp-block-post-title-inline-css>.wp-block-post-title{box-sizing:border-box;word-break:break-word}</style>
<style id=wp-block-list-inline-css>ol,ul{box-sizing:border-box}:root :where(.wp-block-list.has-background){padding:1.25em 2.375em}</style>
<style>.wp-block-table{overflow-x:auto}.wp-block-table table{border-collapse:collapse;width:100%}.wp-block-table thead{border-bottom:3px solid}.wp-block-table td,.wp-block-table th{border:1px solid;padding:.5em}.wp-block-table.is-style-stripes{background-color:initial;border-bottom:1px solid #f0f0f0;border-collapse:inherit;border-spacing:0}.wp-block-table.is-style-stripes tbody tr:nth-child(odd){background-color:#f0f0f0}.wp-block-table.is-style-stripes td,.wp-block-table.is-style-stripes th{border-color:#0000}</style>
<style id=wp-block-quote-inline-css>.wp-block-quote{box-sizing:border-box;overflow-wrap:break-word}</style>
<style id=wp-block-post-content-inline-css>.wp-block-post-content{display:flow-root}</style>
<style id=wp-block-separator-inline-css>.wp-block-separator{border:none;border-top:2px solid}:root :where(.wp-block-separator.is-style-dots){height:auto;line-height:1;text-align:center}:root :where(.wp-block-separator.is-style-dots):before{color:currentColor;content:"···";font-family:serif;font-size:1.5em;letter-spacing:2em;padding-left:2em}</style>
<style>.wp-block-jetpack-subscriptions.wp-block-jetpack-subscriptions__supports-newline:not(.wp-block-jetpack-subscriptions__use-newline) .is-not-subscriber .wp-block-jetpack-subscriptions__form-elements{align-items:flex-start;display:flex}.wp-block-jetpack-subscriptions.wp-block-jetpack-subscriptions__supports-newline:not(.wp-block-jetpack-subscriptions__use-newline) p#subscribe-submit{display:flex;justify-content:center}.wp-block-jetpack-subscriptions.wp-block-jetpack-subscriptions__supports-newline form button,.wp-block-jetpack-subscriptions.wp-block-jetpack-subscriptions__supports-newline form input[type=email]{box-sizing:border-box;cursor:pointer;line-height:1.3;min-width:auto!important;white-space:nowrap!important}.wp-block-jetpack-subscriptions.wp-block-jetpack-subscriptions__supports-newline .wp-block-jetpack-subscriptions__form input[type=email]::placeholder,.wp-block-jetpack-subscriptions.wp-block-jetpack-subscriptions__supports-newline .wp-block-jetpack-subscriptions__form input[type=email]:disabled,.wp-block-jetpack-subscriptions.wp-block-jetpack-subscriptions__supports-newline form input[type=email]::placeholder,.wp-block-jetpack-subscriptions.wp-block-jetpack-subscriptions__supports-newline form input[type=email]:disabled{color:currentColor;opacity:.5}.wp-block-jetpack-subscriptions.wp-block-jetpack-subscriptions__supports-newline form button{border-color:#0000;border-style:solid}.wp-block-jetpack-subscriptions.wp-block-jetpack-subscriptions__supports-newline form p#subscribe-email{background:#0000;flex-grow:1}.wp-block-jetpack-subscriptions.wp-block-jetpack-subscriptions__supports-newline form p#subscribe-email input[type=email]{height:auto;margin:0;width:100%}.wp-block-jetpack-subscriptions.wp-block-jetpack-subscriptions__supports-newline form p#subscribe-email,.wp-block-jetpack-subscriptions.wp-block-jetpack-subscriptions__supports-newline form p#subscribe-submit{line-height:0;margin:0;padding:0}@keyframes jetpack-memberships_button__spinner-animation{to{transform:rotate(1turn)}}.jetpack-memberships-spinner svg{height:100%;margin-bottom:-2px;width:100%}.jetpack-memberships-spinner-rotating{animation:jetpack-memberships_button__spinner-animation .75s linear infinite;transform-origin:center}</style>
<style id=wp-block-post-date-inline-css>.wp-block-post-date{box-sizing:border-box}</style>
<style id=wp-block-post-terms-inline-css>.wp-block-post-terms{box-sizing:border-box}.wp-block-post-terms .wp-block-post-terms__separator{white-space:pre-wrap}</style>
<style id=wp-block-columns-inline-css>.wp-block-columns{align-items:normal!important;box-sizing:border-box;flex-wrap:wrap!important}@media (min-width:782px){.wp-block-columns{flex-wrap:nowrap!important}}@media (max-width:781px){.wp-block-columns:not(.is-not-stacked-on-mobile)>.wp-block-column{flex-basis:100%!important}}@media (min-width:782px){.wp-block-columns:not(.is-not-stacked-on-mobile)>.wp-block-column{flex-basis:0;flex-grow:1}}:where(.wp-block-columns){margin-bottom:1.75em}:where(.wp-block-columns.has-background){padding:1.25em 2.375em}.wp-block-column{flex-grow:1;min-width:0;overflow-wrap:break-word;word-break:break-word}</style>
<style id=wp-block-post-comments-form-inline-css>:where(.wp-block-post-comments-form) input:not([type=submit]),:where(.wp-block-post-comments-form) textarea{border:1px solid #949494;font-family:inherit;font-size:1em}:where(.wp-block-post-comments-form) input:where(:not([type=submit]):not([type=checkbox])),:where(.wp-block-post-comments-form) textarea{padding:calc(.667em + 2px)}.wp-block-post-comments-form{box-sizing:border-box}.wp-block-post-comments-form :where(input[type=submit]){box-shadow:none;cursor:pointer;display:inline-block;overflow-wrap:break-word;text-align:center}.wp-block-post-comments-form .comment-form input:not([type=submit]):not([type=checkbox]):not([type=hidden]),.wp-block-post-comments-form .comment-form textarea{box-sizing:border-box;display:block;width:100%}.wp-block-post-comments-form .comment-form-author label,.wp-block-post-comments-form .comment-form-email label,.wp-block-post-comments-form .comment-form-url label{display:block;margin-bottom:.25em}.wp-block-post-comments-form .comment-form-cookies-consent{display:flex;gap:.25em}.wp-block-post-comments-form .comment-form-cookies-consent #wp-comment-cookies-consent{margin-top:.35em}.wp-block-post-comments-form .comment-reply-title{margin-bottom:0}.wp-block-post-comments-form .comment-reply-title :where(small){font-size:var(--wp--preset--font-size--medium,smaller);margin-left:.5em}</style>
<style id=wp-block-button-inline-css>.wp-block-button__link{box-sizing:border-box;display:inline-block;text-align:center;word-break:break-word}:where(.wp-block-button__link){border-radius:9999px;box-shadow:none;padding:calc(.667em + 2px) calc(1.333em + 2px);text-decoration:none}.wp-block-button__link.no-border-radius{border-radius:0!important}:root :where(.wp-block-button .wp-block-button__link.is-style-outline),:root :where(.wp-block-button.is-style-outline>.wp-block-button__link){border:2px solid;padding:.667em 1.333em}:root :where(.wp-block-button .wp-block-button__link.is-style-outline:not(.has-text-color)),:root :where(.wp-block-button.is-style-outline>.wp-block-button__link:not(.has-text-color)){color:currentColor}:root :where(.wp-block-button .wp-block-button__link.is-style-outline:not(.has-background)),:root :where(.wp-block-button.is-style-outline>.wp-block-button__link:not(.has-background)){background-color:initial;background-image:none}</style>
<style id=wp-block-comments-inline-css>:where(.wp-block-post-comments input[type=submit]){border:none}</style>
<style>:root{--wp-admin-theme-color:#007cba;--wp-admin-theme-color--rgb:0,124,186;--wp-admin-theme-color-darker-10:#006ba1;--wp-admin-theme-color-darker-10--rgb:0,107,161;--wp-admin-theme-color-darker-20:#005a87;--wp-admin-theme-color-darker-20--rgb:0,90,135;--wp-admin-border-width-focus:2px;--wp-block-synced-color:#7a00df;--wp-block-synced-color--rgb:122,0,223;--wp-bound-block-color:var(--wp-block-synced-color)}@media (min-resolution:192dpi){:root{--wp-admin-border-width-focus:1.5px}}.wp-element-button{cursor:pointer}:root{--wp--preset--font-size--normal:16px;--wp--preset--font-size--huge:42px}.has-text-align-center{text-align:center}.has-text-align-right{text-align:right}.screen-reader-text{border:0;clip:rect(1px,1px,1px,1px);clip-path:inset(50%);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px;word-wrap:normal!important}.screen-reader-text:focus{background-color:#ddd;clip:auto!important;clip-path:none;color:#444;display:block;font-size:1em;height:auto;left:5px;line-height:normal;padding:15px 23px 14px;text-decoration:none;top:5px;width:auto;z-index:100000}html :where(.has-border-color){border-style:solid}html :where([style*=border-top-color]){border-top-style:solid}html :where([style*=border-right-color]){border-right-style:solid}html :where([style*=border-bottom-color]){border-bottom-style:solid}html :where([style*=border-left-color]){border-left-style:solid}html :where([style*=border-width]){border-style:solid}html :where([style*=border-top-width]){border-top-style:solid}html :where([style*=border-right-width]){border-right-style:solid}html :where([style*=border-bottom-width]){border-bottom-style:solid}html :where([style*=border-left-width]){border-left-style:solid}html :where(img[class*=wp-image-]){height:auto;max-width:100%}:where(figure){margin:0 0 1em}html :where(.is-position-sticky){--wp-admin--admin-bar--position-offset:var(--wp-admin--admin-bar--height,0px)}@media screen and (max-width:600px){html :where(.is-position-sticky){--wp-admin--admin-bar--position-offset:0px}}</style>
<style id=global-styles-inline-css>:root{--wp--preset--aspect-ratio--square:1;--wp--preset--aspect-ratio--4-3:4/3;--wp--preset--aspect-ratio--3-4:3/4;--wp--preset--aspect-ratio--3-2:3/2;--wp--preset--aspect-ratio--2-3:2/3;--wp--preset--aspect-ratio--16-9:16/9;--wp--preset--aspect-ratio--9-16:9/16;--wp--preset--color--black:#000000;--wp--preset--color--cyan-bluish-gray:#abb8c3;--wp--preset--color--white:#ffffff;--wp--preset--color--pale-pink:#f78da7;--wp--preset--color--vivid-red:#cf2e2e;--wp--preset--color--luminous-vivid-orange:#ff6900;--wp--preset--color--luminous-vivid-amber:#fcb900;--wp--preset--color--light-green-cyan:#7bdcb5;--wp--preset--color--vivid-green-cyan:#00d084;--wp--preset--color--pale-cyan-blue:#8ed1fc;--wp--preset--color--vivid-cyan-blue:#0693e3;--wp--preset--color--vivid-purple:#9b51e0;--wp--preset--color--base:#ffffff;--wp--preset--color--contrast:#000000;--wp--preset--color--primary:#9DFF20;--wp--preset--color--secondary:#345C00;--wp--preset--color--tertiary:#F6F6F6;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple:linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan:linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange:linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red:linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray:linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum:linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple:linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux:linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk:linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean:linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass:linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight:linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small:clamp(0.875rem,0.875rem + ((1vw - 0.2rem)*0.227),1rem);--wp--preset--font-size--medium:clamp(1rem,1rem + ((1vw - 0.2rem)*0.227),1.125rem);--wp--preset--font-size--large:clamp(1.75rem,1.75rem + ((1vw - 0.2rem)*0.227),1.875rem);--wp--preset--font-size--x-large:2.25rem;--wp--preset--font-size--xx-large:clamp(6.1rem,6.1rem + ((1vw - 0.2rem)*7.091),10rem);--wp--preset--font-family--dm-sans:"DM Sans",sans-serif;--wp--preset--font-family--ibm-plex-mono:"IBM Plex Mono",monospace;--wp--preset--font-family--inter:"Inter",sans-serif;--wp--preset--font-family--system-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;--wp--preset--font-family--source-serif-pro:"Source Serif Pro",serif;--wp--preset--spacing--20:0.44rem;--wp--preset--spacing--30:clamp(1.5rem,5vw,2rem);--wp--preset--spacing--40:clamp(1.8rem,1.8rem + ((1vw - 0.48rem)*2.885),3rem);--wp--preset--spacing--50:clamp(2.5rem,8vw,4.5rem);--wp--preset--spacing--60:clamp(3.75rem,10vw,7rem);--wp--preset--spacing--70:clamp(5rem,5.25rem + ((1vw - 0.48rem)*9.096),8rem);--wp--preset--spacing--80:clamp(7rem,14vw,11rem);--wp--preset--shadow--natural:6px 6px 9px rgba(0,0,0,0.2);--wp--preset--shadow--deep:12px 12px 50px rgba(0,0,0,0.4);--wp--preset--shadow--sharp:6px 6px 0px rgba(0,0,0,0.2);--wp--preset--shadow--outlined:6px 6px 0px -3px rgba(255,255,255,1),6px 6px rgba(0,0,0,1);--wp--preset--shadow--crisp:6px 6px 0px rgba(0,0,0,1)}:root{--wp--style--global--content-size:650px;--wp--style--global--wide-size:1200px}:where(body){margin:0}.has-global-padding{padding-right:var(--wp--style--root--padding-right);padding-left:var(--wp--style--root--padding-left)}.has-global-padding :where(:not(.alignfull.is-layout-flow)>.has-global-padding:not(.wp-block-block,.alignfull)){padding-right:0;padding-left:0}:where(.wp-site-blocks)>*{margin-block-start:1.5rem;margin-block-end:0}:where(.wp-site-blocks)>:first-child{margin-block-start:0}:where(.wp-site-blocks)>:last-child{margin-block-end:0}:root{--wp--style--block-gap:1.5rem}:root :where(.is-layout-flow)>:first-child{margin-block-start:0}:root :where(.is-layout-flow)>:last-child{margin-block-end:0}:root :where(.is-layout-flow)>*{margin-block-start:1.5rem;margin-block-end:0}:root :where(.is-layout-constrained)>:first-child{margin-block-start:0}:root :where(.is-layout-constrained)>:last-child{margin-block-end:0}:root :where(.is-layout-constrained)>*{margin-block-start:1.5rem;margin-block-end:0}:root :where(.is-layout-flex){gap:1.5rem}:root :where(.is-layout-grid){gap:1.5rem}.is-layout-constrained>:where(:not(.alignleft):not(.alignright):not(.alignfull)){max-width:var(--wp--style--global--content-size);margin-left:auto!important;margin-right:auto!important}.is-layout-constrained>.alignwide{max-width:var(--wp--style--global--wide-size)}body .is-layout-flex{display:flex}.is-layout-flex{flex-wrap:wrap;align-items:center}.is-layout-flex>:is(*,div){margin:0}body{background-color:var(--wp--preset--color--base);color:var(--wp--preset--color--contrast);font-family:var(--wp--preset--font-family--system-font);font-size:var(--wp--preset--font-size--medium);line-height:1.6;--wp--style--root--padding-top:var(--wp--preset--spacing--40);--wp--style--root--padding-right:var(--wp--preset--spacing--30);--wp--style--root--padding-bottom:var(--wp--preset--spacing--40);--wp--style--root--padding-left:var(--wp--preset--spacing--30)}a:where(:not(.wp-element-button)){color:var(--wp--preset--color--contrast);text-decoration:underline}:root :where(a:where(:not(.wp-element-button)):hover){text-decoration:none}:root :where(a:where(:not(.wp-element-button)):focus){text-decoration:underline dashed}:root :where(a:where(:not(.wp-element-button)):active){color:var(--wp--preset--color--secondary);text-decoration:none}h3,h4,h5{line-height:1.4}h1{line-height:1.2}h2{line-height:1.2}h5{text-transform:uppercase}:root :where(.wp-element-button,.wp-block-button__link){background-color:var(--wp--preset--color--primary);border-radius:0;border-width:0;color:var(--wp--preset--color--contrast);font-family:inherit;font-size:inherit;line-height:inherit;padding:calc(0.667em + 2px) calc(1.333em + 2px);text-decoration:none}:root :where(.wp-element-button:visited,.wp-block-button__link:visited){color:var(--wp--preset--color--contrast)}:root :where(.wp-element-button:hover,.wp-block-button__link:hover){background-color:var(--wp--preset--color--contrast);color:var(--wp--preset--color--base)}:root :where(.wp-element-button:focus,.wp-block-button__link:focus){background-color:var(--wp--preset--color--contrast);color:var(--wp--preset--color--base)}:root :where(.wp-element-button:active,.wp-block-button__link:active){background-color:var(--wp--preset--color--secondary);color:var(--wp--preset--color--base)}.has-small-font-size{font-size:var(--wp--preset--font-size--small)!important}:root :where(.wp-block-navigation){font-size:var(--wp--preset--font-size--small)}:root :where(.wp-block-navigation a:where(:not(.wp-element-button))){text-decoration:none}:root :where(.wp-block-navigation a:where(:not(.wp-element-button)):hover){text-decoration:underline}:root :where(.wp-block-navigation a:where(:not(.wp-element-button)):focus){text-decoration:underline dashed}:root :where(.wp-block-navigation a:where(:not(.wp-element-button)):active){text-decoration:none}:root :where(.wp-block-post-content a:where(:not(.wp-element-button))){color:var(--wp--preset--color--secondary)}:root :where(.wp-block-post-date){font-size:var(--wp--preset--font-size--small);font-weight:400}:root :where(.wp-block-post-date a:where(:not(.wp-element-button))){text-decoration:none}:root :where(.wp-block-post-date a:where(:not(.wp-element-button)):hover){text-decoration:underline}:root :where(.wp-block-post-terms){font-size:var(--wp--preset--font-size--small)}:root :where(.wp-block-post-title){font-weight:400;margin-top:1.25rem;margin-bottom:1.25rem}:root :where(.wp-block-post-title a:where(:not(.wp-element-button))){text-decoration:none}:root :where(.wp-block-post-title a:where(:not(.wp-element-button)):hover){text-decoration:underline}:root :where(.wp-block-post-title a:where(:not(.wp-element-button)):focus){text-decoration:underline dashed}:root :where(.wp-block-post-title a:where(:not(.wp-element-button)):active){color:var(--wp--preset--color--secondary);text-decoration:none}:root :where(.wp-block-comments-title){font-size:var(--wp--preset--font-size--large);margin-bottom:var(--wp--preset--spacing--40)}:root :where(.wp-block-comments-pagination){margin-top:var(--wp--preset--spacing--40)}:root :where(.wp-block-comments-pagination a:where(:not(.wp-element-button))){text-decoration:none}:root :where(.wp-block-quote){border-left-color:inherit;border-left-width:1px;border-left-style:solid;padding-right:var(--wp--preset--spacing--30);padding-left:var(--wp--preset--spacing--30)}:root :where(.wp-block-quote cite){font-size:var(--wp--preset--font-size--small);font-style:normal}:root :where(.wp-block-separator){}:root :where(.wp-block-separator:not(.is-style-wide):not(.is-style-dots):not(.alignwide):not(.alignfull)){width:100px}</style>
<style id=core-block-supports-inline-css>.wp-container-core-group-is-layout-1{flex-wrap:nowrap}.wp-container-core-group-is-layout-2{flex-direction:column;align-items:flex-end}.wp-container-core-group-is-layout-3{flex-wrap:nowrap;justify-content:space-between}.wp-container-core-group-is-layout-5{flex-wrap:nowrap;justify-content:flex-start}.wp-container-core-group-is-layout-9>:where(:not(.alignleft):not(.alignright):not(.alignfull)){max-width:480px;margin-left:auto!important;margin-right:auto!important}.wp-container-core-group-is-layout-11{gap:0.5ch}.wp-container-core-column-is-layout-1>*{margin-block-start:0;margin-block-end:0}.wp-container-core-columns-is-layout-1{flex-wrap:nowrap;gap:var(--wp--preset--spacing--30)}.wp-container-core-group-is-layout-15{justify-content:space-between}</style>
<style id=wp-block-template-skip-link-inline-css>.skip-link.screen-reader-text{border:0;clip:rect(1px,1px,1px,1px);clip-path:inset(50%);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute!important;width:1px;word-wrap:normal!important}.skip-link.screen-reader-text:focus{background-color:#eee;clip:auto!important;clip-path:none;color:#444;display:block;font-size:1em;height:auto;left:5px;line-height:normal;padding:15px 23px 14px;text-decoration:none;top:5px;width:auto;z-index:100000}</style>
<style>body{margin:0;--wp--style--global--content-size:960px;--wp--style--global--wide-size:960px;--wp--preset--color--primary:#00a82d;--wp--preset--color--secondary:#00c234;--wp--preset--color--tertiary:#f6f6f6;--wp--preset--color--contrast:#000000;--wp--preset--color--highlight:#c64f00}main{margin-top:0!important}h1{font-size:2.50rem;font-weight:400;margin-top:48px}h2{font-size:2.00rem;font-weight:400;margin-top:36px}h3{font-size:1.625rem;font-weight:400;margin-top:28px}h4{font-size:1.375rem;font-weight:400;margin-top:22px}h5{font-size:1.125rem;font-weight:400;margin-top:20px}.wp-site-blocks{padding-top:1em;padding-bottom:1em}header div#cx-mainheader{margin-top:-12px}header div#cx-mainheader p{font-size:14px}header form.wp-block-search>div{padding:1px}header form.wp-block-search button{padding:0}header form.wp-block-search svg.search-icon{height:16px;min-height:16px;min-width:16px}header div#cx-mainheader div{gap:0;margin-bottom:3px}.wp-block-navigation li.colfax-sites-menu{border:none!important;padding-left:0;font-size:90%;color:var(--wp--preset--color--primary)}div#cx-mainmenu>div{padding-bottom:0em!important;border-radius:3px;border:1px solid #eeeeee;border-bottom:2px solid #cccccc;background:#fefefe;background:-webkit-linear-gradient(top,#fefefe 0%,#ededed 100%);background:-moz-linear-gradient(top,#fefefe 0%,#ededed 100%);background:-o-linear-gradient(top,#fefefe 0%,#ededed 100%);background:-ms-linear-gradient(top,#fefefe 0%,#ededed 100%)}.wp-block-navigation ul,.wp-block-navigation ul li{line-height:35px;gap:0px}.wp-block-navigation .wp-block-navigation-item{margin-left:-1px;border-right:1px dotted #cccccc!important;border-left:1px dotted #cccccc!important}.wp-block-navigation .wp-block-navigation-item a{padding-left:1em;padding-right:1em}button.wp-block-navigation__submenu-icon{padding-right:1em!important}.wp-block-navigation:hover .wp-block-navigation-item:hover{background-color:#00c234;-webkit-transition:all 1s ease;-moz-transition:all 1s ease;-ms-transition:all 1s ease;-o-transition:all 1s ease;transition:all 1s ease;text-decoration:none}.wp-block-navigation a:where(:not(.wp-element-button)):hover{text-decoration:none!important}.wp-element-button:visited,.wp-block-button__link:visited,.wp-element-button,.wp-block-button__link{color:var(--wp--preset--color--base);text-transform:uppercase;font-size:20px;font-style:normal}.wp-element-button:hover,.wp-block-button__link:hover{background-color:var(--wp--preset--color--secondary)}.wp-element-button,.wp-block-button__link{color:var(--wp--preset--color--base);background-color:var(--wp--preset--color--primary);-webkit-transition:all .25s ease;-moz-transition:all .25s ease;-ms-transition:all .25s ease;-o-transition:all .25s ease;transition:all .25s ease}.wp-block-post-content a:where(:not(.wp-element-button)){color:var(--wp--preset--color--primary)}.wp-block-post-content a:hover:where(:not(.wp-element-button)){color:var(--wp--preset--color--highlight);text-decoration:underline}</style>
<style>div.sharedaddy{clear:both}div.sharedaddy h3.sd-title{margin:0 0 1em 0;display:inline-block;line-height:1.2;font-size:9pt;font-weight:bold}.sd-sharing{margin-bottom:1em}.sd-content ul{padding:0!important;margin:0!important;list-style:none!important}.sd-content ul li{display:inline-block}.sd-social-icon-text .sd-content ul li a.sd-button{text-decoration:none!important;display:inline-block;font-size:13px;font-family:"Open Sans",sans-serif;font-weight:500;border-radius:4px;color:#2C3338!important;background:#fff;box-shadow:0 1px 2px rgba(0,0,0,0.12),0 0 0 1px rgba(0,0,0,0.12);text-shadow:none;line-height:23px;padding:4px 11px 3px 9px}.sd-social-icon-text .sd-content ul li a.sd-button>span{line-height:23px;margin-left:6px}.sd-social-icon .sd-content ul li a.sd-button:hover,.sd-social-icon .sd-content ul li a.sd-button:active,.sd-social-text .sd-content ul li a.sd-button:hover,.sd-social-text .sd-content ul li a.sd-button:active,.sd-social-icon-text .sd-content ul li a.sd-button:hover,.sd-social-icon-text .sd-content ul li a.sd-button:active,.sd-social-official .sd-content>ul>li>a.sd-button:hover,.sd-social-official .sd-content>ul>li>a.sd-button:active,.sd-social-official .sd-content>ul>li .digg_button>a:hover,.sd-social-official .sd-content>ul>li .digg_button>a:active{box-shadow:0 1px 2px rgba(0,0,0,0.22),0 0 0 1px rgba(0,0,0,0.22)}.sd-social-icon .sd-content ul li a.sd-button:active,.sd-social-text .sd-content ul li a.sd-button:active,.sd-social-icon-text .sd-content ul li a.sd-button:active,.sd-social-official .sd-content>ul>li>a.sd-button:active,.sd-social-official .sd-content>ul>li .digg_button>a:active{box-shadow:inset 0 1px 0 rgba(0,0,0,.16)}.sd-content ul li a.sd-button:before{display:inline-block;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;font:normal 18px/1"social-logos";vertical-align:top;text-align:center}.sd-social-icon-text ul li a.sd-button:before{position:relative;top:2px}@media screen and (-webkit-min-device-pixel-ratio:0){.sd-content ul li a.sd-button:before{position:relative;top:2px}}@media screen and (-webkit-min-device-pixel-ratio:0){}.sd-content ul li{margin:0 8px 12px 0;padding:0}@media screen and (-webkit-min-device-pixel-ratio:0){}@-moz-document url-prefix(){.sd-social-icon .sd-content ul li a.share-more{top:2px}}.sd-social-icon .sd-content ul li.share-linkedin a:before,.sd-social-text .sd-content ul li.share-linkedin a:before,.sd-content ul li.share-linkedin div.option.option-smart-off a:before,.sd-social-icon-text .sd-content li.share-linkedin a:before{content:""}.sd-social-icon .sd-content ul li.share-twitter a:before,.sd-social-text .sd-content ul li.share-twitter a:before,.sd-content ul li.share-twitter div.option.option-smart-off a:before,.sd-social-icon-text .sd-content li.share-twitter a:before{content:""}.sd-social-icon .sd-content ul li.share-reddit a:before,.sd-social-text .sd-content ul li.share-reddit a:before,.sd-content ul li.share-reddit div.option.option-smart-off a:before,.sd-social-icon-text .sd-content li.share-reddit a:before,.sd-social-official .sd-content li.share-reddit a:before{content:""}.sd-social-icon .sd-content ul li.share-facebook a:before,.sd-social-text .sd-content ul li.share-facebook a:before,.sd-content ul li.share-facebook div.option.option-smart-off a:before,.sd-social-icon-text .sd-content li.share-facebook a:before{content:""}</style>
<style>@font-face{font-family:social-logos;src:url(data:application/octet-stream;base64,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)format("woff2");display:inline-block;vertical-align:middle;line-height:1;font-weight:400;font-style:normal;speak:none;text-decoration:inherit;text-transform:none;text-rendering:auto;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}</style>
<link rel=https://api.w.org/ href=https://research.colfax-intl.com/wp-json/><link rel=alternate title=JSON type=application/json href=https://research.colfax-intl.com/wp-json/wp/v2/posts/10593><link rel=EditURI type=application/rsd+xml title=RSD href=https://research.colfax-intl.com/xmlrpc.php?rsd>
<meta name=generator content="WordPress 6.7.1">
<link rel=canonical href=https://research.colfax-intl.com/tutorial-hopper-tma/>
<link rel=shortlink href=https://wp.me/pfwen0-2KR>
<link rel=alternate title="oEmbed (JSON)" type=application/json+oembed href="https://research.colfax-intl.com/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fresearch.colfax-intl.com%2Ftutorial-hopper-tma%2F">
<link rel=alternate title="oEmbed (XML)" type=text/xml+oembed href="https://research.colfax-intl.com/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fresearch.colfax-intl.com%2Ftutorial-hopper-tma%2F&amp;format=xml">
<meta property=og:type content=article>
<meta property=og:title content="CUTLASS Tutorial: Mastering the NVIDIA® Tensor Memory Accelerator (TMA)">
<meta property=og:url content=https://research.colfax-intl.com/tutorial-hopper-tma/>
<meta property=og:description content="TMA (Tensor Memory Accelerator) is a new feature introduced in the NVIDIA Hopper™ architecture for doing asynchronous memory copy between a GPU’s global memory (GMEM) and the shared me…">
<meta property=article:published_time content=2024-06-24T16:39:24+00:00>
<meta property=article:modified_time content=2024-11-19T17:22:14+00:00>
<meta property=og:site_name content="Colfax Research">
<meta property=og:image content="https://i0.wp.com/research.colfax-intl.com/wp-content/uploads/2024/06/tma_graphic.png?fit=1200%2C550&amp;ssl=1">
<meta property=og:image:width content=1200>
<meta property=og:image:height content=550>
<meta property=og:image:alt content>
<meta property=og:locale content=en_US>
<meta name=twitter:site content=@colfaxintl>
<meta name=twitter:text:title content="CUTLASS Tutorial: Mastering the NVIDIA® Tensor Memory Accelerator (TMA)">
<meta name=twitter:image content="https://i0.wp.com/research.colfax-intl.com/wp-content/uploads/2024/06/tma_graphic.png?fit=1200%2C550&amp;ssl=1&amp;w=640">
<meta name=twitter:card content=summary_large_image>
<link rel=icon href="data:image/webp;base64,UklGRlAAAABXRUJQVlA4TEQAAAAvv8AvEA8wqDEt8x84FLWN5ERaAAtpqR+kA9DLu98jov8K0jZgWg87FpiQIXclD7rctZ+GieXwBcLnrHcHgBvD9/xrAA==" sizes=192x192>
<meta name=msapplication-TileImage content="https://i0.wp.com/research.colfax-intl.com/wp-content/uploads/2024/02/colfax-favicon.png?fit=270%2C270&amp;ssl=1">
<style>.syntaxhighlighter div,.syntaxhighlighter code,.syntaxhighlighter table,.syntaxhighlighter table td,.syntaxhighlighter table tr,.syntaxhighlighter table tbody{-moz-border-radius:0 0 0 0!important;-webkit-border-radius:0 0 0 0!important;background:none!important;border:0!important;bottom:auto!important;float:none!important;height:auto!important;left:auto!important;line-height:1.1em!important;margin:0!important;outline:0!important;overflow:visible!important;padding:0!important;position:static!important;right:auto!important;text-align:left!important;top:auto!important;vertical-align:baseline!important;width:auto!important;box-sizing:content-box!important;font-family:Monaco,"Consolas","Bitstream Vera Sans Mono","Courier New",Courier,monospace!important;font-weight:normal!important;font-style:normal!important;font-size:1em!important;direction:ltr!important;-webkit-box-shadow:none!important;-moz-box-shadow:none!important;-ms-box-shadow:none!important;-o-box-shadow:none!important;box-shadow:none!important}.syntaxhighlighter{width:100%!important;margin:1em 0 1em 0!important;position:relative!important;overflow:auto!important;overflow-y:hidden!important;font-size:1em!important;padding:0.5em 1em!important}.syntaxhighlighter code{display:inline!important}.syntaxhighlighter .bold{font-weight:bold!important}.syntaxhighlighter .line{white-space:pre!important}.syntaxhighlighter table{table-layout:auto!important;width:100%!important}.syntaxhighlighter table td.code{width:100%!important}.syntaxhighlighter table td.code .container{position:relative!important}.syntaxhighlighter table td.code .container:after,.syntaxhighlighter table td.code .container:before{display:block!important;content:""}.syntaxhighlighter table td.gutter .line{text-align:right!important;padding:0 0.5em 0 0!important;margin-right:1em!important}.syntaxhighlighter table td.code .line{padding:0!important}</style><style>.syntaxhighlighter{background-color:white!important}.syntaxhighlighter .line.alt1{background-color:white!important}.syntaxhighlighter .line.alt2{background-color:white!important}.syntaxhighlighter .line.highlighted.alt1,.syntaxhighlighter .line.highlighted.alt2{background-color:#e0e0e0!important}.syntaxhighlighter .gutter{color:#afafaf!important}.syntaxhighlighter .gutter .line{border-right:3px solid #6ce26c!important}.syntaxhighlighter .gutter .line.highlighted{background-color:#6ce26c!important;color:white!important}.syntaxhighlighter .plain{color:black!important}.syntaxhighlighter .comments{color:#008200!important}.syntaxhighlighter .string{color:blue!important}.syntaxhighlighter .keyword{color:#006699!important}.syntaxhighlighter .functions{color:#ff1493!important}.syntaxhighlighter .color1{color:gray!important}.syntaxhighlighter .keyword{font-weight:bold!important}</style><style id=ctre_styles>@media (prefers-color-scheme:dark){}</style><meta name=referrer content=no-referrer><style>.sf-hidden{display:none!important}</style><meta http-equiv=content-security-policy content="default-src 'none'; font-src 'self' data:; img-src 'self' data:; style-src 'unsafe-inline'; media-src 'self' data:; script-src 'unsafe-inline' data:; object-src 'self' data:; frame-src 'self' data:;"><style>img[src="data:,"],source[src="data:,"]{display:none!important}</style></head>
<body class="post-template-default single single-post postid-10593 single-format-standard wp-custom-logo wp-embed-responsive">
<a class="skip-link screen-reader-text" href=#wp--skip-link--target>Skip to content</a><div class=wp-site-blocks><header class=wp-block-template-part>
<div id=cx-mainheader class="wp-block-group has-global-padding is-layout-constrained wp-block-group-is-layout-constrained">
<div class="wp-block-group is-content-justification-space-between is-nowrap is-layout-flex wp-container-core-group-is-layout-3 wp-block-group-is-layout-flex">
<div class="wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-1 wp-block-group-is-layout-flex">
<figure class="wp-block-image size-large is-resized"><img fetchpriority=high decoding=async width=1024 height=357 src="data:image/webp;base64,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" alt class=wp-image-9229 style=width:162px srcset sizes></figure>
<nav class="is-responsive wp-block-navigation is-layout-flex wp-block-navigation-is-layout-flex" aria-label="Colfax Sites" data-wp-interactive=core/navigation data-wp-context='{"overlayOpenedBy":{"click":false,"hover":false,"focus":false},"type":"overlay","roleAttribute":"","ariaLabel":"Menu"}'><button aria-haspopup=dialog aria-label="Open menu" class="wp-block-navigation__responsive-container-open sf-hidden" data-wp-on-async--click=actions.openMenuOnClick data-wp-on--keydown=actions.handleMenuKeydown><svg width=24 height=24 xmlns=http://www.w3.org/2000/svg viewBox="0 0 24 24" aria-hidden=true focusable=false><rect x=4 y=7.5 width=16 height=1.5></rect><rect x=4 y=15 width=16 height=1.5></rect></svg></button>
 <div class=wp-block-navigation__responsive-container id=modal-1 data-wp-class--has-modal-open=state.isMenuOpen data-wp-class--is-menu-open=state.isMenuOpen data-wp-watch=callbacks.initMenu data-wp-on--keydown=actions.handleMenuKeydown data-wp-on-async--focusout=actions.handleMenuFocusout tabindex=-1>
 <div class=wp-block-navigation__responsive-close tabindex=-1>
 <div class=wp-block-navigation__responsive-dialog data-wp-bind--aria-modal=state.ariaModal data-wp-bind--aria-label=state.ariaLabel data-wp-bind--role=state.roleAttribute>
 <button aria-label="Close menu" class="wp-block-navigation__responsive-container-close sf-hidden" data-wp-on-async--click=actions.closeMenuOnClick><svg xmlns=http://www.w3.org/2000/svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true focusable=false><path d="m13.06 12 6.47-6.47-1.06-1.06L12 10.94 5.53 4.47 4.47 5.53 10.94 12l-6.47 6.47 1.06 1.06L12 13.06l6.47 6.47 1.06-1.06L13.06 12Z"></path></svg></button>
 <div class=wp-block-navigation__responsive-container-content data-wp-watch=callbacks.focusFirstElement id=modal-1-content>
 <ul class="wp-block-navigation__container is-responsive wp-block-navigation"><li data-wp-context='{ "submenuOpenedBy": { "click": false, "hover": false, "focus": false }, "type": "submenu" }' data-wp-interactive=core/navigation data-wp-on--focusout=actions.handleMenuFocusout data-wp-on--keydown=actions.handleMenuKeydown data-wp-on-async--mouseenter=actions.openMenuOnHover data-wp-on-async--mouseleave=actions.closeMenuOnHover data-wp-watch=callbacks.initMenu tabindex=-1 class="wp-block-navigation-item has-child open-on-hover-click colfax-sites-menu wp-block-navigation-submenu"><a class=wp-block-navigation-item__content>▼</a><button data-wp-bind--aria-expanded=state.isMenuOpen data-wp-on-async--click=actions.toggleMenuOnClick aria-label="▼ submenu" class="wp-block-navigation__submenu-icon wp-block-navigation-submenu__toggle sf-hidden" aria-expanded=false><svg xmlns=http://www.w3.org/2000/svg width=12 height=12 viewBox="0 0 12 12" fill=none aria-hidden=true focusable=false><path d="M1.50002 4L6.00002 8L10.5 4" stroke-width=1.5></path></svg></button><ul data-wp-on-async--focus=actions.openMenuOnFocus class="wp-block-navigation__submenu-container colfax-sites-menu wp-block-navigation-submenu"><li class="wp-block-navigation-item wp-block-navigation-link"><a class=wp-block-navigation-item__content href=https://www.colfax-intl.com/><span class=wp-block-navigation-item__label>Systems</span></a><li class="wp-block-navigation-item wp-block-navigation-link"><a class=wp-block-navigation-item__content href=https://colfaxdirect.com/><span class=wp-block-navigation-item__label>Components</span></a><li class="wp-block-navigation-item wp-block-navigation-link"><a class=wp-block-navigation-item__content href=https://research.colfax-intl.com/><span class=wp-block-navigation-item__label>Research</span></a><li class="wp-block-navigation-item wp-block-navigation-link"><a class=wp-block-navigation-item__content href=https://experience.colfax-intl.com/><span class=wp-block-navigation-item__label>Experience Center</span></a></ul></ul>
 </div>
 </div>
 </div>
 </div></nav></div>
<div class="wp-block-group is-vertical is-content-justification-right is-layout-flex wp-container-core-group-is-layout-2 wp-block-group-is-layout-flex">
<p class=has-text-align-right>2805 Bowers Ave, Santa Clara, CA 95051 | 408-730-2275<br><a href=mailto:research@colfax-intl.com>research@colfax-intl.com</a></p>
<form role=search action=https://research.colfax-intl.com/ class="wp-block-search__button-inside wp-block-search__icon-button wp-block-search"><label class="wp-block-search__label screen-reader-text" for=wp-block-search__input-2>Search</label><div class=wp-block-search__inside-wrapper style=width:100%><input class=wp-block-search__input id=wp-block-search__input-2 placeholder value type=search name=s required><button aria-label=Search class="wp-block-search__button has-icon wp-element-button" type=submit><svg class=search-icon viewBox="0 0 24 24" width=24 height=24>
 <path d="M13 5c-3.3 0-6 2.7-6 6 0 1.4.5 2.7 1.3 3.7l-3.8 3.8 1.1 1.1 3.8-3.8c1 .8 2.3 1.3 3.7 1.3 3.3 0 6-2.7 6-6S16.3 5 13 5zm0 10.5c-2.5 0-4.5-2-4.5-4.5s2-4.5 4.5-4.5 4.5 2 4.5 4.5-2 4.5-4.5 4.5z"></path>
 </svg></button></div></form></div>
</div>
</div>
<div id=cx-mainmenu class="wp-block-group has-global-padding is-layout-constrained wp-block-group-is-layout-constrained">
<div class="wp-block-group alignwide is-content-justification-left is-nowrap is-layout-flex wp-container-core-group-is-layout-5 wp-block-group-is-layout-flex" style=padding-bottom:var(--wp--preset--spacing--40)><nav class="is-responsive wp-block-navigation is-layout-flex wp-block-navigation-is-layout-flex" aria-label=Navigation data-wp-interactive=core/navigation data-wp-context='{"overlayOpenedBy":{"click":false,"hover":false,"focus":false},"type":"overlay","roleAttribute":"","ariaLabel":"Menu"}'><button aria-haspopup=dialog aria-label="Open menu" class="wp-block-navigation__responsive-container-open sf-hidden" data-wp-on-async--click=actions.openMenuOnClick data-wp-on--keydown=actions.handleMenuKeydown><svg width=24 height=24 xmlns=http://www.w3.org/2000/svg viewBox="0 0 24 24" aria-hidden=true focusable=false><rect x=4 y=7.5 width=16 height=1.5></rect><rect x=4 y=15 width=16 height=1.5></rect></svg></button>
 <div class=wp-block-navigation__responsive-container id=modal-3 data-wp-class--has-modal-open=state.isMenuOpen data-wp-class--is-menu-open=state.isMenuOpen data-wp-watch=callbacks.initMenu data-wp-on--keydown=actions.handleMenuKeydown data-wp-on-async--focusout=actions.handleMenuFocusout tabindex=-1>
 <div class=wp-block-navigation__responsive-close tabindex=-1>
 <div class=wp-block-navigation__responsive-dialog data-wp-bind--aria-modal=state.ariaModal data-wp-bind--aria-label=state.ariaLabel data-wp-bind--role=state.roleAttribute>
 <button aria-label="Close menu" class="wp-block-navigation__responsive-container-close sf-hidden" data-wp-on-async--click=actions.closeMenuOnClick><svg xmlns=http://www.w3.org/2000/svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true focusable=false><path d="m13.06 12 6.47-6.47-1.06-1.06L12 10.94 5.53 4.47 4.47 5.53 10.94 12l-6.47 6.47 1.06 1.06L12 13.06l6.47 6.47 1.06-1.06L13.06 12Z"></path></svg></button>
 <div class=wp-block-navigation__responsive-container-content data-wp-watch=callbacks.focusFirstElement id=modal-3-content>
 <ul class="wp-block-navigation__container is-responsive wp-block-navigation"><li class="wp-block-navigation-item wp-block-home-link"><a class="wp-block-home-link__content wp-block-navigation-item__content" href=https://research.colfax-intl.com/ rel=home>Home</a><li class="wp-block-navigation-item wp-block-navigation-link"><a class=wp-block-navigation-item__content href=https://research.colfax-intl.com/research/><span class=wp-block-navigation-item__label>Research</span></a><li class="wp-block-navigation-item wp-block-navigation-link"><a class=wp-block-navigation-item__content href=https://research.colfax-intl.com/blog/><span class=wp-block-navigation-item__label>Articles</span></a><li class="wp-block-navigation-item wp-block-navigation-link"><a class=wp-block-navigation-item__content href=https://research.colfax-intl.com/videos/><span class=wp-block-navigation-item__label>Videos</span></a><li data-wp-context='{ "submenuOpenedBy": { "click": false, "hover": false, "focus": false }, "type": "submenu" }' data-wp-interactive=core/navigation data-wp-on--focusout=actions.handleMenuFocusout data-wp-on--keydown=actions.handleMenuKeydown data-wp-on-async--mouseenter=actions.openMenuOnHover data-wp-on-async--mouseleave=actions.closeMenuOnHover data-wp-watch=callbacks.initMenu tabindex=-1 class="wp-block-navigation-item has-child open-on-hover-click wp-block-navigation-submenu"><a class=wp-block-navigation-item__content href=#>About</a><button data-wp-bind--aria-expanded=state.isMenuOpen data-wp-on-async--click=actions.toggleMenuOnClick aria-label="About submenu" class="wp-block-navigation__submenu-icon wp-block-navigation-submenu__toggle" aria-expanded=false><svg xmlns=http://www.w3.org/2000/svg width=12 height=12 viewBox="0 0 12 12" fill=none aria-hidden=true focusable=false><path d="M1.50002 4L6.00002 8L10.5 4" stroke-width=1.5></path></svg></button><ul data-wp-on-async--focus=actions.openMenuOnFocus class="wp-block-navigation__submenu-container wp-block-navigation-submenu"><li class="wp-block-navigation-item wp-block-navigation-link"><a class=wp-block-navigation-item__content href=https://research.colfax-intl.com/mission-statement/><span class=wp-block-navigation-item__label>Mission</span></a></ul><li class="wp-block-navigation-item wp-block-navigation-link"><a class=wp-block-navigation-item__content href=https://colfaxresearch.com/><span class=wp-block-navigation-item__label> Archived Content</span></a></ul>
 </div>
 </div>
 </div>
 </div></nav></div>
</div>
<div style=height:40px aria-hidden=true class=wp-block-spacer></div>
</header>
<main class="wp-block-group is-layout-flow wp-block-group-is-layout-flow" style=margin-top:var(--wp--preset--spacing--50) id=wp--skip-link--target>
<div class="wp-block-group has-global-padding is-layout-constrained wp-block-group-is-layout-constrained"><h1 style=margin-bottom:var(--wp--preset--spacing--40) class=wp-block-post-title>CUTLASS Tutorial: Mastering the NVIDIA® Tensor Memory Accelerator (TMA)</h1></div>
<div class="entry-content wp-block-post-content has-global-padding is-layout-constrained wp-block-post-content-is-layout-constrained">
<p>TMA (Tensor Memory Accelerator) is a new feature introduced in the NVIDIA Hopper™ architecture for doing asynchronous memory copy between a GPU’s global memory (GMEM) and the shared memory (SMEM) of its threadblocks (i.e., CTAs). Compared to prior approaches, TMA offers a number of advantages, such as (1) improving GPU utilization through facilitating <a href=https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md#warp-specialization data-type=link data-id=https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md#warp-specialization>warp-specialized</a> kernel schedules via asynchrony, and (2) handling the computation of auxiliary copy data such as addresses and strides in a single-threaded manner via the TMA copy descriptor, which is both more register-efficient and necessarily handles predication (e.g., out-of-bounds checks). These advantages are well articulated in NVIDIA’s <a href=https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/ data-type=link data-id=https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/>technical blog</a> and <a href=https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html#tensor-memory-accelerator data-type=link data-id=https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html#tensor-memory-accelerator>Hopper tuning guide</a>, which we highly recommend to readers for understanding the rationales behind the design of TMA.</p>
<p>In contrast to those sources, this blog post is focused on achieving an operational understanding of how to write kernels that use TMA. Throughout, we rely on the <a href=https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/00_quickstart.md data-type=link data-id=https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/00_quickstart.md>CuTe library</a>, in which TMA is exposed through APIs wrapping lower-level GPU instructions. These instructions include PTX instructions <code><a href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor data-type=link data-id=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor>cp.async.bulk.tensor</a></code> and <a href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-reduce-async-bulk-tensor data-type=link data-id=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-reduce-async-bulk-tensor><code>cp.reduce.async.bulk.tensor</code></a>, as well as the <a href=https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html data-type=link data-id=https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html>cuTensorMap</a> operand, which we will also discuss in this post.</p>
<p>We organize this blog post into three main sections: the first about TMA load, the second about TMA store, and lastly the third covering more advanced operations such as TMA store reduce and TMA load multicast. In essence, TMA load copies (“loads”) data from the GPU’s GMEM into one of its CTA’s SMEM, while TMA store copies (“stores”) data from a CTA’s SMEM to the GPU’s GMEM. Since TMA load, TMA store, and the more advanced variants share many concepts, we will introduce the bulk of the necessary concepts in the TMA load section and only focus on the remaining differences in the subsequent sections.</p>
<p>Also, given that TMA is an asynchronous operation (executed in the&nbsp;<a href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#async-proxy>async proxy</a>), we will need to use certain memory consistency enforcement tools, such as&nbsp;async memory barrier (i.e., <a href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-mbarrier><code>mbarrier</code></a>)&nbsp;and async memory fence&nbsp;(i.e., <a href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-membar-fence><code>fence.proxy.async</code></a>), to ensure correct behavior of the kernel. Synchronization is a vast topic of discussion by itself, so we will only cover these concepts to the degree needed for their practical use.</p>
<p>Finally, for readers looking for a resource that covers many of the same points with no reference to CUTLASS or CuTe concepts, we recommend <a href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#tensor-memory-access>the treatment of TMA</a> in the CUDA® programming guide.</p>
<h2 class=wp-block-heading>TMA Load</h2>
<p>TMA load copies data from GMEM into SMEM. In this section, we demonstrate how to write a kernel that uses TMA load for this goal. A kernel that uses TMA load is quite different from a kernel that uses other memory copy methods, so we will first show how to write such a kernel for a simple example task. Then, we will explain the involved concepts.</p>
<h4 class=wp-block-heading><strong>Example task</strong></h4>
<p>To demonstrate the usage of TMA load, we consider a simple task of tiling a 2D row-major matrix. We are given a matrix <code>A</code> of shape <code>[m,n]</code> and two positive integers <code>CTA_M</code> and <code>CTA_N</code>. Note that <code>CTA_M</code> and <code>CTA_N</code> are known at compilation time, while <code>m</code> and <code>n</code> are given to us at runtime via the matrix <code>A</code>. For simplicity, let’s also assume that <code>m % CTA_M == n % CTA_N == 0</code>, though we will see later that this requirement can be relaxed.</p>
<p>We launch a grid of CTAs with size <code>{m/CTA_M, n/CTA_N, 1}</code>, where the SMEM of the <code>(i,j)</code>-th CTA holds the <code>(i,j)</code>-th tile with shape <code>[CTA_M, CTA_N]</code> from <code>A</code>. We can depict this assignment in <code>numpy</code> pseudocode as:</p>
<div class=wp-block-syntaxhighlighter-code><div><div id=highlighter_732056 class="syntaxhighlighter python"><table border=0 cellpadding=0 cellspacing=0><tbody><tr><td class=gutter><div class="line number1 index0 alt2">1</div><div class="line number2 index1 alt1">2</div><div class="line number3 index2 alt2">3</div><div class="line number4 index3 alt1">4</div><td class=code><div class=container><div class="line number1 index0 alt2"><code class="python plain">A </code><code class="python keyword">=</code> <code class="python plain">np.random.uniform(M, N)</code></div><div class="line number2 index1 alt1"><code class="python keyword">for</code> <code class="python plain">i </code><code class="python keyword">in</code> <code class="python functions">range</code><code class="python plain">(M):</code></div><div class="line number3 index2 alt2"><code class="python spaces">&nbsp;&nbsp;</code><code class="python keyword">for</code> <code class="python plain">j </code><code class="python keyword">in</code> <code class="python functions">range</code><code class="python plain">(N):</code></div><div class="line number4 index3 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">cta_i_j </code><code class="python keyword">=</code> <code class="python plain">A.reshape(M </code><code class="python keyword">/</code><code class="python keyword">/</code> <code class="python plain">CTA_M, CTA_M, N </code><code class="python keyword">/</code><code class="python keyword">/</code> <code class="python plain">CTA_N, N)[i, :, j, :]</code></div></div></table></div></div></div>
<p><strong>The two-step process.</strong> To perform this task, we use TMA load. In CuTe, a TMA load operation is implemented in two steps. The first step is the construction of the TMA copy descriptor in the <em>host code</em>, while the second step is the execution of the actual TMA load using this descriptor inside the <em>kernel code.</em> Note that this two-step process is different from what we normally do with CuTe’s TiledCopy — where all the copy steps are written in the kernel code — as shown in <a href=https://github.com/NVIDIA/cutlass/blob/637b15906358191cb4238af419d408a65819d7ec/examples/cute/tutorial/tiled_copy.cu#L120-L124>this tutorial</a>.</p>
<h4 class=wp-block-heading><strong>Host code</strong></h4>
<p>On the host, we create three objects: the GMEM tensor which we copy <em>from</em>, the layout of the SMEM tensor on <em>each</em> of the CTAs that we copy <em>into</em>, and a <code>tma_load</code> object that takes these two as arguments. Note that since we create the SMEM layout on the host, all CTAs will share the same SMEM layout for the purposes of the TMA load. Once we have these objects, they can be passed to the kernel on device, inside of which the TMA load operation is invoked.</p>
<p>The entire code block on the host is:</p>
<div class=wp-block-syntaxhighlighter-code><div><div id=highlighter_320162 class="syntaxhighlighter cpp"><table border=0 cellpadding=0 cellspacing=0><tbody><tr><td class=gutter><div class="line number1 index0 alt2">1</div><div class="line number2 index1 alt1">2</div><div class="line number3 index2 alt2">3</div><div class="line number4 index3 alt1">4</div><div class="line number5 index4 alt2">5</div><div class="line number6 index5 alt1">6</div><div class="line number7 index6 alt2">7</div><div class="line number8 index7 alt1">8</div><div class="line number9 index8 alt2">9</div><div class="line number10 index9 alt1">10</div><div class="line number11 index10 alt2">11</div><div class="line number12 index11 alt1 highlighted">12</div><div class="line number13 index12 alt2 highlighted">13</div><div class="line number14 index13 alt1">14</div><div class="line number15 index14 alt2">15</div><div class="line number16 index15 alt1">16</div><div class="line number17 index16 alt2">17</div><div class="line number18 index17 alt1">18</div><div class="line number19 index18 alt2">19</div><td class=code><div class=container><div class="line number1 index0 alt2"><code class="cpp keyword bold">template</code> <code class="cpp plain">&lt;</code><code class="cpp keyword bold">typename</code> <code class="cpp plain">T, </code><code class="cpp color1 bold">int</code> <code class="cpp plain">CTA_M, </code><code class="cpp color1 bold">int</code> <code class="cpp plain">CTA_N&gt;</code></div><div class="line number2 index1 alt1"><code class="cpp keyword bold">void</code> <code class="cpp plain">host_fn(T* data, </code><code class="cpp color1 bold">int</code> <code class="cpp plain">M, </code><code class="cpp color1 bold">int</code> <code class="cpp plain">N) {</code></div><div class="line number3 index2 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">using</code> <code class="cpp keyword bold">namespace</code> <code class="cpp plain">cute;</code></div><div class="line number4 index3 alt1">&nbsp;</div><div class="line number5 index4 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp comments">// create the GMEM tensor</code></div><div class="line number6 index5 alt1"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">auto</code> <code class="cpp plain">gmem_layout = make_layout(make_shape(M, N), LayoutRight{});</code></div><div class="line number7 index6 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">auto</code> <code class="cpp plain">gmem_tensor = make_tensor(make_gmem_ptr(T), gmem_layout);</code></div><div class="line number8 index7 alt1">&nbsp;</div><div class="line number9 index8 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp comments">// create the SMEM layout</code></div><div class="line number10 index9 alt1"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">auto</code> <code class="cpp plain">smem_layout = make_layout(make_shape(CTA_M, CTA_N), LayoutRight{});</code></div><div class="line number11 index10 alt2">&nbsp;</div><div class="line number12 index11 alt1 highlighted"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp comments">// create the TMA object</code></div><div class="line number13 index12 alt2 highlighted"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">auto</code> <code class="cpp plain">tma_load = make_tma_copy(SM90_TMA_LOAD{}, gmem_tensor, smem_layout);</code></div><div class="line number14 index13 alt1">&nbsp;</div><div class="line number15 index14 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp comments">// invoke the kernel</code></div><div class="line number16 index15 alt1"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">tma_load_kernel&lt;CTA_M, CTA_N&gt;</code></div><div class="line number17 index16 alt2"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">&lt;&lt;&lt;1, dim3{M / CTA_M, N / CTA_N, 1}&gt;&gt;&gt;</code></div><div class="line number18 index17 alt1"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">(tma_load, gmem_tensor, smem_layout);</code></div><div class="line number19 index18 alt2"><code class="cpp plain">}</code></div></div></table></div></div></div>
<p>The lines that create <code>gmem_layout</code>, <code>gmem_tensor</code>, and <code>smem_tensor</code> simply use basic CuTE concepts, so we refer readers to <a href=https://github.com/NVIDIA/cutlass/blob/637b15906358191cb4238af419d408a65819d7ec/media/docs/cute/01_layout.md>these</a> <a href=https://github.com/NVIDIA/cutlass/blob/637b15906358191cb4238af419d408a65819d7ec/media/docs/cute/02_layout_algebra.md>CuTe</a> <a href=https://github.com/NVIDIA/cutlass/blob/637b15906358191cb4238af419d408a65819d7ec/media/docs/cute/03_tensor.md>tutorials</a> for a memory refresh. Here, we focus on explaining the <code>tma_load</code> object. This object is an instance of <code>cute::TiledCopy</code>, which holds the information and implements the methods to perform a CTA-wide copy operation. In the code snippet, the <code>tma_load</code> object is created via <a href=https://github.com/NVIDIA/cutlass/blob/637b15906358191cb4238af419d408a65819d7ec/include/cute/atom/copy_traits_sm90_tma.hpp#L1206-L1217>this explicit default</a> of the <code>cute::make_tma_copy</code> function. This function’s full implementation has some nuances, which we will dive into when we discuss <code>MULTICAST</code> later in this blog post, but the explicit default suffices for most use cases, such as our example task. We recommend using the explicit default to avoid unnecessary complications (and bugs).</p>
<p>Let’s look into the signature that we used for <code>make_tma_copy</code>:</p>
<ul class=wp-block-list>
<li>Its last two arguments are <code>gmem_tensor</code> and <code>smem_layout</code>. Under the hood, <code>make_tma_copy</code> uses this information to create a <code>TmaDescriptor</code>, which is just an alias for <a href=https://github.com/NVIDIA/cutlass/blob/637b15906358191cb4238af419d408a65819d7ec/include/cute/arch/copy_sm90_desc.hpp#L178>CUtensorMap</a>. This descriptor object is used inside the TMA kernel.</li>
<li>Its first argument is an instance of <code><a href=https://github.com/NVIDIA/cutlass/blob/637b15906358191cb4238af419d408a65819d7ec/include/cute/arch/copy_sm90_tma.hpp#L269>SM90_TMA_LOAD</a></code>. This object dispatches the copy operation to the desired <code>cp.async.bulk.tensor</code> PTX call, which we will go into deeper in the third section below.</li>
</ul>
<h4 class=wp-block-heading><strong>Kernel code</strong></h4>
<p>The relevant kernel code snippet looks like this. These lines pack many important TMA concepts, which we will explain below.</p>
<div class=wp-block-syntaxhighlighter-code><div><div id=highlighter_103611 class="syntaxhighlighter cpp"><table border=0 cellpadding=0 cellspacing=0><tbody><tr><td class=gutter><div class="line number1 index0 alt2">1</div><div class="line number2 index1 alt1">2</div><div class="line number3 index2 alt2">3</div><div class="line number4 index3 alt1 highlighted">4</div><div class="line number5 index4 alt2">5</div><div class="line number6 index5 alt1">6</div><div class="line number7 index6 alt2">7</div><div class="line number8 index7 alt1">8</div><div class="line number9 index8 alt2">9</div><div class="line number10 index9 alt1">10</div><div class="line number11 index10 alt2">11</div><div class="line number12 index11 alt1">12</div><div class="line number13 index12 alt2 highlighted">13</div><div class="line number14 index13 alt1">14</div><div class="line number15 index14 alt2 highlighted">15</div><div class="line number16 index15 alt1 highlighted">16</div><div class="line number17 index16 alt2 highlighted">17</div><div class="line number18 index17 alt1 highlighted">18</div><div class="line number19 index18 alt2">19</div><div class="line number20 index19 alt1 highlighted">20</div><div class="line number21 index20 alt2">21</div><div class="line number22 index21 alt1 highlighted">22</div><div class="line number23 index22 alt2">23</div><div class="line number24 index23 alt1">24</div><div class="line number25 index24 alt2 highlighted">25</div><div class="line number26 index25 alt1 highlighted">26</div><div class="line number27 index26 alt2 highlighted">27</div><div class="line number28 index27 alt1">28</div><div class="line number29 index28 alt2">29</div><div class="line number30 index29 alt1 highlighted">30</div><div class="line number31 index30 alt2">31</div><div class="line number32 index31 alt1">32</div><div class="line number33 index32 alt2">33</div><td class=code><div class=container><div class="line number1 index0 alt2"><code class="cpp keyword bold">template</code> <code class="cpp plain">&lt;</code><code class="cpp keyword bold">typename</code> <code class="cpp plain">T, </code><code class="cpp color1 bold">int</code> <code class="cpp plain">CTA_M, </code><code class="cpp color1 bold">int</code> <code class="cpp plain">CTA_N, </code><code class="cpp keyword bold">class</code> <code class="cpp plain">TmaLoad, </code><code class="cpp keyword bold">class</code> <code class="cpp plain">GmemTensor&gt;</code></div><div class="line number2 index1 alt1"><code class="cpp keyword bold">void</code> <code class="cpp plain">tma_load_kernel(__grid_constant__ </code><code class="cpp keyword bold">const</code> <code class="cpp plain">TmaLoad tma_load, GmemTensor gmem_tensor) {</code></div><div class="line number3 index2 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">using</code> <code class="cpp keyword bold">namespace</code> <code class="cpp plain">cute;</code></div><div class="line number4 index3 alt1 highlighted"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">constexpr</code> <code class="cpp color1 bold">int</code> <code class="cpp plain">tma_transaction_bytes = CTA_M * CTA_N * </code><code class="cpp keyword bold">sizeof</code><code class="cpp plain">(T);</code></div><div class="line number5 index4 alt2">&nbsp;</div><div class="line number6 index5 alt1"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">__shared__ T smem_data[CTA_M * CTA_N];</code></div><div class="line number7 index6 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">__shared__ </code><code class="cpp color1 bold">uint64_t</code> <code class="cpp plain">tma_load_mbar;</code></div><div class="line number8 index7 alt1">&nbsp;</div><div class="line number9 index8 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">auto</code> <code class="cpp plain">smem_layout = make_layout(make_shape(CTA_M, CTA_N), LayoutRight{});</code></div><div class="line number10 index9 alt1"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">auto</code> <code class="cpp plain">smem_tensor = make_tensor(make_smem_ptr(smem_data), smem_layout);</code></div><div class="line number11 index10 alt2">&nbsp;</div><div class="line number12 index11 alt1"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">if</code> <code class="cpp plain">(threadIdx.x == 0) {</code></div><div class="line number13 index12 alt2 highlighted"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp keyword bold">auto</code> <code class="cpp plain">gmem_tensor_coord = tma_load.get_tma_tensor(shape(gmem_tensor));</code></div><div class="line number14 index13 alt1">&nbsp;</div><div class="line number15 index14 alt2 highlighted"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp keyword bold">auto</code> <code class="cpp plain">gmem_tensor_coord_cta = local_tile(</code></div><div class="line number16 index15 alt1 highlighted"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">gmem_tensor_coord,</code></div><div class="line number17 index16 alt2 highlighted"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">Tile&lt;Int&lt;CTA_M&gt;, Int&lt;CTA_N&gt;&gt;{},</code></div><div class="line number18 index17 alt1 highlighted"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">make_coord(blockIdx.x, blockIdx.y));</code></div><div class="line number19 index18 alt2">&nbsp;</div><div class="line number20 index19 alt1 highlighted"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">initialize_barrier(tma_load_mbar, </code><code class="cpp comments">/* arrival count */</code> <code class="cpp plain">1);</code></div><div class="line number21 index20 alt2">&nbsp;</div><div class="line number22 index21 alt1 highlighted"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">set_barrier_transaction_bytes(tma_load_mbar, tma_transaction_bytes);</code></div><div class="line number23 index22 alt2">&nbsp;</div><div class="line number24 index23 alt1"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp keyword bold">auto</code> <code class="cpp plain">tma_load_per_cta = tma_load.get_slice(0);</code></div><div class="line number25 index24 alt2 highlighted"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">copy(tma_load.with(tma_load_mbar),</code></div><div class="line number26 index25 alt1 highlighted"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">tma_load_per_cta.partition_S(gmem_tensor_coord_cta),</code></div><div class="line number27 index26 alt2 highlighted"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">tma_load_per_cta.partition_D(smem_tensor));</code></div><div class="line number28 index27 alt1"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">}</code></div><div class="line number29 index28 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">__syncthreads();</code></div><div class="line number30 index29 alt1 highlighted"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">wait_barrier(tma_load_mbar, </code><code class="cpp comments">/* phase */</code> <code class="cpp plain">0);</code></div><div class="line number31 index30 alt2">&nbsp;</div><div class="line number32 index31 alt1"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp comments">// after this line, the TMA load is finished</code></div><div class="line number33 index32 alt2"><code class="cpp plain">}</code></div></div></table></div></div></div>
<p>First, at line 2, the <code>tma_load</code> argument for the kernel must be annotated with __<code>grid_constant__ const</code>. If we have two tensors that we want to copy from GMEM into SMEM, <em>each</em> of them must have its own <code>TiledCopy</code> instance, and each instance must be __<code>grid_constant__ const</code>. This is a requirement for passing a <code>cuTensorMap</code> from host to device as documented <a href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-data-copies-using-tensor-memory-access-tma>here</a>, for instance.</p>
<p>The next important point is that for a TMA copy, only one thread will be responsible for issuing the TMA operation. In the code snippet, all the TMA-related variables and instructions are contained in the <code>if</code> block starting at line 12, which is only executed by thread 0. On the other hand, line 30 contains an instruction for all threads in the CTA to wait for the TMA operations to finish.</p>
<h5 class=wp-block-heading>Coordinates and Arithmetic tuples</h5>
<p>For now, let’s look into the TMA load logic. This starts at line 13, where we create a <code>gmem_tensor_coord</code> object that holds <em>the coordinates</em> of the GMEM tensor to be copied. If we try the following:</p>
<div class=wp-block-syntaxhighlighter-code><div><div id=highlighter_670730 class="syntaxhighlighter plain"><table border=0 cellpadding=0 cellspacing=0><tbody><tr><td class=gutter><div class="line number1 index0 alt2">1</div><td class=code><div class=container><div class="line number1 index0 alt2"><code class=plain>if (cute::thread(0)) { cute::print(gmem_tensor_coord); }</code></div></div></table></div></div></div>
<p>then we see the output like so (for <code>M=N=1024</code>):</p>
<div class=wp-block-syntaxhighlighter-code><div><div id=highlighter_728443 class="syntaxhighlighter plain"><table border=0 cellpadding=0 cellspacing=0><tbody><tr><td class=gutter><div class="line number1 index0 alt2">1</div><td class=code><div class=container><div class="line number1 index0 alt2"><code class=plain>ArithTuple(_0,_0) o (1024,1024):(_1@1,_1@0)</code></div></div></table></div></div></div>
<p>Lines 15-18 are self-explanatory for readers familiar with the way tiled copy works in CuTe, where a GMEM tensor is tiled into smaller partitions, and each CTA slices into the tiled tensor according to the block coordinate to obtain its view of GMEM. Note however that the partitioning applies to the aforementioned <code>ArithTuple</code> representing the coordinates of <code>gmem_tensor</code>, instead of to <code>gmem_tensor</code> itself. In particular, the <code>ArithTuple</code> is partitioned into tiles of shape <code>[CTA_M,CTA_N]</code>, and then each CTA takes its tile.</p>
<p>If we print <code>gmem_tensor_coord_cta</code> using <code>print_tensor</code> as follows:</p>
<div class=wp-block-syntaxhighlighter-code><div><div id=highlighter_312683 class="syntaxhighlighter plain"><table border=0 cellpadding=0 cellspacing=0><tbody><tr><td class=gutter><div class="line number1 index0 alt2">1</div><td class=code><div class=container><div class="line number1 index0 alt2"><code class=plain>if (cute::block(7)) { cute::print_tensor(gmem_tensor_coord_cta); }</code></div></div></table></div></div></div>
<p>then for <code>CTA_M == CTA_N == 16</code>, we see:</p>
<div class=wp-block-syntaxhighlighter-code><div><div id=highlighter_647468 class="syntaxhighlighter plain"><table border=0 cellpadding=0 cellspacing=0><tbody><tr><td class=gutter><div class="line number1 index0 alt2">1</div><div class="line number2 index1 alt1">2</div><div class="line number3 index2 alt2">3</div><div class="line number4 index3 alt1">4</div><div class="line number5 index4 alt2">5</div><td class=code><div class=container><div class="line number1 index0 alt2"><code class=plain>ArithTuple(0,112) o (_16,_16):(_1@1,_1@0):</code></div><div class="line number2 index1 alt1"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>(0,112)&nbsp; (1,112)&nbsp; (2,112)&nbsp; (3,112)&nbsp; (4,112)&nbsp; (5,112)&nbsp; (6,112)&nbsp; (7,112)&nbsp; (8,112)&nbsp; (9,112)&nbsp; (10,112)&nbsp; (11,112)&nbsp; (12,112)&nbsp; (13,112)&nbsp; (14,112)&nbsp; (15,112)</code></div><div class="line number3 index2 alt2"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>(0,113)&nbsp; (1,113)&nbsp; (2,113)&nbsp; (3,113)&nbsp; (4,113)&nbsp; (5,113)&nbsp; (6,113)&nbsp; (7,113)&nbsp; (8,113)&nbsp; (9,113)&nbsp; (10,113)&nbsp; (11,113)&nbsp; (12,113)&nbsp; (13,113)&nbsp; (14,113)&nbsp; (15,113)</code></div><div class="line number4 index3 alt1"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>// more lines</code></div><div class="line number5 index4 alt2"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>(0,127)&nbsp; (1,127)&nbsp; (2,127)&nbsp; (3,127)&nbsp; (4,127)&nbsp; (5,127)&nbsp; (6,127)&nbsp; (7,127)&nbsp; (8,127)&nbsp; (9,127)&nbsp; (10,127)&nbsp; (11,127)&nbsp; (12,127)&nbsp; (13,127)&nbsp; (14,127)&nbsp; (15,127)</code></div></div></table></div></div></div>
<p>These numbers are the <em>coordinates</em> in <code>gmem_tensor</code> whose values will be copied into the <code>smem_tensor</code> of CTA 7. We encourage readers to try running this code snippet while replacing <code>cute::block(7)</code> with other indices to understand which CTAs copy from which coordinates in <code>gmem_tensor</code>.</p>
<p>Next, the copy operation itself, issued in lines 25-27, has the usual signature of a TiledCopy operation, where the source tensor is replaced by the partitioned coordinates.</p>
<h5 class=wp-block-heading>Memory barrier</h5>
<p>We have left out lines 20, 22, and 30, all of which involve the <code>uint64_t</code> variable <code>tma_load_mbar</code> which lives in SMEM. This is the <strong>asynchronous transaction barrier</strong> that we use to synchronize the TMA load with the rest of the kernel that consumes the resulting data loaded into SMEM. A high-level description of this type of barrier is given in the NVIDIA <a href=https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/ data-type=link data-id=https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/>technical blog</a> on Hopper architecture. In terms of our kernel, the important points are as follows:</p>
<ol class=wp-block-list>
<li>We initialize the mbarrier object in shared memory on line 20. The CuTe method <code>initialize_barrier</code> wraps the PTX instruction <code>mbarrier.init.shared.b64</code>, which takes in an additional <em>arrival count</em> parameter. In our context, since a single thread will initiate the TMA load, we should set the arrival count to 1. Moreover, the starting phase of the mbarrier will always be set to 0.<br>&nbsp;</li>
<li>We both perform an arrive-on operation and set the expected transaction count for the mbarrier object on line 22 with the CuTe method <code>set_barrier_transaction_bytes</code>, which wraps the PTX instruction <code>mbarrier.arrive.expect_tx.shared::cta.b64</code>. The transaction count is set to equal the number of bytes transferred by the TMA load, which we compute on line 4.<br><br> </li>
<li>On lines 25-27, the copy instruction, which dispatches to the desired flavor of <code>cp.async.bulk.tensor</code>, always has for its completion mechanism <code>mbarrier::complete_tx::bytes</code> with the provided mbarrier object.<br><br> </li>
<li>On line 30, we execute the wait operation on the mbarrier object. Note that all threads wait on the mbarrier, in contrast to only thread 0 arriving at the mbarrier, and the invocation of <code>__syncthreads()</code> is necessary prior to the <code>wait_barrier</code> to resolve the thread divergence.<br><br>Here, <code>wait_barrier</code> wraps the PTX instruction <code>mbarrier.try_wait.parity.shared::cta.b64</code>. The <code>try_wait</code> qualifier (as opposed to <code>test_wait</code>) indicates that the wait is a blocking instruction. The <code>parity</code> qualifier, whose use entails providing a phase bit, indicates that the thread sleeps until that phase bit of the mbarrier flips. Because this is the first use of the mbarrier post-initialization to track completion, we supply 0 as the phase. If we were doing another TMA load, we would then have to flip the phase to reuse the mbarrier.<br><br>In general, the CUTLASS <a href=https://github.com/NVIDIA/cutlass/blob/main/media/docs/pipeline.md data-type=link data-id=https://github.com/NVIDIA/cutlass/blob/main/media/docs/pipeline.md>Pipeline APIs</a> offer a higher-level way to handle the lifecycle of the mbarrier objects when doing a series of TMA loads, as one might do in a <a href=https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md#pipelining data-type=link data-id=https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md#pipelining>software pipelining</a> scheme.<br><br> </li>
<li>After <code>wait_barrier</code>, the memory consistency model furnishes us the following guarantee: the write to SMEM done by the TMA load is made visible to all threads that invoked the mbarrier wait (so in our example kernel, all threads in the CTA).<br> <br></li>
</ol>
<h5 class=wp-block-heading>REMAINDER TILES WITH TMA and STRIDE REQUIREMENTS</h5>
<p>In our above example, we supposed that <code>m%CTA_M==0</code> and <code>n%CTA_N==0</code>. However, for the purposes of doing a TMA load, we can dispense with this assumption entirely. Instead of needing to handle the out-of-bounds logic ourselves when loading in remainder tiles from GMEM to SMEM, the TMA copy unit will necessarily <a href=https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/0y_predication.md data-type=link data-id=https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/0y_predication.md>predicate</a> the memory copy to not read out-of-bounds. This is consistent with the use of special “implicit” CuTe tensors with <code>ArithTuple</code> as described above in the TMA load — if we used ordinary CuTe tensors instead, then they could be sliced to produce new CuTe tensors with possibly out-of-bounds pointers to GMEM, invariably leading to bugs.</p>
<p>However, there is one important requirement on the strides of the GMEM tensor itself to bear in mind for TMA, which is the <strong>16-byte boundary</strong> requirement. As one might expect, TMA doesn’t support copying arbitrarily strided regions of GMEM. Rather, we need to assume that the tile being copied has (i) a contiguous direction (stride 1), and (ii) other strides as multiples of 16 bytes. This is <a href=https://github.com/NVIDIA/cutlass/blob/7d49e6c7e2f8896c47f586706e67e1fb215529dc/include/cute/atom/copy_traits_sm90_tma.hpp#L846>asserted</a> in the CUTLASS codebase.</p>
<p>For example, for our row-major GMEM tensor of floats, with shape <code>(m, n)</code> and stride <code>(n, 1)</code>, this imposes the requirement that <code>n%4==0</code>. If this isn’t satisfied, then one can pad the input tensors to be of the right extent before invoking the kernel.</p>
<h2 class=wp-block-heading>TMA Store</h2>
<p>Equipped with the basics of TMA load, studying TMA store is a lot easier thanks to the many similarities between the two operations. Similar to TMA load, implementing TMA store is a two-step process: defining the TMA copy descriptor on the host, and then issuing the TMA store operation inside the kernel.</p>
<h4 class=wp-block-heading>Example task and code</h4>
<p>For illustration purposes, let’s consider the reverse example of TMA load, where we copy from the SMEM in multiple CTAs to corresponding tiles in a partitioned GMEM tensor. A difference here is that we will fill the SMEM tiles in the CTAs with a simple pattern of numbers before copying them to GMEM (otherwise, we would be copying undefined values). A functional code snippet is as follows:</p>
<div class=wp-block-syntaxhighlighter-code><div><div id=highlighter_601978 class="syntaxhighlighter cpp"><table border=0 cellpadding=0 cellspacing=0><tbody><tr><td class=gutter><div class="line number1 index0 alt2">1</div><div class="line number2 index1 alt1">2</div><div class="line number3 index2 alt2">3</div><div class="line number4 index3 alt1">4</div><div class="line number5 index4 alt2">5</div><div class="line number6 index5 alt1">6</div><div class="line number7 index6 alt2">7</div><div class="line number8 index7 alt1">8</div><div class="line number9 index8 alt2">9</div><div class="line number10 index9 alt1">10</div><div class="line number11 index10 alt2">11</div><div class="line number12 index11 alt1">12</div><div class="line number13 index12 alt2">13</div><div class="line number14 index13 alt1">14</div><div class="line number15 index14 alt2 highlighted">15</div><div class="line number16 index15 alt1 highlighted">16</div><div class="line number17 index16 alt2 highlighted">17</div><div class="line number18 index17 alt1 highlighted">18</div><div class="line number19 index18 alt2">19</div><div class="line number20 index19 alt1">20</div><div class="line number21 index20 alt2">21</div><div class="line number22 index21 alt1">22</div><div class="line number23 index22 alt2">23</div><div class="line number24 index23 alt1">24</div><div class="line number25 index24 alt2">25</div><div class="line number26 index25 alt1">26</div><div class="line number27 index26 alt2">27</div><div class="line number28 index27 alt1">28</div><div class="line number29 index28 alt2 highlighted">29</div><div class="line number30 index29 alt1 highlighted">30</div><div class="line number31 index30 alt2 highlighted">31</div><div class="line number32 index31 alt1 highlighted">32</div><div class="line number33 index32 alt2">33</div><div class="line number34 index33 alt1 highlighted">34</div><div class="line number35 index34 alt2 highlighted">35</div><div class="line number36 index35 alt1">36</div><div class="line number37 index36 alt2">37</div><div class="line number38 index37 alt1">38</div><div class="line number39 index38 alt2">39</div><div class="line number40 index39 alt1">40</div><div class="line number41 index40 alt2">41</div><div class="line number42 index41 alt1">42</div><div class="line number43 index42 alt2">43</div><div class="line number44 index43 alt1">44</div><div class="line number45 index44 alt2">45</div><div class="line number46 index45 alt1">46</div><div class="line number47 index46 alt2">47</div><div class="line number48 index47 alt1">48</div><div class="line number49 index48 alt2 highlighted">49</div><div class="line number50 index49 alt1">50</div><div class="line number51 index50 alt2 highlighted">51</div><div class="line number52 index51 alt1">52</div><td class=code><div class=container><div class="line number1 index0 alt2"><code class="cpp keyword bold">template</code> <code class="cpp plain">&lt;</code><code class="cpp keyword bold">typename</code> <code class="cpp plain">T, </code><code class="cpp color1 bold">int</code> <code class="cpp plain">CTA_M=32, </code><code class="cpp color1 bold">int</code> <code class="cpp plain">CTA_N=32&gt;</code></div><div class="line number2 index1 alt1"><code class="cpp keyword bold">void</code> <code class="cpp plain">host_fn(T* data, </code><code class="cpp color1 bold">int</code> <code class="cpp plain">M, </code><code class="cpp color1 bold">int</code> <code class="cpp plain">N) {</code></div><div class="line number3 index2 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">using</code> <code class="cpp keyword bold">namespace</code> <code class="cpp plain">cute;</code></div><div class="line number4 index3 alt1">&nbsp;</div><div class="line number5 index4 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp comments">// create the GMEM tensor</code></div><div class="line number6 index5 alt1"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">auto</code> <code class="cpp plain">gmem_layout = make_layout(make_shape(M, N), LayoutRight{});</code></div><div class="line number7 index6 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">auto</code> <code class="cpp plain">gmem_tensor = make_tensor(make_gmem_ptr(T), gmem_layout);</code></div><div class="line number8 index7 alt1">&nbsp;</div><div class="line number9 index8 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp comments">// create the SMEM layout</code></div><div class="line number10 index9 alt1"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">auto</code> <code class="cpp plain">smem_layout = make_layout(make_shape(CTA_M, CTA_N), LayoutRight{});</code></div><div class="line number11 index10 alt2">&nbsp;</div><div class="line number12 index11 alt1"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp comments">// create the TMA object</code></div><div class="line number13 index12 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">auto</code> <code class="cpp plain">tma_store = make_tma_copy(SM90_TMA_STORE{}, gmem_tensor, smem_layout);</code></div><div class="line number14 index13 alt1">&nbsp;</div><div class="line number15 index14 alt2 highlighted"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp comments">// invoke the kernel</code></div><div class="line number16 index15 alt1 highlighted"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">tma_store_kernel&lt;CTA_M, CTA_N&gt;</code></div><div class="line number17 index16 alt2 highlighted"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">&lt;&lt;&lt;CTA_M, dim3{M / CTA_M, N / CTA_N, 1}&gt;&gt;&gt;</code></div><div class="line number18 index17 alt1 highlighted"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">(tma_store, gmem_tensor, smem_layout);</code></div><div class="line number19 index18 alt2"><code class="cpp plain">}</code></div><div class="line number20 index19 alt1">&nbsp;</div><div class="line number21 index20 alt2"><code class="cpp keyword bold">template</code> <code class="cpp plain">&lt;</code><code class="cpp keyword bold">typename</code> <code class="cpp plain">T, </code><code class="cpp color1 bold">int</code> <code class="cpp plain">CTA_M, </code><code class="cpp color1 bold">int</code> <code class="cpp plain">CTA_N, </code><code class="cpp keyword bold">class</code> <code class="cpp plain">TmaStore, </code><code class="cpp keyword bold">class</code> <code class="cpp plain">GmemTensor&gt;</code></div><div class="line number22 index21 alt1"><code class="cpp keyword bold">void</code> <code class="cpp plain">tma_store_kernel(__grid_constant__ </code><code class="cpp keyword bold">const</code> <code class="cpp plain">TmaStore tma_store, GmemTensor gmem_tensor) {</code></div><div class="line number23 index22 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">using</code> <code class="cpp keyword bold">namespace</code> <code class="cpp plain">cute;</code></div><div class="line number24 index23 alt1"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">__shared__ T smem_data[CTA_M * CTA_N];</code></div><div class="line number25 index24 alt2">&nbsp;</div><div class="line number26 index25 alt1"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">auto</code> <code class="cpp plain">smem_layout = make_layout(make_shape(CTA_M, CTA_N), LayoutRight{});</code></div><div class="line number27 index26 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">auto</code> <code class="cpp plain">smem_tensor = make_tensor(make_smem_ptr(T), smem_layout);</code></div><div class="line number28 index27 alt1">&nbsp;</div><div class="line number29 index28 alt2 highlighted"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp comments">// fill the rows of smem_data</code></div><div class="line number30 index29 alt1 highlighted"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">for</code> <code class="cpp plain">(</code><code class="cpp color1 bold">int</code> <code class="cpp plain">j = 0; j &lt; CTA_N; ++j) {</code></div><div class="line number31 index30 alt2 highlighted"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">smem_data(threadIdx.x, j) = threadIdx.x;</code></div><div class="line number32 index31 alt1 highlighted"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">}</code></div><div class="line number33 index32 alt2"><code class="cpp spaces">&nbsp;</code>&nbsp;</div><div class="line number34 index33 alt1 highlighted"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">__syncthreads();</code></div><div class="line number35 index34 alt2 highlighted"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">tma_store_fence();</code></div><div class="line number36 index35 alt1">&nbsp;</div><div class="line number37 index36 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">if</code> <code class="cpp plain">(threadIdx.x == 0) {</code></div><div class="line number38 index37 alt1"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp keyword bold">auto</code> <code class="cpp plain">gmem_tensor_coord = tma_store.get_tma_tensor(shape(gmem_tensor));</code></div><div class="line number39 index38 alt2">&nbsp;</div><div class="line number40 index39 alt1"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp keyword bold">auto</code> <code class="cpp plain">gmem_tensor_coord_cta = local_tile(</code></div><div class="line number41 index40 alt2"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">gmem_tensor_coord,</code></div><div class="line number42 index41 alt1"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">Tile&lt;Int&lt;CTA_M&gt;, Int&lt;CTA_N&gt;&gt;{},</code></div><div class="line number43 index42 alt2"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">make_coord(blockIdx.x, blockIdx.y));</code></div><div class="line number44 index43 alt1">&nbsp;</div><div class="line number45 index44 alt2"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp keyword bold">auto</code> <code class="cpp plain">tma_store_per_cta = tma_store.get_slice(0);</code></div><div class="line number46 index45 alt1"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">copy(tma_store,</code></div><div class="line number47 index46 alt2"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">tma_store_per_cta.partition_S(smem_tensor),</code></div><div class="line number48 index47 alt1"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">tma_store_per_cta.partition_D(gmem_tensor_coord_per_cta));</code></div><div class="line number49 index48 alt2 highlighted"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp comments">// tma_store_arrive();</code></div><div class="line number50 index49 alt1"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">}</code></div><div class="line number51 index50 alt2 highlighted"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp comments">// tma_store_wait&lt;0&gt;();</code></div><div class="line number52 index51 alt1"><code class="cpp plain">}</code></div></div></table></div></div></div>
<p>The host code looks almost identical to that of TMA load, except for the call to <code>tma_store_kernel</code>. Note that we have arranged for each CTA to have <code>CTA_M</code> threads. Our example then has each CTA hold a<code>[CTA_M,CTA_N]</code>tile in SMEM such that in lines 29-32, thread <code>i</code> fills row <code>i</code> with the value <code>i</code>.</p>
<p>In the kernel code, the <code>if</code> block in lines 39-49 is similar to the <code>if</code> block in the <code>tma_load_kernel</code>. In particular, only thread <code>0</code> issues the TMA store operation. All of the tensor tiling logic is conceptually the same. However, the copying direction is reversed: for TMA store, the <code>tma_store_per_cta.partition_S</code> method is applied to <code>smem_tensor</code>, while the <code>tma_store_per_cta.partition_D</code> method is applied to the coordinates of the GMEM tensor. Note that the coordinates are also represented as an <code>ArithTuple</code>, similar to TMA load.</p>
<h5 class=wp-block-heading>Memory fence</h5>
<p>The most important difference between the code for TMA load and store is that we no longer see any mbarrier object being used with TMA store. This is because TMA store uses another mechanism to enforce memory consistency: a <em>memory fence</em>.</p>
<p>The intention of a memory fence is to establish a guaranteed ordering between memory accesses requested by the executing thread before and after the fence. In our example, we need to ensure that all the writes to SMEM done in lines 29-32 are visible to the TMA store executed by thread 0. To this end, on line 35 we have the CuTe method <code>tma_store_fence()</code> that wraps the PTX instruction <code>fence.proxy.async.shared::cta</code>.</p>
<p>This instruction contains two important qualifiers that describe the effect of the fence: the <em>scope</em> and the <em>proxykind</em>. The <a href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#scope data-type=link data-id=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#scope>scope</a> indicates the set of threads that participate in the ordering enforced by the fence. In our case, the qualifier <code>cta</code> defines the scope as given by all threads in the CTA (which is the smallest possible scope for the purposes of the memory consistency model). The <a href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#proxies data-type=link data-id=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#proxies>proxykind</a> indicates the type of proxy that will participate in the ordering enforced by the fence, in addition to the generic proxy. In our case, we choose the proxykind to be <code>async.shared</code> since the TMA store is executed in the async proxy (with respect to each CTA). If we replaced the async fence by a different memory fence primitive such as <code>__threadfence_block()</code> that doesn’t involve the async proxy, we would destroy the guarantee needed for correct behavior of the kernel, leading to race conditions in practice.</p>
<h5 class=wp-block-heading>TMA STORE ARRIVE AND WAIT</h5>
<p>In lines 49 and 51, we have <code>tma_store_arrive()</code>, which commits the TMA store operation (technically, as a <code>cp.async.bulk-group</code>), and <code>tma_store_wait&lt;Count&gt;()</code>, which waits until at most <code>Count</code> many of the committed TMA store operations are pending (e.g., if all should be completed, then set <code>Count</code> to be 0). These operations are useful when one has other in-kernel work waiting on the completion of the TMA store — for example, this would be needed to reuse the freed SMEM made available after writing out. However, because our kernel simply exits after the TMA store is done, we don’t need the TMA store arrive and wait pattern here, so we comment out those lines.</p>
<h2 class=wp-block-heading>A Deeper Look at TMA Operations</h2>
<div class="wp-block-group has-global-padding is-layout-constrained wp-block-group-is-layout-constrained">
<figure class="wp-block-table is-style-stripes"><table><thead><tr><th><th>TMA LOAD<th>TMA STORE<tbody><tr><td>Direction<td>GMEM -&gt; SMEM<td>SMEM -&gt; GMEM<tr><td>Sync method<td>Memory barrier<td>Proxy fence<tr><td>When to sync<td>After the operation<td>Before the operation</table><figcaption class=wp-element-caption>Summary of TMA operations.</figcaption></figure>
</div>
<p>Thus far, we have learned how to invoke the TMA load and TMA store operations. The above table compares and contrasts these operations. To invoke either operation, we need to create an object akin to <code>TiledCopy</code> via the <code>cute::make_tma_copy</code> method on the host code, and then pass this object into a kernel function, where we use them in <code>cute::copy</code> to actually invoke the operation. In this section, we take a deeper dive into what really happens when we call these <code>TiledCopy</code> objects in the kernel function. From this deep dive, we discuss two extensions: TMA store reduce and TMA load multicast.</p>
<h4 class=wp-block-heading>PTX Instructions of TMA Load and Store</h4>
<p>PTX (<a href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html>Parallel Thread Execution</a>) is a low-level intermediate language for NVIDIA GPUs. For our discussion, the relevant part of PTX comprises a set of instructions that can be inserted into CUDA code via blocks wrapped by the <code>asm volatile</code> key words. In particular, when we call <code>cute::copy(tma_load, ...)</code> or <code>cute::copy(tma_store, ...)</code> as described in previous sections, certain PTX instructions are called to perform these operations. By studying the PTX, we can better understand TMA load and TMA store.</p>
<p>Let us start with TMA load. Recall that when we create the <code>tma_load</code> object in the host code, we must provide the GMEM tensor (which contains the source data to copy from) and the SMEM layout (which describes how the data will look like inside each CTA). Using this tensor and layout, CuTe determines the underlying PTX instruction to be executed when <code>cute::copy(tma_load, ...)</code> is invoked in the kernel. The PTX instruction is chosen depending on the <em>rank</em> of the GMEM tensor (note that <em>rank</em> here means the number of dimensions of the tensor, as opposed to matrix rank/nullity in linear algebra). In our example, the GMEM tensor has rank two, so the <a href=https://github.com/NVIDIA/cutlass/blob/637b15906358191cb4238af419d408a65819d7ec/include/cute/arch/copy_sm90_tma.hpp#L100-L106>following PTX instruction</a> will be executed:</p>
<div class=wp-block-syntaxhighlighter-code><div><div id=highlighter_98153 class="syntaxhighlighter cpp"><table border=0 cellpadding=0 cellspacing=0><tbody><tr><td class=gutter><div class="line number1 index0 alt2">1</div><div class="line number2 index1 alt1">2</div><div class="line number3 index2 alt2">3</div><div class="line number4 index3 alt1">4</div><div class="line number5 index4 alt2">5</div><div class="line number6 index5 alt1">6</div><div class="line number7 index6 alt2">7</div><td class=code><div class=container><div class="line number1 index0 alt2"><code class="cpp plain">asm </code><code class="cpp keyword bold">volatile</code> <code class="cpp plain">(</code></div><div class="line number2 index1 alt1"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp string">"cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes"</code></div><div class="line number3 index2 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp string">" [%0], [%1, {%3, %4}], [%2];"</code></div><div class="line number4 index3 alt1"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">:</code></div><div class="line number5 index4 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">: </code><code class="cpp string">"r"</code><code class="cpp plain">(smem_int_ptr), </code><code class="cpp string">"l"</code><code class="cpp plain">(gmem_int_desc), </code><code class="cpp string">"r"</code><code class="cpp plain">(smem_int_mbar),</code></div><div class="line number6 index5 alt1"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp string">"r"</code><code class="cpp plain">(crd0), </code><code class="cpp string">"r"</code><code class="cpp plain">(crd1)</code></div><div class="line number7 index6 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">: </code><code class="cpp string">"memory"</code><code class="cpp plain">);</code></div></div></table></div></div></div>
<p>Looking at this PTX instruction, we see many familiar concepts. For instance, <code>gmem_int_desc</code> refers to the coordinates kept in the TMA descriptor, while <code>mbarrier::complete_tx::bytes</code> and <code>smem_int_mbar</code> refer to the memory barrier. Note also that <code>tensor.2d</code> refers to the fact that we are copying a rank-2 tensor, i.e., a 2D matrix.</p>
<p>It turns out that not only TMA load but all TMA operations are wrappers around certain <code>cp.async.bulk</code> instructions. The <a href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk>NVIDIA PTX documentation dedicates an entire section</a> to discuss <code>cp.async.bulk</code> instructions, specifically their syntaxes and operands. We encourage readers to read that section and the references therein for a more thorough study of TMA operations, which cover a much larger scope than this blog post is intended to. Here, we will discuss two extensions of TMA that are exposed via these <code>cp.async.bulk</code> instructions.</p>
<h4 class=wp-block-heading>TMA Store Reduce</h4>
<p>Recall that TMA store copies data from the SMEM of multiple CTAs into the corresponding tiles in a GMEM tensor. We can <em>interpret</em> TMA store as an assignment operation illustrated by the following Python pseudocode:</p>
<div class=wp-block-syntaxhighlighter-code><div><div id=highlighter_863944 class="syntaxhighlighter python"><table border=0 cellpadding=0 cellspacing=0><tbody><tr><td class=gutter><div class="line number1 index0 alt2">1</div><div class="line number2 index1 alt1">2</div><td class=code><div class=container><div class="line number1 index0 alt2"><code class="python keyword">for</code> <code class="python plain">cta_idx </code><code class="python keyword">in</code> <code class="python functions">range</code><code class="python plain">(number_of_ctas):</code></div><div class="line number2 index1 alt1"><code class="python spaces">&nbsp;&nbsp;</code><code class="python plain">gmem_dst[cta_idx] </code><code class="python keyword">=</code> <code class="python plain">smem_src[cta_idx]</code></div></div></table></div></div></div>
<p>What if we want to do the following instead?</p>
<div class=wp-block-syntaxhighlighter-code><div><div id=highlighter_984985 class="syntaxhighlighter python"><table border=0 cellpadding=0 cellspacing=0><tbody><tr><td class=gutter><div class="line number1 index0 alt2">1</div><div class="line number2 index1 alt1">2</div><div class="line number3 index2 alt2">3</div><div class="line number4 index3 alt1">4</div><div class="line number5 index4 alt2">5</div><div class="line number6 index5 alt1">6</div><td class=code><div class=container><div class="line number1 index0 alt2"><code class="python keyword">for</code> <code class="python plain">cta_idx </code><code class="python keyword">in</code> <code class="python functions">range</code><code class="python plain">(number_of_ctas):</code></div><div class="line number2 index1 alt1"><code class="python spaces">&nbsp;&nbsp;</code><code class="python plain">gmem_dst[cta_idx] </code><code class="python keyword">+</code><code class="python keyword">=</code> <code class="python plain">smem_src[cta_idx]</code></div><div class="line number3 index2 alt2"><code class="python spaces">&nbsp;&nbsp;</code><code class="python comments"># or this:</code></div><div class="line number4 index3 alt1"><code class="python spaces">&nbsp;&nbsp;</code><code class="python plain">gmem_dst[cta_idx] </code><code class="python keyword">=</code> <code class="python functions">max</code><code class="python plain">(gmem_dst[cta_idx], smem_src[cta_idx])</code></div><div class="line number5 index4 alt2"><code class="python spaces">&nbsp;&nbsp;</code><code class="python comments"># or this:</code></div><div class="line number6 index5 alt1"><code class="python spaces">&nbsp;&nbsp;</code><code class="python plain">gmem_dst[cta_idx] </code><code class="python keyword">=</code> <code class="python functions">min</code><code class="python plain">(gmem_dst[cta_idx], smem_src[cta_idx])</code></div></div></table></div></div></div>
<p>All of these operations — namely reduce sum, reduce max, and reduce min — are fairly common in tensor programs. In particular, reduce sum is an inevitable subroutine in Split-K GEMM, while reduce max and reduce min are often used in attention. As simple as these operations look, implementing them in CUDA kernels is not very straightforward. We invite readers to briefly think through how many rounds of data movements between GMEM and SMEM must be carried out to achieve these goals before reading the next paragraph.</p>
<p>The vanilla implementation of a reduce operation that “accumulates” values from a CTA’s SMEM into a tile in a GMEM tensor consists of one GMEM read, one processing block, and one GMEM write. First, the original value from the GMEM is loaded into the CTA’s SMEM or register, then the reduce operation happens, and finally the result is written back out. This process is slow.</p>
<p>Making a slight modification to the constructor of the TMA store <code>TiledCopy</code> object allows us to condense this three-step procedure to <em>just one</em> PTX instruction, namely <code><a href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-reduce-async-bulk>cp.reduce.async.bulk</a></code> instead of <code>cp.async.bulk</code>. Precisely, we can make the following <em>one line change</em> on the host code:</p>
<div class=wp-block-syntaxhighlighter-code><div><div id=highlighter_779417 class="syntaxhighlighter cpp"><table border=0 cellpadding=0 cellspacing=0><tbody><tr><td class=gutter><div class="line number1 index0 alt2">1</div><div class="line number2 index1 alt1">2</div><div class="line number3 index2 alt2">3</div><div class="line number4 index3 alt1">4</div><div class="line number5 index4 alt2">5</div><td class=code><div class=container><div class="line number1 index0 alt2"><code class="cpp comments">// original: create a TMA store object</code></div><div class="line number2 index1 alt1"><code class="cpp keyword bold">auto</code> <code class="cpp plain">tma_store = make_tma_copy(SM90_TMA_STORE{}, gmem_tensor, smem_layout);</code></div><div class="line number3 index2 alt2">&nbsp;</div><div class="line number4 index3 alt1"><code class="cpp comments">// to create a TMA reduce sum object</code></div><div class="line number5 index4 alt2"><code class="cpp keyword bold">auto</code> <code class="cpp plain">tma_reduce_sum = make_tma_copy(SM90_TMA_REDUCE_ADD{}, gmem_tensor, smem_layout);</code></div></div></table></div></div></div>
<p>and then use <code>tma_reduce_sum</code> instead, which now calls <code>cp.reduce.async.bulk</code> instead of <code>cp.async.bulk</code> under the hood.</p>
<p>As an aside, the PTX instruction <code>cp.reduce.async.bulk</code> has been available since the release of CUDA 12.0, but was not exposed through CUTLASS and CuTe until the <a href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-reduce-async-bulk>CUTLASS 3.5</a> release. We hope other reduction operations will be exposed in future releases, but if they are not, it’s fairly simple to adapt the CuTe code for TMA reduce add to perform the max and min reductions, as well as other bitwise reductions that <code>cp.reduce.async.bulk</code> offers: <code>and</code>, <code>or</code>, <code>xor</code>, <code>inc</code>, and <code>dec</code>.</p>
<h4 class=wp-block-heading>TMA Load Multicast</h4>
<p>In the previous section, we have seen that studying PTX instructions allows us to discover TMA reduce operations, which can be used instead of TMA store for certain applications. In this section, we will study the <em>multicast</em> extension of TMA load.</p>
<p>To aid our understanding, we first take a look at the full syntax of <a href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor data-type=link data-id=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor><code>cp.async.bulk.tensor</code></a>:</p>
<div class=wp-block-syntaxhighlighter-code><div><div id=highlighter_21176 class="syntaxhighlighter plain"><table border=0 cellpadding=0 cellspacing=0><tbody><tr><td class=gutter><div class="line number1 index0 alt2">1</div><div class="line number2 index1 alt1">2</div><div class="line number3 index2 alt2">3</div><div class="line number4 index3 alt1">4</div><div class="line number5 index4 alt2">5</div><div class="line number6 index5 alt1">6</div><div class="line number7 index6 alt2">7</div><div class="line number8 index7 alt1">8</div><div class="line number9 index8 alt2">9</div><div class="line number10 index9 alt1">10</div><div class="line number11 index10 alt2">11</div><div class="line number12 index11 alt1">12</div><div class="line number13 index12 alt2">13</div><div class="line number14 index13 alt1">14</div><div class="line number15 index14 alt2">15</div><div class="line number16 index15 alt1">16</div><div class="line number17 index16 alt2">17</div><td class=code><div class=container><div class="line number1 index0 alt2"><code class=plain>// global -&gt; shared::cluster:</code></div><div class="line number2 index1 alt1"><code class=plain>cp.async.bulk.tensor.dim.dst.src{.load_mode}.completion_mechanism</code></div><div class="line number3 index2 alt2"><code class=plain>{.multicast}{.level::cache_hint}</code></div><div class="line number4 index3 alt1"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>[dstMem],</code></div><div class="line number5 index4 alt2"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>[tensorMap, tensorCoords],</code></div><div class="line number6 index5 alt1"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>[mbar]</code></div><div class="line number7 index6 alt2"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>{, im2colOffsets}</code></div><div class="line number8 index7 alt1"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>{, ctaMask}</code></div><div class="line number9 index8 alt2"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>{, cache-policy}</code></div><div class="line number10 index9 alt1">&nbsp;</div><div class="line number11 index10 alt2"><code class=plain>.dst =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; { .shared::cluster }</code></div><div class="line number12 index11 alt1"><code class=plain>.src =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; { .global }</code></div><div class="line number13 index12 alt2"><code class=plain>.dim =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; { .1d, .2d, .3d, .4d, .5d }</code></div><div class="line number14 index13 alt1"><code class=plain>.completion_mechanism = { .mbarrier::complete_tx::bytes }</code></div><div class="line number15 index14 alt2"><code class=plain>.load_mode =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; { .tile, .im2col }</code></div><div class="line number16 index15 alt1"><code class=plain>.level::cache_hint =&nbsp;&nbsp;&nbsp; { .L2::cache_hint }</code></div><div class="line number17 index16 alt2"><code class=plain>.multicast =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; { .multicast::cluster&nbsp; }</code></div></div></table></div></div></div>
<p>Again, without the need to completely understand the syntax of PTX instructions, we see many familiar concepts such as <code>.dim</code>, <code>.global</code> for <code>src</code>, and <code>.mbarrier</code> for <code>completion_mechanism</code>. This section focuses on the <code>multicast</code> operand.</p>
<p>Multicast refers to a situation where we have a tile in a GMEM tensor that we want to copy to <em>multiple</em> SMEM locations in multiple CTAs. This is typically the case in GEMM kernels (i.e., matrix multiplication), where an input matrix column tile is needed for multiple row tiles or vice versa. In such cases, while TMA load is still perfectly functional — we simply provide the same TMA descriptor to the multiple CTAs that need it — the <code>.multicast</code> operand allows us to <em>guarantee</em> L2-cache hits.</p>
<p>Let’s consider an extension of the above TMA load example to one with multicast. To begin with, we need to define the <em>cluster</em> dimensions of our kernel to be non-trivial, since a requirement for a subset of CTAs to collectively participate in a TMA load multicast operation is that they belong to the same <a href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#thread-block-clusters data-type=link data-id=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#thread-block-clusters>(threadblock) cluster</a>. In order to keep things simple, we will just change the grid dimensions as so:</p>
<div class=wp-block-syntaxhighlighter-code><div><div id=highlighter_718545 class="syntaxhighlighter cpp"><table border=0 cellpadding=0 cellspacing=0><tbody><tr><td class=gutter><div class="line number1 index0 alt2">1</div><div class="line number2 index1 alt1">2</div><div class="line number3 index2 alt2">3</div><div class="line number4 index3 alt1">4</div><div class="line number5 index4 alt2">5</div><div class="line number6 index5 alt1">6</div><div class="line number7 index6 alt2">7</div><td class=code><div class=container><div class="line number1 index0 alt2"><code class="cpp comments">// old grid dimensions and implicit trivial cluster dimensions</code></div><div class="line number2 index1 alt1"><code class="cpp plain">dim3 grid_dims = dim3{M / CTA_M, N / CTA_N, 1};</code></div><div class="line number3 index2 alt2"><code class="cpp plain">dim3 cluster_dums = dim3{1, 1, 1};</code></div><div class="line number4 index3 alt1">&nbsp;</div><div class="line number5 index4 alt2"><code class="cpp comments">// new grid dimensions and cluster dimensions</code></div><div class="line number6 index5 alt1"><code class="cpp plain">dim3 grid_dims = dim3{M / CTA_M, N / CTA_N, 2};</code></div><div class="line number7 index6 alt2"><code class="cpp plain">dim3 cluster_dums = dim3{1, 1, 2};</code></div></div></table></div></div></div>
<p>Note that when using clusters, the cluster dimensions must evenly divide into the grid dimensions, or the kernel will not launch. In our new kernel, we will then arrange for the same tile of GMEM to be loaded into each CTA’s SMEM for every pair of CTAs in the same cluster, which occurs if and only if the two CTAs have the same <code>blockIdx.x</code> and <code>blockIdx.y</code>.</p>
<p>First, in the host code we make the following change to the definition of the TMA load <code>TiledCopy</code> object:</p>
<div class=wp-block-syntaxhighlighter-code><div><div id=highlighter_108457 class="syntaxhighlighter cpp"><table border=0 cellpadding=0 cellspacing=0><tbody><tr><td class=gutter><div class="line number1 index0 alt2">1</div><div class="line number2 index1 alt1">2</div><div class="line number3 index2 alt2">3</div><div class="line number4 index3 alt1">4</div><div class="line number5 index4 alt2">5</div><div class="line number6 index5 alt1">6</div><td class=code><div class=container><div class="line number1 index0 alt2"><code class="cpp comments">// original: create a TMA load object</code></div><div class="line number2 index1 alt1"><code class="cpp keyword bold">auto</code> <code class="cpp plain">tma_load = make_tma_copy(SM90_TMA_LOAD{}, gmem_tensor, smem_layout);</code></div><div class="line number3 index2 alt2">&nbsp;</div><div class="line number4 index3 alt1"><code class="cpp comments">// new: create a TMA load multicast object for the given cluster size</code></div><div class="line number5 index4 alt2"><code class="cpp keyword bold">auto</code> <code class="cpp plain">tma_load = make_tma_copy(SM90_TMA_LOAD_MULTICAST{},</code></div><div class="line number6 index5 alt1"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">gmem_tensor, smem_layout, cute::_2{});</code></div></div></table></div></div></div>
<p>We write <code>_2{}</code> for the last parameter (the cluster size) to pass it as a compile-time constant, using the <a href=https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/01_layout.md#integers>CuTe integer types</a> provided for this purpose. In practice and more idiomatically, we would have defined the <code>ClusterShape</code> type prior (in our case, to be <code>Shape&lt;_1,_1,_2&gt;</code>) and then write <code>size&lt;2&gt;ClusterShape{}</code> for that parameter.</p>
<p>We then change the kernel code as follows:</p>
<div class=wp-block-syntaxhighlighter-code><div><div id=highlighter_320227 class="syntaxhighlighter cpp"><table border=0 cellpadding=0 cellspacing=0><tbody><tr><td class=gutter><div class="line number1 index0 alt2">1</div><div class="line number2 index1 alt1">2</div><div class="line number3 index2 alt2">3</div><div class="line number4 index3 alt1">4</div><div class="line number5 index4 alt2">5</div><div class="line number6 index5 alt1 highlighted">6</div><div class="line number7 index6 alt2 highlighted">7</div><div class="line number8 index7 alt1 highlighted">8</div><div class="line number9 index8 alt2">9</div><div class="line number10 index9 alt1">10</div><div class="line number11 index10 alt2">11</div><div class="line number12 index11 alt1">12</div><div class="line number13 index12 alt2">13</div><div class="line number14 index13 alt1">14</div><div class="line number15 index14 alt2">15</div><div class="line number16 index15 alt1">16</div><div class="line number17 index16 alt2">17</div><div class="line number18 index17 alt1">18</div><div class="line number19 index18 alt2">19</div><div class="line number20 index19 alt1">20</div><div class="line number21 index20 alt2">21</div><div class="line number22 index21 alt1">22</div><div class="line number23 index22 alt2">23</div><div class="line number24 index23 alt1">24</div><div class="line number25 index24 alt2">25</div><div class="line number26 index25 alt1 highlighted">26</div><div class="line number27 index26 alt2 highlighted">27</div><div class="line number28 index27 alt1">28</div><div class="line number29 index28 alt2">29</div><div class="line number30 index29 alt1">30</div><div class="line number31 index30 alt2 highlighted">31</div><div class="line number32 index31 alt1 highlighted">32</div><div class="line number33 index32 alt2">33</div><div class="line number34 index33 alt1">34</div><div class="line number35 index34 alt2">35</div><div class="line number36 index35 alt1">36</div><div class="line number37 index36 alt2">37</div><div class="line number38 index37 alt1">38</div><div class="line number39 index38 alt2">39</div><div class="line number40 index39 alt1">40</div><div class="line number41 index40 alt2 highlighted">41</div><div class="line number42 index41 alt1">42</div><td class=code><div class=container><div class="line number1 index0 alt2"><code class="cpp keyword bold">template</code> <code class="cpp plain">&lt;</code><code class="cpp keyword bold">typename</code> <code class="cpp plain">T, </code><code class="cpp color1 bold">int</code> <code class="cpp plain">CTA_M, </code><code class="cpp color1 bold">int</code> <code class="cpp plain">CTA_N, </code><code class="cpp keyword bold">class</code> <code class="cpp plain">ClusterShape,</code></div><div class="line number2 index1 alt1"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp keyword bold">class</code> <code class="cpp plain">TmaLoad, </code><code class="cpp keyword bold">class</code> <code class="cpp plain">GmemTensor&gt;</code></div><div class="line number3 index2 alt2"><code class="cpp keyword bold">void</code> <code class="cpp plain">tma_load_kernel(__grid_constant__ </code><code class="cpp keyword bold">const</code> <code class="cpp plain">TmaLoad tma_load,</code></div><div class="line number4 index3 alt1"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">GmemTensor gmem_tensor) {</code></div><div class="line number5 index4 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">using</code> <code class="cpp keyword bold">namespace</code> <code class="cpp plain">cute;</code></div><div class="line number6 index5 alt1 highlighted"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp color1 bold">uint32_t</code> <code class="cpp plain">block_rank_in_cluster = cute::block_rank_in_cluster();</code></div><div class="line number7 index6 alt2 highlighted"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">constexpr</code> <code class="cpp color1 bold">uint32_t</code> <code class="cpp plain">cluster_size = size&lt;2&gt;(ClusterShape{}));</code></div><div class="line number8 index7 alt1 highlighted"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">constexpr</code> <code class="cpp color1 bold">uint16_t</code> <code class="cpp plain">tma_mcast_mask = (</code><code class="cpp color1 bold">uint16_t</code><code class="cpp plain">(1) &lt;&lt; cluster_size) - 1;</code></div><div class="line number9 index8 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">constexpr</code> <code class="cpp color1 bold">int</code> <code class="cpp plain">tma_transaction_bytes = CTA_M * CTA_N * </code><code class="cpp keyword bold">sizeof</code><code class="cpp plain">(T);</code></div><div class="line number10 index9 alt1">&nbsp;</div><div class="line number11 index10 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">__shared__ T smem_data[CTA_M * CTA_N];</code></div><div class="line number12 index11 alt1"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">__shared__ </code><code class="cpp color1 bold">uint64_t</code> <code class="cpp plain">tma_load_mbar;</code></div><div class="line number13 index12 alt2">&nbsp;</div><div class="line number14 index13 alt1"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">auto</code> <code class="cpp plain">smem_layout = make_layout(make_shape(CTA_M, CTA_N), LayoutRight{});</code></div><div class="line number15 index14 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">auto</code> <code class="cpp plain">smem_tensor = make_tensor(make_smem_ptr(T), smem_layout);</code></div><div class="line number16 index15 alt1"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">auto</code> <code class="cpp plain">gmem_tensor_coord = tma_load.get_tma_tensor(shape(gmem_tensor));</code></div><div class="line number17 index16 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">auto</code> <code class="cpp plain">gmem_tensor_coord_cta = local_tile(</code></div><div class="line number18 index17 alt1"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">gmem_tensor_coord,</code></div><div class="line number19 index18 alt2"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">Tile&lt;Int&lt;CTA_M&gt;, Int&lt;CTA_N&gt;&gt;{},</code></div><div class="line number20 index19 alt1"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">make_coord(blockIdx.x, blockIdx.y));</code></div><div class="line number21 index20 alt2">&nbsp;</div><div class="line number22 index21 alt1"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">if</code> <code class="cpp plain">(threadIdx.x == 0) {</code></div><div class="line number23 index22 alt2"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">initialize_barrier(tma_load_mbar, </code><code class="cpp comments">/* arrival count */</code> <code class="cpp plain">1);</code></div><div class="line number24 index23 alt1"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">}</code></div><div class="line number25 index24 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">__syncthreads();</code></div><div class="line number26 index25 alt1 highlighted"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">cute::cluster_sync();</code></div><div class="line number27 index26 alt2 highlighted"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">cutlass::arch::fence_barrier_init();</code></div><div class="line number28 index27 alt1">&nbsp;</div><div class="line number29 index28 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp keyword bold">if</code> <code class="cpp plain">(threadIdx.x == 0) {</code></div><div class="line number30 index29 alt1"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">set_barrier_transaction_bytes(tma_load_mbar, tma_transaction_bytes);</code></div><div class="line number31 index30 alt2 highlighted"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp keyword bold">auto</code> <code class="cpp plain">tma_load_per_cta = tma_load.get_slice(block_rank_in_cluster);</code></div><div class="line number32 index31 alt1 highlighted"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">copy(tma_load.with(tma_load_mbar, tma_mcast_mask),</code></div><div class="line number33 index32 alt2"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">tma_load_per_cta.partition_S(gmem_tensor_coord_per_cta),</code></div><div class="line number34 index33 alt1"><code class="cpp spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="cpp plain">tma_load_per_cta.partition_D(smem_tensor));</code></div><div class="line number35 index34 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">}</code></div><div class="line number36 index35 alt1"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">__syncthreads();</code></div><div class="line number37 index36 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">wait_barrier(tma_load_mbar, </code><code class="cpp comments">/* phase */</code> <code class="cpp plain">0);</code></div><div class="line number38 index37 alt1">&nbsp;</div><div class="line number39 index38 alt2"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp comments">// after this line, the TMA load is finished</code></div><div class="line number40 index39 alt1">&nbsp;</div><div class="line number41 index40 alt2 highlighted"><code class="cpp spaces">&nbsp;&nbsp;</code><code class="cpp plain">cute::cluster_sync();</code></div><div class="line number42 index41 alt1"><code class="cpp plain">}</code></div></div></table></div></div></div>
<p>We have highlighted the relevant changes. First, we now need to track the internal index of the CTA within its cluster, which we fetch via the CuTe method <code>block_rank_in_cluster()</code>. This returns the value of the special register <code><a href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#special-registers-cluster-ctarank data-type=link data-id=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#special-registers-cluster-ctarank>%cluster_ctarank</a></code>, which will take on values 0 and 1 in our example. For brevity, let us refer to this as the <code>ctaid</code>. We then have the following three modifications to the code to unpack:</p>
<ol class=wp-block-list>
<li>Additional cluster synchronization primitives.</li>
<li>Use of the <code>uint16</code> bitmask in the multicast operation.</li>
<li>Use of the <code>ctaid</code> in determining the slice of the <code>TiledCopy</code> object used to partition the GMEM and SMEM tensors.</li>
</ol>
<p>For (1), we use the CuTe method <code>cluster_sync()</code>, which does <em>both</em> a cluster barrier arrive and wait operation in sequence. We insert this in two places: in lines 7-8 we use <code>cluster_sync()</code> together with a fence to ensure cluster-wide visibility of the mbarrier initialization, and on line 41 we use another <code>cluster_sync()</code> to ensure that one of the two CTAs in the cluster doesn’t exit prematurely while the other is still waiting for the multicast load to complete. In general, there would be compute done on the data loaded into SMEM, and the last <code>cluster_sync()</code> would appear at the very end of the kernel code.</p>
<p>For (2), we pass a <code>uint16</code> bitmask to the <code>copy</code> operation to specify which CTAs will participate in the TMA multicast load. The bits set to 1 in the mask indicate which CTAs are active, with a maximum of 16 CTAs in a cluster (<a href=https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html#thread-block-clusters data-type=link data-id=https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html#thread-block-clusters>maximum nonportable size</a>) and the position of the bit corresponding to the <code>ctaid</code>. Thus, in our example, by setting <code>tma_mcast_mask</code> to <code>0b11</code> we specify that both CTAs in the cluster will participate.</p>
<p>Finally, for (3), the <code>ctaid</code> is used to specify the offset used when slicing into GMEM for the TMA multicast load operation launched from the given CTA. To explain this point clearly, consider the following example of loading in a 16 x 16 tile of integers, initialized to be 0-255 in ascending row-major order, from GMEM to the SMEM of two CTAs in a cluster. Suppose we mistakenly gave 0 as the parameter to <code>tma_load.get_slice</code> for <strong>both</strong> CTAs. Then we obtain the following in both CTAs’ SMEM after completion of the load:</p>
<div class=wp-block-syntaxhighlighter-code><div><div id=highlighter_418568 class="syntaxhighlighter plain"><table border=0 cellpadding=0 cellspacing=0><tbody><tr><td class=gutter><div class="line number1 index0 alt2">1</div><div class="line number2 index1 alt1">2</div><div class="line number3 index2 alt2">3</div><div class="line number4 index3 alt1">4</div><div class="line number5 index4 alt2">5</div><div class="line number6 index5 alt1">6</div><div class="line number7 index6 alt2">7</div><div class="line number8 index7 alt1">8</div><div class="line number9 index8 alt2">9</div><div class="line number10 index9 alt1">10</div><div class="line number11 index10 alt2">11</div><div class="line number12 index11 alt1">12</div><div class="line number13 index12 alt2">13</div><div class="line number14 index13 alt1">14</div><div class="line number15 index14 alt2">15</div><div class="line number16 index15 alt1">16</div><td class=code><div class=container><div class="line number1 index0 alt2"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>0&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp; 2&nbsp;&nbsp;&nbsp; 3&nbsp;&nbsp;&nbsp; 4&nbsp;&nbsp;&nbsp; 5&nbsp;&nbsp;&nbsp; 6&nbsp;&nbsp;&nbsp; 7&nbsp;&nbsp;&nbsp; 8&nbsp;&nbsp;&nbsp; 9&nbsp;&nbsp; 10&nbsp;&nbsp; 11&nbsp;&nbsp; 12&nbsp;&nbsp; 13&nbsp;&nbsp; 14&nbsp;&nbsp; 15</code></div><div class="line number2 index1 alt1"><code class="plain spaces">&nbsp;</code><code class=plain>16&nbsp;&nbsp; 17&nbsp;&nbsp; 18&nbsp;&nbsp; 19&nbsp;&nbsp; 20&nbsp;&nbsp; 21&nbsp;&nbsp; 22&nbsp;&nbsp; 23&nbsp;&nbsp; 24&nbsp;&nbsp; 25&nbsp;&nbsp; 26&nbsp;&nbsp; 27&nbsp;&nbsp; 28&nbsp;&nbsp; 29&nbsp;&nbsp; 30&nbsp;&nbsp; 31</code></div><div class="line number3 index2 alt2"><code class="plain spaces">&nbsp;</code><code class=plain>32&nbsp;&nbsp; 33&nbsp;&nbsp; 34&nbsp;&nbsp; 35&nbsp;&nbsp; 36&nbsp;&nbsp; 37&nbsp;&nbsp; 38&nbsp;&nbsp; 39&nbsp;&nbsp; 40&nbsp;&nbsp; 41&nbsp;&nbsp; 42&nbsp;&nbsp; 43&nbsp;&nbsp; 44&nbsp;&nbsp; 45&nbsp;&nbsp; 46&nbsp;&nbsp; 47</code></div><div class="line number4 index3 alt1"><code class="plain spaces">&nbsp;</code><code class=plain>48&nbsp;&nbsp; 49&nbsp;&nbsp; 50&nbsp;&nbsp; 51&nbsp;&nbsp; 52&nbsp;&nbsp; 53&nbsp;&nbsp; 54&nbsp;&nbsp; 55&nbsp;&nbsp; 56&nbsp;&nbsp; 57&nbsp;&nbsp; 58&nbsp;&nbsp; 59&nbsp;&nbsp; 60&nbsp;&nbsp; 61&nbsp;&nbsp; 62&nbsp;&nbsp; 63</code></div><div class="line number5 index4 alt2"><code class="plain spaces">&nbsp;</code><code class=plain>64&nbsp;&nbsp; 65&nbsp;&nbsp; 66&nbsp;&nbsp; 67&nbsp;&nbsp; 68&nbsp;&nbsp; 69&nbsp;&nbsp; 70&nbsp;&nbsp; 71&nbsp;&nbsp; 72&nbsp;&nbsp; 73&nbsp;&nbsp; 74&nbsp;&nbsp; 75&nbsp;&nbsp; 76&nbsp;&nbsp; 77&nbsp;&nbsp; 78&nbsp;&nbsp; 79</code></div><div class="line number6 index5 alt1"><code class="plain spaces">&nbsp;</code><code class=plain>80&nbsp;&nbsp; 81&nbsp;&nbsp; 82&nbsp;&nbsp; 83&nbsp;&nbsp; 84&nbsp;&nbsp; 85&nbsp;&nbsp; 86&nbsp;&nbsp; 87&nbsp;&nbsp; 88&nbsp;&nbsp; 89&nbsp;&nbsp; 90&nbsp;&nbsp; 91&nbsp;&nbsp; 92&nbsp;&nbsp; 93&nbsp;&nbsp; 94&nbsp;&nbsp; 95</code></div><div class="line number7 index6 alt2"><code class="plain spaces">&nbsp;</code><code class=plain>96&nbsp;&nbsp; 97&nbsp;&nbsp; 98&nbsp;&nbsp; 99&nbsp; 100&nbsp; 101&nbsp; 102&nbsp; 103&nbsp; 104&nbsp; 105&nbsp; 106&nbsp; 107&nbsp; 108&nbsp; 109&nbsp; 110&nbsp; 111</code></div><div class="line number8 index7 alt1"><code class=plain>112&nbsp; 113&nbsp; 114&nbsp; 115&nbsp; 116&nbsp; 117&nbsp; 118&nbsp; 119&nbsp; 120&nbsp; 121&nbsp; 122&nbsp; 123&nbsp; 124&nbsp; 125&nbsp; 126&nbsp; 127</code></div><div class="line number9 index8 alt2"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0</code></div><div class="line number10 index9 alt1"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0</code></div><div class="line number11 index10 alt2"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0</code></div><div class="line number12 index11 alt1"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0</code></div><div class="line number13 index12 alt2"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0</code></div><div class="line number14 index13 alt1"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0</code></div><div class="line number15 index14 alt2"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0</code></div><div class="line number16 index15 alt1"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0</code></div></div></table></div></div></div>
<p>In contrast, if we have 1 be the given parameter for both CTAs, then we get this in both CTAs’ SMEM:</p>
<div class=wp-block-syntaxhighlighter-code><div><div id=highlighter_344621 class="syntaxhighlighter plain"><table border=0 cellpadding=0 cellspacing=0><tbody><tr><td class=gutter><div class="line number1 index0 alt2">1</div><div class="line number2 index1 alt1">2</div><div class="line number3 index2 alt2">3</div><div class="line number4 index3 alt1">4</div><div class="line number5 index4 alt2">5</div><div class="line number6 index5 alt1">6</div><div class="line number7 index6 alt2">7</div><div class="line number8 index7 alt1">8</div><div class="line number9 index8 alt2">9</div><div class="line number10 index9 alt1">10</div><div class="line number11 index10 alt2">11</div><div class="line number12 index11 alt1">12</div><div class="line number13 index12 alt2">13</div><div class="line number14 index13 alt1">14</div><div class="line number15 index14 alt2">15</div><div class="line number16 index15 alt1">16</div><td class=code><div class=container><div class="line number1 index0 alt2"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0</code></div><div class="line number2 index1 alt1"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0</code></div><div class="line number3 index2 alt2"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0</code></div><div class="line number4 index3 alt1"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0</code></div><div class="line number5 index4 alt2"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0</code></div><div class="line number6 index5 alt1"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0</code></div><div class="line number7 index6 alt2"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0</code></div><div class="line number8 index7 alt1"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0</code></div><div class="line number9 index8 alt2"><code class=plain>128&nbsp; 129&nbsp; 130&nbsp; 131&nbsp; 132&nbsp; 133&nbsp; 134&nbsp; 135&nbsp; 136&nbsp; 137&nbsp; 138&nbsp; 139&nbsp; 140&nbsp; 141&nbsp; 142&nbsp; 143</code></div><div class="line number10 index9 alt1"><code class=plain>144&nbsp; 145&nbsp; 146&nbsp; 147&nbsp; 148&nbsp; 149&nbsp; 150&nbsp; 151&nbsp; 152&nbsp; 153&nbsp; 154&nbsp; 155&nbsp; 156&nbsp; 157&nbsp; 158&nbsp; 159</code></div><div class="line number11 index10 alt2"><code class=plain>160&nbsp; 161&nbsp; 162&nbsp; 163&nbsp; 164&nbsp; 165&nbsp; 166&nbsp; 167&nbsp; 168&nbsp; 169&nbsp; 170&nbsp; 171&nbsp; 172&nbsp; 173&nbsp; 174&nbsp; 175</code></div><div class="line number12 index11 alt1"><code class=plain>176&nbsp; 177&nbsp; 178&nbsp; 179&nbsp; 180&nbsp; 181&nbsp; 182&nbsp; 183&nbsp; 184&nbsp; 185&nbsp; 186&nbsp; 187&nbsp; 188&nbsp; 189&nbsp; 190&nbsp; 191</code></div><div class="line number13 index12 alt2"><code class=plain>192&nbsp; 193&nbsp; 194&nbsp; 195&nbsp; 196&nbsp; 197&nbsp; 198&nbsp; 199&nbsp; 200&nbsp; 201&nbsp; 202&nbsp; 203&nbsp; 204&nbsp; 205&nbsp; 206&nbsp; 207</code></div><div class="line number14 index13 alt1"><code class=plain>208&nbsp; 209&nbsp; 210&nbsp; 211&nbsp; 212&nbsp; 213&nbsp; 214&nbsp; 215&nbsp; 216&nbsp; 217&nbsp; 218&nbsp; 219&nbsp; 220&nbsp; 221&nbsp; 222&nbsp; 223</code></div><div class="line number15 index14 alt2"><code class=plain>224&nbsp; 225&nbsp; 226&nbsp; 227&nbsp; 228&nbsp; 229&nbsp; 230&nbsp; 231&nbsp; 232&nbsp; 233&nbsp; 234&nbsp; 235&nbsp; 236&nbsp; 237&nbsp; 238&nbsp; 239</code></div><div class="line number16 index15 alt1"><code class=plain>240&nbsp; 241&nbsp; 242&nbsp; 243&nbsp; 244&nbsp; 245&nbsp; 246&nbsp; 247&nbsp; 248&nbsp; 249&nbsp; 250&nbsp; 251&nbsp; 252&nbsp; 253&nbsp; 254&nbsp; 255</code></div></div></table></div></div></div>
<p>Finally, giving <em>either</em> 0 from <code>ctaid</code> 1 and 1 from <code>ctaid</code> 0, <em>or</em> 0 from <code>ctaid</code> 0 and 1 from <code>ctaid</code> 1, would correctly load in the entire tile to both CTAs’ SMEM. These printouts illustrate that issuing the multicast operation from one CTA in the cluster loads half of the GMEM into each of the two CTAs’ SMEM, with the slice of the <code>TiledCopy</code> determining the respective half. This is consistent with the description of multicast for <code>cp.async.bulk.tensor</code> in the PTX documentation:</p>
<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>The source data is multicast to the same CTA-relative offset as <code>dstMem</code> in the shared memory of each destination CTA.</p>
</blockquote>
<p>In terms of the <code>TiledCopy</code> object, which generically has a layout <code>TiledLayout_TV</code> mapping thread-value tuples to logical coordinates of the tile, CuTe treats the <code>ctaid</code> as the <em>thread index</em> for the purposes of slicing. For example, printing out the <code>TiledCopy</code> in our 16 x 16 example yields the following:</p>
<div class=wp-block-syntaxhighlighter-code><div><div id=highlighter_992269 class="syntaxhighlighter plain"><table border=0 cellpadding=0 cellspacing=0><tbody><tr><td class=gutter><div class="line number1 index0 alt2">1</div><div class="line number2 index1 alt1">2</div><div class="line number3 index2 alt2">3</div><div class="line number4 index3 alt1">4</div><div class="line number5 index4 alt2">5</div><div class="line number6 index5 alt1">6</div><div class="line number7 index6 alt2">7</div><div class="line number8 index7 alt1">8</div><div class="line number9 index8 alt2">9</div><td class=code><div class=container><div class="line number1 index0 alt2"><code class=plain>TiledCopy</code></div><div class="line number2 index1 alt1"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>Tiler_MN:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (_16,_16)</code></div><div class="line number3 index2 alt2"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>TiledLayout_TV: (_2,((_16,_16))):(_8,((_16,_1)))</code></div><div class="line number4 index3 alt1"><code class=plain>Copy_Atom</code></div><div class="line number5 index4 alt2"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>ThrID:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; _1:_0</code></div><div class="line number6 index5 alt1"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>ValLayoutSrc: (_1,_256):(_0,_1)</code></div><div class="line number7 index6 alt2"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>ValLayoutDst: (_1,_256):(_0,_1)</code></div><div class="line number8 index7 alt1"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>ValLayoutRef: (_1,_256):(_0,_1)</code></div><div class="line number9 index8 alt2"><code class="plain spaces">&nbsp;&nbsp;</code><code class=plain>ValueType:&nbsp;&nbsp;&nbsp; 32b</code></div></div></table></div></div></div>
<p>which has two “threads” corresponding to the two CTAs in the cluster, with the offset position given by the logical coordinate <code>(8,0)</code> in the <code>(16,16)</code> tile for <code>ctaid</code> 1.</p>
<h2 class=wp-block-heading>Conclusion</h2>
<p>In this blog post, we walked through a few simplified examples of using TMA load, store, store reduce, and load multicast to perform memory copy between GMEM and SMEM in a CUDA kernel, using the methods provided by the CUTLASS library.</p>
<p>We started by providing an overview of TMA and went into how a user can invoke these operations in a GPU kernel. Then, we dived deeper into the low-level PTX instructions in order to elicit a greater understanding of TMA. We hope this blog post is helpful for readers who want to understand TMA, to refresh their knowledge on the topic, or to debug their existing projects which use TMA.</p>
<p>We left out a few important topics such as supported swizzling modes for TMA and the ability for TMA to copy GMEM to SMEM in an <strong>interleaved</strong> format, permuting strides outside the contiguous dimension. These are important when using TMA in conjunction with the Warpgroup Matrix-Multiply-Accumulate (WGMMA) instructions, also new to the Hopper architecture, in order to load tensor data in a memory format compatible with WGMMA. We will explain these points when we discuss Hopper-based GEMM in a future post.</p>
<p>Lastly, fully-worked out examples of the kernels discussed in this blog post can be found on our <a href=https://github.com/ColfaxResearch/cfx-article-src/tree/master/tma data-type=link data-id=https://github.com/ColfaxResearch/cfx-article-src/tree/master/tma>Colfax Research GitHub repository</a>.</p>
<div class="sharedaddy sd-sharing-enabled"><div class="robots-nocontent sd-block sd-social sd-social-icon-text sd-sharing"><h3 class=sd-title>Share this:</h3><div class=sd-content><ul data-sharing-events-added=true><li class=share-linkedin><a rel="nofollow noopener noreferrer" data-shared=sharing-linkedin-10593 class="share-linkedin sd-button share-icon" href="https://research.colfax-intl.com/tutorial-hopper-tma/?share=linkedin&amp;nb=1" target=_blank title="Click to share on LinkedIn"><span>LinkedIn</span></a><li class=share-twitter><a rel="nofollow noopener noreferrer" data-shared=sharing-twitter-10593 class="share-twitter sd-button share-icon" href="https://research.colfax-intl.com/tutorial-hopper-tma/?share=twitter&amp;nb=1" target=_blank title="Click to share on Twitter"><span>Twitter</span></a><li class=share-facebook><a rel="nofollow noopener noreferrer" data-shared=sharing-facebook-10593 class="share-facebook sd-button share-icon" href="https://research.colfax-intl.com/tutorial-hopper-tma/?share=facebook&amp;nb=1" target=_blank title="Click to share on Facebook"><span>Facebook</span></a><li class=share-reddit><a rel="nofollow noopener noreferrer" data-shared class="share-reddit sd-button share-icon" href="https://research.colfax-intl.com/tutorial-hopper-tma/?share=reddit&amp;nb=1" target=_blank title="Click to share on Reddit"><span>Reddit</span></a><li class=share-end></ul></div></div></div></div><div class="wp-block-group has-global-padding is-layout-constrained wp-block-group-is-layout-constrained" style=margin-top:48px;margin-bottom:48px;padding-top:5px;padding-bottom:5px>
 
 <hr class="wp-block-separator has-alpha-channel-opacity is-style-wide" style=margin-bottom:36px>
 
 
 <h3 class="wp-block-heading has-text-align-center" style=margin-top:4px;margin-bottom:10px>Discover more from Colfax Research</h3>
 
 
 <p class=has-text-align-center style="margin-top:4px;margin-bottom:0px;font-size:clamp(14px,0.875rem + ((1vw - 3.2px)*0.114),15px)">Subscribe to get the latest posts sent to your email.</p>
 
 
 <div class="wp-block-group has-global-padding is-layout-constrained wp-container-core-group-is-layout-9 wp-block-group-is-layout-constrained">	<div class="wp-block-jetpack-subscriptions__supports-newline wp-block-jetpack-subscriptions">
 <div class="wp-block-jetpack-subscriptions__container is-not-subscriber">
 <form action=https://wordpress.com/email-subscriptions method=post accept-charset=utf-8 data-blog=229326778 data-post_access_level=everybody data-subscriber_email id=subscribe-blog>
 <div class=wp-block-jetpack-subscriptions__form-elements>
 <p id=subscribe-email>
 <label id=subscribe-field-label for=subscribe-field class=screen-reader-text>
 Type your email… </label>
 <input required type=email name=email class=no-border-radius style="font-size:16px;padding:15px 23px 15px 23px;border-radius:0px;border-width:1px" placeholder="Type your email…" value id=subscribe-field title="Please fill in this field."> </p>
 <p id=subscribe-submit>
 
 
 
 
 
 
 
 <button type=submit class="wp-block-button__link no-border-radius" style="font-size:16px;padding:15px 23px 15px 23px;margin:0;margin-left:10px;border-radius:0px;border-width:1px" name=jetpack_subscriptions_widget>
 Subscribe <span class="jetpack-memberships-spinner sf-hidden">	<svg width=24 height=24 viewBox="0 0 24 24" xmlns=http://www.w3.org/2000/svg> <path d=M12,1A11,11,0,1,0,23,12,11,11,0,0,0,12,1Zm0,19a8,8,0,1,1,8-8A8,8,0,0,1,12,20Z opacity=.25 fill=currentColor></path> <path d=M10.14,1.16a11,11,0,0,0-9,8.92A1.59,1.59,0,0,0,2.46,12,1.52,1.52,0,0,0,4.11,10.7a8,8,0,0,1,6.66-6.61A1.42,1.42,0,0,0,12,2.69h0A1.57,1.57,0,0,0,10.14,1.16Z class=jetpack-memberships-spinner-rotating fill=currentColor></path>	</svg></span></button>
 </p>
 </div>
 </form>
 </div>
 </div>
 </div>
 
</div>
<div class=wp-block-template-part>
<div style=height:0 aria-hidden=true class=wp-block-spacer></div>
<div class="wp-block-group has-global-padding is-layout-constrained wp-block-group-is-layout-constrained" style=margin-top:var(--wp--preset--spacing--70)>
<div class="wp-block-columns alignwide has-small-font-size is-layout-flex wp-container-core-columns-is-layout-1 wp-block-columns-is-layout-flex" style=margin-top:var(--wp--preset--spacing--30)>
<div class="wp-block-column is-layout-flow wp-container-core-column-is-layout-1 wp-block-column-is-layout-flow">
<div class="wp-block-group is-layout-flex wp-container-core-group-is-layout-11 wp-block-group-is-layout-flex">
<p>
 Posted </p>
<div class=wp-block-post-date><time datetime=2024-06-24T09:39:24-07:00>June 24, 2024</time></div>
<p>
 in </p>
<div class="taxonomy-category wp-block-post-terms"><a href=https://research.colfax-intl.com/category/article/ rel=tag>Article</a><span class=wp-block-post-terms__separator>, </span><a href=https://research.colfax-intl.com/category/blog/ rel=tag>Blog</a><span class=wp-block-post-terms__separator>, </span><a href=https://research.colfax-intl.com/category/papers/tutorials/ rel=tag>Tutorials</a></div></div>
</div>
</div>
</div>
</div>
<section class=wp-block-template-part>
<div class="wp-block-group has-global-padding is-layout-constrained wp-container-core-group-is-layout-13 wp-block-group-is-layout-constrained" style=padding-top:var(--wp--preset--spacing--40);padding-right:var(--wp--preset--spacing--40);padding-bottom:var(--wp--preset--spacing--40);padding-left:var(--wp--preset--spacing--40)>
<div class=wp-block-comments>
<h2 class=wp-block-heading>Comments</h2>
 <div id=respond class="comment-respond wp-block-post-comments-form">
 <h3 id=reply-title class=comment-reply-title>Leave a Reply <small><a rel=nofollow id=cancel-comment-reply-link href=#respond style=display:none>Cancel reply</a></small></h3><form action=https://research.colfax-intl.com/wp-comments-post.php method=post id=commentform class=comment-form novalidate><p class=comment-notes><span id=email-notes>Your email address will not be published.</span> <span class=required-field-message>Required fields are marked <span class=required>*</span></span><p class=comment-form-comment><label for=comment>Comment <span class=required>*</span></label> <textarea id=comment name=comment cols=45 rows=8 maxlength=65525 required value></textarea><p class=comment-form-author><label for=author>Name</label> <input id=author name=author type=text value size=30 maxlength=245 autocomplete=name></p>
<p class=comment-form-email><label for=email>Email</label> <input id=email name=email type=email value size=30 maxlength=100 aria-describedby=email-notes autocomplete=email></p>
<p class=comment-form-url><label for=url>Website</label> <input id=url name=url type=url value size=30 maxlength=200 autocomplete=url></p>
<p class=comment-form-cookies-consent><input id=wp-comment-cookies-consent name=wp-comment-cookies-consent type=checkbox value=yes> <label for=wp-comment-cookies-consent>Save my name, email, and website in this browser for the next time I comment.</label></p>
<p class="form-submit wp-block-button"><input name=submit type=submit id=submit class="wp-block-button__link wp-element-button" value="Post Comment"> 
<p style=display:none><p style=display:none!important class=akismet-fields-container data-prefix=ak_></p></form>	</div>
 </div>
</div>
</section></main>
<footer class=wp-block-template-part>
<div class="wp-block-group has-global-padding is-layout-constrained wp-block-group-is-layout-constrained">
<div class="wp-block-group alignwide is-content-justification-space-between is-layout-flex wp-container-core-group-is-layout-15 wp-block-group-is-layout-flex" style=padding-top:var(--wp--preset--spacing--40)>
<p class=has-small-font-size>Copyright © 2023-2024 Colfax International. All rights reserved.</p>
</div>
</div>
</footer></div>
 
 
<img src=data:, alt width=6 height=5 id=wpstats class=sf-hidden>